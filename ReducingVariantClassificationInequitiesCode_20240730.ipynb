{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147e0be8",
   "metadata": {},
   "source": [
    "BSD 3-Clause License\n",
    "\n",
    "Copyright (c) 2024, Moez Dawood\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this\n",
    "   list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "   this list of conditions and the following disclaimer in the documentation\n",
    "   and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its\n",
    "   contributors may be used to endorse or promote products derived from\n",
    "   this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641b6dd",
   "metadata": {},
   "source": [
    "# Code and Analysis for Defining and Reducing Variant Classification Disparities\n",
    "\n",
    "# Notebook Author: Moez Dawood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e598281",
   "metadata": {},
   "source": [
    "# Using this notebook:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6377c4",
   "metadata": {},
   "source": [
    "1. First clone and download the github repository. https://github.com/MoezDawood/ReducingVariantClassificationInequities.git\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Then download the input data\n",
    "\n",
    "    Go to https://drive.google.com/drive/folders/1sdp27bSK-N4LlDRI3Fmqu9N00xuIVxnx?usp=drive_link\n",
    "\n",
    "    In the google drive there will be two folders: requiredinputs and exampleoutput. Put all the files without changing their names from the requiredinputs folder into the inputs folder (from the cloned and downloaded github repo). The exampleoutput folder is for comparison. Total download size for all the files is about ~12.5 GB.\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "3. Run notebook from top to bottom in order. If the entire github repository is downloaded and the contents are kept in the same order then you can click the 'Run all below' button.\n",
    "\n",
    "\n",
    "4. Python version 3.7.7 was used to run this notebook.\n",
    "\n",
    "\n",
    "5. The R version is R version 4.2.2 (2022-10-31). This is very pertinent for the rpy2 installation. If possible install rpy2 v3.5.7 and ggstatsplots v0.12.1\n",
    "\n",
    "\n",
    "6. Please make sure to use a similar and compatible version of python otherwise some packages may be outdated or function differently.\n",
    "\n",
    "\n",
    "7. Will take around 8-9 hours to produce all figures and tables found in the paper (including main text figures and supplement and a few additional figures) for gnomAD v2.1.1 and gnomAD v3.1.2 (non v2). The figures and tables used in the paper can be found in-line as outputs of this jupyter notebook as well as in the output folder.\n",
    "\n",
    "\n",
    "8. Per the policy of the All of Us program, the code, inputs, and outputs for the All of Us analyses are in a declared workspace in the All of Us workbench. The code in the All of Us workspace is the same as below with modifications made for the specific data requirements of the All of Us platform. The workspace will be gladly and immediately shared with approved users upon request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216306d4",
   "metadata": {},
   "source": [
    "## Table Of Contents: <a class=\"anchor\" id=\"step-toc\"></a>\n",
    "\n",
    "* [Part 1: Setup](#part-1)\n",
    "* [Step 1](#step-1): Install and import all libraries. \n",
    "* [Step 2](#step-2): Set up hard coded variables\n",
    "* [Step 3](#step-3): Breakdown of the databases by ancestry\n",
    "* [Step 4](#step-4): Function to map all Clinical Significance calls into just 6 categories\n",
    "* [Step 5](#step-5): Import Input Data from gnomAD\n",
    "* [Step 6](#step-6): Set Main Dictionaries\n",
    "* [Step 7](#step-7): Breakdown Databases By Medical Specialty and Variant Type\n",
    "* [Step 8](#step-8): Filter out non-clinical genes for SGE and VAMPSEQ sets\n",
    "* [Step 9](#step-9): Establish input dictionaries of the different gene sets\n",
    "<br><br>\n",
    "* [Part 2: Gene by Gene Statistical Testing (Wilcoxon singed-rank, matched-pairs test)](#part-2)\n",
    "* [Step 10](#step-10): Function for Filtering by Variant Type\n",
    "* [Step 11](#step-11): Functions for Grouping Genes\n",
    "* [Step 12](#step-12): Functions for Calculating Allele Prevalence\n",
    "* [Step 13](#step-13): Functions for Subsetting Needed Columns\n",
    "* [Step 14](#step-14): Functions for Combining Allele Counts\n",
    "* [Step 15](#step-15): Trigger Functions For Setting Up Plotting Dataframe\n",
    "* [Step 16](#step-16): Miscellaneous Helper Functions\n",
    "* [Step 17](#step-17): Filtering Zeroes For Plotting\n",
    "* [Step 18](#step-18): Estimating Statistical Power\n",
    "* [Step 19](#step-19): Functions to Generate Plots\n",
    "* [Step 20](#step-20): Functions For Stats in R\n",
    "* [Step 21](#step-21): Functions to Trigger Generating Combination Boxplots\n",
    "<br><br>\n",
    "* [Part 3: Figures for Gene by Gene Comparisons](#part-3) \n",
    "* [Step 22](#step-22): Population Breakdown Plots\n",
    "* [Step 23](#step-23): Variant Type Breakdown Plots\n",
    "* [Step 24](#step-24): Allele Prevalence and Clinical Significance Breakdown Plots\n",
    "* [Step 25](#step-25): Make Forest Plots for Effect Size Comparisons\n",
    "* [Step 26](#step-26): Bar Graphs of Allele Prevalence\n",
    "<br><br>\n",
    "* [Part 4: Unique Variant Examination (Orthogonal Chi-Square Test)](#part-4)\n",
    "* [Step 27](#step-27): Functions to Set Up Unique Variants\n",
    "* [Step 28](#step-28): Functions to Set Up Bar Graphs and Orthogonal Chi-Square Tests\n",
    "* [Step 29](#step-29): Plotting Bar Graphs and Orthogonal Chi-Square Tests\n",
    "* [Step 30](#step-30): Bar Graphs of Top Genes\n",
    "<br><br>\n",
    "* [Part 5: Variant Reclassification Setup](#part-5)\n",
    "* [Step 31](#step-31): Import input data specifically for variant reclassification\n",
    "* [Step 32](#step-32): Filling in Evidence Codes based on 2015 Criteria\n",
    "* [Step 33](#step-33): Import MAVE data\n",
    "* [Step 34](#step-34): General functions for variant reclassification\n",
    "<br><br>\n",
    "* [Part 6: ClinGen VCEP Gene-specific Criteria Specifications for VUS Reclassification](#part-6)\n",
    "* [Step 35](#step-35): Updating Final Decision Trees of Evidence Codes\n",
    "* [Step 36](#step-36): Updating BRCA1 evidence codes\n",
    "* [Step 37](#step-37): BRCA1 Reclassifications\n",
    "* [Step 38](#step-38): Updating TP53 evidence codes\n",
    "* [Step 39](#step-39): TP53 Reclassifications\n",
    "* [Step 40](#step-40): Updating PTEN evidence codes\n",
    "* [Step 41](#step-41): PTEN Reclassifications\n",
    "<br><br>\n",
    "* [Part 7: Sankey Flow Diagrams of VUS Reclassification](#part-7)\n",
    "* [Step 42](#step-42): Functions for Sankey flow visuals of variant reclassification\n",
    "* [Step 43](#step-43): Sankey Flow Visuals for BRCA1\n",
    "* [Step 44](#step-44): Sankey Flow Visuals for TP53\n",
    "* [Step 45](#step-45): Sankey Flow Visuals for PTEN\n",
    "<br><br>\n",
    "* [Part 8: Overall VUS Reclassifications](#part-8)\n",
    "* [Step 46](#step-46): Functions for Combining Variant Dataframes for Sankey Flow Visuals\n",
    "* [Step 47](#step-47): Combined Sankey Flow Visuals for Each Gene\n",
    "* [Step 48](#step-48): Combined Sankey Flow Visuals for Each Database\n",
    "* [Step 49](#step-49): Combined Sankey Flow Visuals for Combined Genes and Databases\n",
    "* [Step 50](#step-50): VUS Reclassifications Bar Graph\n",
    "<br><br>\n",
    "* [Part 9: Evidence Code Analysis](#part-9)\n",
    "* [Step 51](#step-51): Functions for Evidence Code Breakdown\n",
    "* [Step 52](#step-52): Plotting Evidence Code Frequencies\n",
    "* [Step 53](#step-53): Paired Frequencies of Evidence Codes Based on Superpopulation Grouping\n",
    "* [Step 54](#step-54): Functions for Quantifying Essential Evidence Codes\n",
    "* [Step 55](#step-55): Plotting Evidence Code Breakdown with Essential Codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6be274",
   "metadata": {},
   "source": [
    "# Part 1: <a class=\"anchor\" id=\"part-1\"></a>Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c88ea",
   "metadata": {},
   "source": [
    "# Step 1: <a class=\"anchor\" id=\"step-1\"></a> Make sure all required libraries are installed. Install and import all libraries. required_libraries is hardcoded and needs to be updated for new packages.\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712f906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install importlib\n",
    "!pip install subprocess.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install above libraries\n",
    "#use importlib and subpocess to check if all other appropriate libs and installed\n",
    "#if not then install those libraries\n",
    "#also import all the libraries and subprocesses\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def check_and_import_libraries():\n",
    "    required_libraries = [\n",
    "        (\"pandas\", \"pd\"),\n",
    "        (\"os\", \"os\"),\n",
    "        (\"numpy\", \"np\"),\n",
    "        (\"matplotlib\", \"matplotlib\"),\n",
    "        (\"matplotlib.font_manager\", \"fm\"),\n",
    "        (\"matplotlib.pyplot\", \"plt\"),\n",
    "        (\"re\", \"re\"),\n",
    "        (\"scipy.stats\", \"scipy.stats\"),\n",
    "        (\"itertools\", \"itertools\"),\n",
    "        (\"statannotations\", \"statannotations\"),\n",
    "        (\"statsmodels\", \"statsmodels\"),\n",
    "        (\"statsmodels.stats.multitest\", \"multipletests\"),\n",
    "        (\"matplotlib.ticker\", \"matplotlib.ticker\"),\n",
    "        (\"bokeh.plotting\", \"bokeh_plotting\"),\n",
    "        (\"seaborn\", \"sns\"),\n",
    "        (\"pptx\", \"Presentation\"),\n",
    "        (\"pptx.util\", \"Inches\"),\n",
    "        (\"math\", \"math\"),\n",
    "        (\"blosum\", \"bl\"),\n",
    "        (\"csv\", \"csv\"),\n",
    "        (\"requests\", \"requests\"),\n",
    "        (\"xml.etree.ElementTree\", \"ET\"),\n",
    "        (\"platform\", \"platform\"),\n",
    "        (\"pathlib\", \"Path\"),\n",
    "        (\"fastaparser\", \"fastaparser\"),\n",
    "        (\"liftover\", \"liftover\"),\n",
    "        (\"json\", \"json\"),\n",
    "        (\"xmltodict\", \"xmltodict\"),\n",
    "        ('urllib','urllib'),\n",
    "        (\"hgvs\", \"hgvs\"),\n",
    "        (\"hgvs.easy\", \"hgvs_easy\"),   \n",
    "        (\"urllib.request\", \"urllib.request\"),\n",
    "        (\"warnings\", \"warnings\"),\n",
    "        ('rpy2','rpy2'),\n",
    "        ('ast','ast'),\n",
    "        ('matplotlib_venn', 'matplotlib_venn'),\n",
    "        ('kaleido', 'kaleido'),\n",
    "        ('nbformat', 'nbformat'),\n",
    "        ('plotly.io', 'pio'),\n",
    "        ('sys','sys'),\n",
    "        (\"matplotlib.ticker\", \"mtick\"),\n",
    "        ('plotly.graph_objects', 'go'),\n",
    "        ('statsmodels.stats.power','statsmodels.stats.power')\n",
    "    ]\n",
    "\n",
    "    missing_libraries = []\n",
    "\n",
    "    for lib, alias in required_libraries:\n",
    "        try:\n",
    "            importlib.import_module(lib)\n",
    "        except ImportError:\n",
    "            missing_libraries.append((lib, alias))\n",
    "\n",
    "    if missing_libraries:\n",
    "        print(\"The following libraries are missing and will be installed:\")\n",
    "        for lib, alias in missing_libraries:\n",
    "            print(f\"- {lib} (as {alias})\")\n",
    "            subprocess.call([\"pip\", \"install\", lib])\n",
    "\n",
    "        # Attempt to import the libraries again\n",
    "        for lib, alias in missing_libraries:\n",
    "            try:\n",
    "                importlib.import_module(lib)\n",
    "            except ImportError:\n",
    "                print(f\"Failed to install {lib}. Please install it manually.\")\n",
    "                return\n",
    "\n",
    "    print(\"All required libraries are installed.\")\n",
    "\n",
    "    # Use globals() to make aliases available in the global namespace\n",
    "    for lib, alias in required_libraries:\n",
    "        globals()[alias] = importlib.import_module(lib)\n",
    "\n",
    "check_and_import_libraries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf378bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code block is redundant as the above function should import all needed libraries and subprocesses\n",
    "#but this code block is easier to work with when building the notebook and getting a sense of everything needed\n",
    "#imports all the libraries and subprocesses\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import mannwhitneyu, normaltest\n",
    "import statannotations\n",
    "import statsmodels\n",
    "from statannotations.Annotator import Annotator\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from bokeh.plotting import figure, show\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import blosum as bl\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import fastaparser\n",
    "from collections import Counter\n",
    "from liftover import get_lifter\n",
    "import json\n",
    "import xmltodict\n",
    "import hgvs \n",
    "from hgvs.easy import *\n",
    "from urllib.request import urlopen\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter, rpy2py\n",
    "from rpy2.robjects.packages import importr\n",
    "from nbformat import read\n",
    "import ast\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "import urllib\n",
    "import plotly.io as pio\n",
    "import kaleido\n",
    "import math\n",
    "import requests, sys\n",
    "from statsmodels.stats.power import GofChisquarePower\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import rankdata\n",
    "from matplotlib_venn import venn2\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7068f",
   "metadata": {},
   "source": [
    "# Step 2: <a class=\"anchor\" id=\"step-2\"></a> Set up hard coded variables including variables from gnomAD for number of individuals sequenced from each population (using same population descriptors assigned by gnomAD)\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of ancestry group numbers for calculating allele count per individual\n",
    "\n",
    "#Numbers from table at https://gnomad.broadinstitute.org/help/what-populations-are-represented-in-the-gnomad-data\n",
    "#Picture of table in github repository as well (picture named )\n",
    "\n",
    "# 59,106 Non-Europeans vs. 64,603 Europeans for gnomad v2\n",
    "ancestry_v2 = {\n",
    "    'African/African American': 12487,\n",
    "    'Latino/Admixed American': 17720,\n",
    "    'East Asian': 9977,\n",
    "    'European (non-Finnish)': 64603,\n",
    "    'South Asian': 15308,\n",
    "    'Other': 3614}\n",
    "\n",
    "#For gnomad v3 there is a 14% overlap of individuals with gnomad v2\n",
    "#Thus we are using the v3 non v2 data that has removed the 14% overlap\n",
    "#Thus the v2 and v3nonv2 datasets are independent of each other and basically represent two different population databases\n",
    "# 25,547 Non-European vs. 25,988 European for gnomad v3 non v2\n",
    "ancestry_v3nonv2 = {\n",
    "    'African/African American': 14377,\n",
    "    'Latino/Admixed American': 6878,\n",
    "    'East Asian': 1414,\n",
    "    'European (non-Finnish)': 25988,\n",
    "    'South Asian': 1946,\n",
    "    'Other': 932}\n",
    "\n",
    "ancestry_aou = {\n",
    "    'African/African American': 53944,\n",
    "    'Latino/Admixed American': 40838,\n",
    "    'East Asian': 5381,\n",
    "    'European': 123072,\n",
    "    'South Asian': 2342,\n",
    "    'Other': 19289\n",
    "}\n",
    "\n",
    "\n",
    "totalafrcount = ancestry_v2['African/African American'] + ancestry_v3nonv2['African/African American'] + ancestry_aou['African/African American']\n",
    "totalsascount = ancestry_v2['South Asian'] + ancestry_v3nonv2['South Asian'] + ancestry_aou['South Asian']\n",
    "totaleascount = ancestry_v2['East Asian'] + ancestry_v3nonv2['East Asian'] + ancestry_aou['East Asian']\n",
    "totalamrcount = ancestry_v2['Latino/Admixed American'] + ancestry_v3nonv2['Latino/Admixed American'] + ancestry_aou['Latino/Admixed American']\n",
    "totalothcount = ancestry_v2['Other'] + ancestry_v3nonv2['Other'] + ancestry_aou['Other'] + 528 #ancestry_aou['Middle Eastern']\n",
    "totalnoneurocount = totalafrcount + totalsascount + totaleascount + totalamrcount + totalothcount\n",
    "#print(totalnoneurocount)\n",
    "totaleurocount = ancestry_v2['European (non-Finnish)'] + ancestry_v3nonv2['European (non-Finnish)'] + ancestry_aou['European']\n",
    "#print(totaleurocount)\n",
    "\n",
    "\n",
    "#VEP Annotations in gnomAD for different variant types\n",
    "coding = ['stop_lost', \n",
    "          'frameshift_variant', \n",
    "          'synonymous_variant', \n",
    "          'missense_variant', \n",
    "          'stop_gained', \n",
    "          'inframe_deletion', \n",
    "          'splice_region_variant', \n",
    "          'splice_donor_variant',\n",
    "          'start_lost', \n",
    "          'splice_acceptor_variant', \n",
    "          'inframe_insertion',\n",
    "          'coding_sequence_variant', \n",
    "          'protein_altering_variant',\n",
    "          'stop_retained_variant', \n",
    "          'incomplete_terminal_codon_variant']\n",
    "\n",
    "coding1 = [ \n",
    "    'missense_variant', \n",
    "    'synonymous_variant',\n",
    "    'stop_gained', \n",
    "    'stop_lost',\n",
    "    'stop_retained_variant',\n",
    "    'splice_region_variant', \n",
    "    'splice_donor_variant',\n",
    "    'splice_acceptor_variant',\n",
    "    'frameshift_variant',\n",
    "    'inframe_deletion', \n",
    "    'inframe_insertion',\n",
    "    'inframe',\n",
    "    'start_lost']\n",
    "\n",
    "codingwithoutmissense = ['stop_lost', \n",
    "          'frameshift_variant', \n",
    "          'synonymous_variant', \n",
    "          'stop_gained', \n",
    "          'inframe_deletion', \n",
    "          'splice_region_variant', \n",
    "          'splice_donor_variant',\n",
    "          'start_lost', \n",
    "          'splice_acceptor_variant', \n",
    "          'inframe_insertion',\n",
    "          'coding_sequence_variant', \n",
    "          'protein_altering_variant',\n",
    "          'stop_retained_variant', \n",
    "          'incomplete_terminal_codon_variant']\n",
    "\n",
    "missense = ['missense_variant']\n",
    "frameshift = ['frameshift_variant']\n",
    "synonymous = ['synonymous_variant']\n",
    "inframes = ['inframe_deletion', 'inframe_insertion']\n",
    "stops = ['stop_lost','stop_gained', 'stop_retained_variant', 'incomplete_terminal_codon_variant']\n",
    "canonicalsplice = ['splice_donor_variant', 'splice_acceptor_variant']\n",
    "spliceregion = ['splice_region_variant']\n",
    "\n",
    "noncoding = ['5_prime_UTR_variant', '3_prime_UTR_variant', 'intron_variant'] \n",
    "\n",
    "varianttypemappingcoding = {\n",
    "    'coding' : coding,\n",
    "    'codingwithoutmissense' : codingwithoutmissense,\n",
    "    'missense' : missense,\n",
    "    'frameshift' : frameshift,\n",
    "    'synonymous' : synonymous,\n",
    "    'inframes' : inframes,\n",
    "    'stops' : stops,\n",
    "    'canonicalsplice' : canonicalsplice}\n",
    "\n",
    "# varianttypemappingnoncoding = {\n",
    "#     'noncoding' : noncoding,\n",
    "#     'spliceregion' : spliceregion\n",
    "# }\n",
    "\n",
    "varianttypemappingforpaper = {\n",
    "    'coding' : coding,\n",
    "    'codingwithoutmissense' : codingwithoutmissense,\n",
    "    'noncoding' : noncoding\n",
    "}\n",
    "\n",
    "varianttypemappingall = {\n",
    "    'coding' : coding1,\n",
    "    'noncoding' : noncoding,\n",
    "\n",
    "}\n",
    "\n",
    "varianttypemappingjustcoding = {\n",
    "    'coding' : coding\n",
    "}\n",
    "\n",
    "\n",
    "varianttypemappingnoncoding = {\n",
    "    'noncoding' : noncoding\n",
    "}\n",
    "\n",
    "variantmappingdictforylabels = {\n",
    "    'stop_lost' : 'Stop Lost', \n",
    "    'frameshift_variant' : 'Frameshift', \n",
    "    'synonymous_variant' : 'Synonymous', \n",
    "    'missense_variant' : 'Missense', \n",
    "    'stop_gained' : 'Stop Gain', \n",
    "    'inframe_deletion' : 'Inframe Deletion', \n",
    "    'splice_region_variant' : 'Splice Region', \n",
    "    'splice_donor_variant' : 'Splice Donor',\n",
    "    'start_lost' : 'Start Lost', \n",
    "    'splice_acceptor_variant' : 'Splice Acceptor', \n",
    "    'inframe_insertion' : 'Inframe Insertion',\n",
    "    'stop_retained_variant' : 'Stop Retained',\n",
    "    '3_prime_UTR_variant' : \"3′ UTR\", \n",
    "    'intron_variant' : 'Intronic', \n",
    "    '5_prime_UTR_variant' : \"5′ UTR\",\n",
    "    'inframe' : 'Inframe'\n",
    "    \n",
    "}\n",
    "#Current inputs are already prepopulated for the two superpopulation groups below in the input files\n",
    "popdescrips2 = ['African/African American', 'Latino/Admixed American', 'East Asian', 'South Asian', 'Other']\n",
    "popdescrips1 = ['European (non-Finnish)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8f37e",
   "metadata": {},
   "source": [
    "# Step 3: <a class=\"anchor\" id=\"step-3\"></a> Breakdown of the databases by genetic ancestry\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09498493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_graph(dictionaries, x_labels_mapping=None, colors=None, legend_labels=None, title=\"Horizontal Bar Graph\", x_label=\"Count (in thousands)\", y_label=\"Categories\"):\n",
    "    # Extract keys and values from the dictionaries\n",
    "    categories = [list(dic.keys()) for dic in dictionaries]\n",
    "    counts = [list(dic.values()) for dic in dictionaries]\n",
    "    num_categories = len(categories[0])\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "    # Set the height of the bars\n",
    "    bar_height = 0.8 / max(len(dictionaries), 1)\n",
    "\n",
    "    # Create figure and axis objects\n",
    "    fig, ax = plt.subplots()\n",
    "    # Hide the top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Define the y positions for the bars\n",
    "    y = np.arange(num_categories)\n",
    "\n",
    "    bars = []\n",
    "    for i, (category, count) in enumerate(zip(categories, counts)):\n",
    "        # Shift the y positions for the bars in each dictionary\n",
    "        if len(dictionaries) > 1:\n",
    "            y_shifted = y + i * bar_height - bar_height / 2\n",
    "        else:\n",
    "            y_shifted = y\n",
    "\n",
    "        # Create the bars for each dictionary with specified color or default\n",
    "        color = colors[i] if colors and i < len(colors) else None\n",
    "        bar = ax.barh(y_shifted, count, bar_height, label=legend_labels[i] if legend_labels and i < len(legend_labels) else f'Dictionary {i+1}', color=color, edgecolor='black')\n",
    "        bars.append(bar)\n",
    "\n",
    "    # Set labels, title, and legend\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Replace x-axis labels if x_labels_mapping is provided\n",
    "    if x_labels_mapping:\n",
    "        print(categories[0])\n",
    "        new_labels = [x_labels_mapping[label] for label in categories[0]]\n",
    "        ax.set_yticklabels([], rotation=0, ha='right')\n",
    "        for i, label in enumerate(new_labels):\n",
    "            # Set the labels at appropriate positions\n",
    "            ax.text(-1000, i, label, ha='right', va='center')\n",
    "        print(new_labels)\n",
    "        ax.set_ylabel(y_label, labelpad=150)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the counts to the right of the bars\n",
    "    label_padding = 3  \n",
    "    for bar in bars:\n",
    "        for b in bar:\n",
    "            width = b.get_width()\n",
    "            label_x_pos = width + label_padding + 1000  \n",
    "            ax.annotate('{}'.format(format(int(width), ',')),\n",
    "                        xy=(label_x_pos, b.get_y() + b.get_height() / 2),\n",
    "                        xytext=(0, 0),  \n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='left', va='center')\n",
    "\n",
    "    # Customize the x-axis labels to display values divided by 1000\n",
    "    def x_axis_formatter(value, _):\n",
    "        return f'{int(value / 1000)}K'\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(x_axis_formatter))\n",
    "\n",
    "    # To make the graph more visually appealing and provide space at the right\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=2.0)  \n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels_mapping = {'Other': 'Other', \n",
    "                    'South Asian': 'South Asian-like',\n",
    "                    'European (non-Finnish)': 'European (non-Finnish)-like',\n",
    "                    'European': 'European-like',\n",
    "                    'East Asian': 'East Asian-like',\n",
    "                    'Latino/Admixed American': 'Latino/Admixed American-like', \n",
    "                    'African/African American': 'African/African American-like'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify custom colors for each dictionary\n",
    "custom_colors = ['red','purple']\n",
    "\n",
    "# Specify legend labels for each dictionary\n",
    "legend_labels = ['gnomAD v2.1.1', 'gnomAD v3.1.2']\n",
    "\n",
    "plot_bar_graph(dictionaries = [ancestry_v2, ancestry_v3nonv2],\n",
    "               x_labels_mapping=x_labels_mapping,\n",
    "               colors=custom_colors, \n",
    "               legend_labels=legend_labels, \n",
    "               title=\"Distribution of Individuals Sequenced between gnomAD v2.1.1 vs v3.1.2\", \n",
    "               y_label=\"Population Descriptors Prescribed by gnomAD\", \n",
    "               x_label=\"Number of Individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify custom colors for each dictionary\n",
    "custom_colors = ['blue','green','red']\n",
    "\n",
    "# Specify legend labels for each dictionary\n",
    "legend_labels = ['All of Us v7', 'gnomAD v2.1.1', 'gnomAD v3.1.2 (non v2)', ]\n",
    "\n",
    "plot_bar_graph(dictionaries = [ancestry_aou, ancestry_v2, ancestry_v3nonv2],\n",
    "               x_labels_mapping=x_labels_mapping,\n",
    "               colors=custom_colors, \n",
    "               legend_labels=legend_labels, \n",
    "               title=\"Distribution of Individuals in All of Us and gnomAD\", \n",
    "               x_label=\"Number of Individuals\", \n",
    "               y_label=\"Genetic Ancestries Precomputed by All of Us or gnomAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458917fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify custom colors for each dictionary\n",
    "custom_colors = ['blue']\n",
    "\n",
    "# Specify legend labels for each dictionary\n",
    "legend_labels = ['All of Us v7']\n",
    "\n",
    "plot_bar_graph([ancestry_aou], \n",
    "               x_labels_mapping=x_labels_mapping,\n",
    "               colors=custom_colors, \n",
    "               legend_labels=legend_labels, \n",
    "               title=\"Distribution of Individuals Sequenced in All of Us v7\", \n",
    "               y_label=\"Population Descriptors Prescribed by All of Us\", \n",
    "               x_label=\"Number of Individuals Sequenced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41515925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_charts(dictionaries, legend_labels=None, title=\"Pie Charts\"):\n",
    "    # Number of charts to plot\n",
    "    num_charts = len(dictionaries)\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    # Create figure and axis objects\n",
    "    fig, axs = plt.subplots(num_charts, 1, figsize=(5, 5 * num_charts), constrained_layout=True)\n",
    "\n",
    "    # If there's only one chart, axs will not be a list, so we wrap it in a list for consistent indexing\n",
    "    if num_charts == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, dic in enumerate(dictionaries):\n",
    "        categories = list(dic.keys())\n",
    "        counts = list(dic.values())\n",
    "\n",
    "        # Decrease the pctdistance to move labels closer to the pie\n",
    "        wedges, texts, autotexts = axs[i].pie(counts, labels=None, autopct='%1.1f%%', startangle=90, pctdistance=1.15, colors=plt.cm.Paired(range(len(categories))), wedgeprops=dict(edgecolor='black'))\n",
    "\n",
    "        # Add a legend on the side with color-coded labels\n",
    "        axs[i].legend(wedges, categories, title=\"Populations\", loc=\"center left\", bbox_to_anchor=(1, 0.6))\n",
    "        \n",
    "        # Set equal aspect ratio to ensure that pie is drawn as a circle.\n",
    "        axs[i].axis('equal')\n",
    "\n",
    "        axs[i].set_title(legend_labels[i] if legend_labels and i < len(legend_labels) else f'Chart {i+1}')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=1)  \n",
    "    plt.subplots_adjust(top=1.1)\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "legend_labels = ['All of Us v7', 'gnomAD v2.1.1', 'gnomAD v3.1.2 (non v2)']\n",
    "\n",
    "plot_pie_charts([ancestry_aou, ancestry_v2, ancestry_v3nonv2], \n",
    "                legend_labels=legend_labels, \n",
    "                title=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce456e0",
   "metadata": {},
   "source": [
    "# Step 4: <a class=\"anchor\" id=\"step-4\"></a> Function to map all Clinical Significance calls into just 6 categories\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map clinvar values for gnomAD\n",
    "def replace_clinvar_values(df):\n",
    "    replace_dict = {\n",
    "        \"Affects\": 'Not Included',\n",
    "        \"Benign\": \"Benign or Likely Benign\",\n",
    "        \"Benign/Likely benign\": \"Benign or Likely Benign\",\n",
    "        \"Benign/Likely benign; other\": \"Benign or Likely Benign\",\n",
    "        \"Benign/Likely benign; risk factor\": \"Benign or Likely Benign\",\n",
    "        \"Benign; other\": \"Benign or Likely Benign\",\n",
    "        \"Likely benign; other\": \"Benign or Likely Benign\",\n",
    "        \"Conflicting interpretations of pathogenicity\": \"Conflicting Interpretations\",\n",
    "        \"Conflicting interpretations of pathogenicity; association; risk factor\": \"Conflicting Interpretations\",\n",
    "        \"Conflicting interpretations of pathogenicity; other\": \"Conflicting Interpretations\",\n",
    "        \"Conflicting interpretations of pathogenicity; other; risk factor\": \"Conflicting Interpretations\",\n",
    "        \"Conflicting interpretations of pathogenicity; risk factor\": \"Conflicting Interpretations\",\n",
    "        \"Likely benign\": \"Benign or Likely Benign\",\n",
    "        \"Likely pathogenic; risk factor\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Likely pathogenic\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Likely pathogenic; drug response\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic/Likely pathogenic/Pathogenic, low penetrance\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic/Likely pathogenic/Pathogenic, low penetrance; other\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic; Affects\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"not provided\": \"Not Included\",\n",
    "        \"drug response\": \"Not Included\",\n",
    "        \"association\": \"Not Included\",\n",
    "        \"other\": \"Not Included\",\n",
    "        \"risk factor\": \"Not Included\",\n",
    "        \"Uncertain significance/Uncertain risk allele\": \"Variant of Uncertain Significance\",\n",
    "        \"Uncertain significance\": \"Variant of Uncertain Significance\",\n",
    "        \"Uncertain significance; association\": \"Variant of Uncertain Significance\",\n",
    "        \"Pathogenic; drug response\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic/Likely risk allele\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic/Likely pathogenic; other\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic/Likely pathogenic\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Pathogenic\": \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Likely risk allele\": \"Not Included\",\n",
    "        \"Benign/Likely benign\": \"Benign or Likely Benign\",\n",
    "        np.nan: \"No Designation\"  # replace empty cells with No Designation\n",
    "    }\n",
    "    df[\"Original ClinVar Clinical Significance\"] = df[\"ClinVar Clinical Significance\"]\n",
    "    df[\"ClinVar Clinical Significance\"] = df[\"ClinVar Clinical Significance\"].replace(replace_dict)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b4b2f",
   "metadata": {},
   "source": [
    "# Step 5: <a class=\"anchor\" id=\"step-5\"></a>Import Input Data from gnomAD\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACMG78_gnomadv2_link = \"https://drive.google.com/file/d/1jTXs5by8LrwNgav8QsJEZhLnzMyiEeJx/view?usp=sharing\"\n",
    "ACMG78_gnomadv2_destination = \"inputs/ACMG78_gnomadv2.csv\"\n",
    "ACMG78_gnomadv2 = pd.read_csv(ACMG78_gnomadv2_destination)\n",
    "print('ACMG78_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba345fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACMG78_gnomadv3_link = \"https://drive.google.com/file/d/1DwsWTbeWJ2ENm8v_JpCPn-Ed2GdG10w9/view?usp=sharing\"\n",
    "ACMG78_gnomadv3_destination = \"inputs/ACMG78_gnomadv3.csv\"\n",
    "ACMG78_gnomadv3 = pd.read_csv(ACMG78_gnomadv3_destination)\n",
    "print('ACMG78_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AllergyandImmunology_gnomadv2_link = \"https://drive.google.com/file/d/1cDpdy_llNQ4SHa8we9UnWm7Pdq0d8-iT/view?usp=sharing\"\n",
    "AllergyandImmunology_gnomadv2_destination = \"inputs/AllergyandImmunology_gnomadv2.csv\"\n",
    "AllergyandImmunology_gnomadv2 = pd.read_csv(AllergyandImmunology_gnomadv2_destination)\n",
    "print('AllergyandImmunology_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48282690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AllergyandImmunology_gnomadv3_link = \"https://drive.google.com/file/d/1USxPdQVHEnxCdfIarDYU5904CGoOQ-wy/view?usp=sharing\"\n",
    "AllergyandImmunology_gnomadv3_destination = \"inputs/AllergyandImmunology_gnomadv3.csv\"\n",
    "AllergyandImmunology_gnomadv3 = pd.read_csv(AllergyandImmunology_gnomadv3_destination)\n",
    "print('AllergyandImmunology_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cancer_gnomadv2_link = \"https://drive.google.com/file/d/1q9rZiS9N5BxZZsLm2LU4hpFth1ebyzgm/view?usp=sharing\"\n",
    "Cancer_gnomadv2_destination = \"inputs/Cancer_gnomadv2.csv\"\n",
    "Cancer_gnomadv2 = pd.read_csv(Cancer_gnomadv2_destination)\n",
    "print('Cancer_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cancer_gnomadv3_link = \"https://drive.google.com/file/d/1W0qcgiD_YFcoGb_xAPSzIk2hYoZmC4K3/view?usp=sharing\"\n",
    "Cancer_gnomadv3_destination = \"inputs/Cancer_gnomadv3.csv\"\n",
    "Cancer_gnomadv3 = pd.read_csv(Cancer_gnomadv3_destination)\n",
    "print('Cancer_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cardiac_gnomadv2_link = \"https://drive.google.com/file/d/1zzjr0McaOcfqif-5V6v374gOgjsXMPIK/view?usp=sharing\"\n",
    "Cardiac_gnomadv2_destination = \"inputs/Cardiac_gnomadv2.csv\"\n",
    "Cardiac_gnomadv2 = pd.read_csv(Cardiac_gnomadv2_destination)\n",
    "print('Cardiac_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cardiac_gnomadv3_link = \"https://drive.google.com/file/d/1i92zp1aQcpLPwVJj1HP41iAgzxObJ2QA/view?usp=sharing\"\n",
    "Cardiac_gnomadv3_destination = \"inputs/Cardiac_gnomadv3.csv\"\n",
    "Cardiac_gnomadv3 = pd.read_csv(Cardiac_gnomadv3_destination)\n",
    "print('Cardiac_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10998448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrier_gnomadv2_link = \"https://drive.google.com/file/d/1hVl1Al3s0RtFE0vhOb9UY8LpeJnEMvmJ/view?usp=sharing\"\n",
    "Carrier_gnomadv2_destination = \"inputs/Carrier_gnomadv2.csv\"\n",
    "Carrier_gnomadv2 = pd.read_csv(Carrier_gnomadv2_destination)\n",
    "print('Carrier_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrier_gnomadv3_link = \"https://drive.google.com/file/d/1ldiTWacBeox_c-JgUlufafSs0c_vGv9t/view?usp=sharing\"\n",
    "Carrier_gnomadv3_destination = \"inputs/Carrier_gnomadv3.csv\"\n",
    "Carrier_gnomadv3 = pd.read_csv(Carrier_gnomadv3_destination)\n",
    "print('Carrier_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDG2P_gnomadv2_link = \"https://drive.google.com/file/d/1IKTld4p4Pq-qFrThgJ5sTzWPJJ9MlAYQ/view?usp=sharing\"\n",
    "DDG2P_gnomadv2_destination = \"inputs/DDG2P_gnomadv2.csv\"\n",
    "DDG2P_gnomadv2 = pd.read_csv(DDG2P_gnomadv2_destination)\n",
    "print('DDG2P_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDG2P_gnomadv3_link = \"https://drive.google.com/file/d/1tsI_1xj_Ap5BWAA4V_Uj2ntUZC_DGmBe/view?usp=sharing\"\n",
    "DDG2P_gnomadv3_destination = \"inputs/DDG2P_gnomadv3.csv\"\n",
    "DDG2P_gnomadv3 = pd.read_csv(DDG2P_gnomadv3_destination)\n",
    "print('DDG2P_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endocrinology_gnomadv2_link = \"https://drive.google.com/file/d/1IWMPkYErcZGsROibEs3qyosNGubi4wPq/view?usp=sharing\"\n",
    "Endocrinology_gnomadv2_destination = \"inputs/Endocrinology_gnomadv2.csv\"\n",
    "Endocrinology_gnomadv2 = pd.read_csv(Endocrinology_gnomadv2_destination)\n",
    "print('Endocrinology_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce17aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endocrinology_gnomadv3_link = \"https://drive.google.com/file/d/1Bb8QLk-UO3NKV7z3faaDHEbg6YeYz-Qv/view?usp=sharing\"\n",
    "Endocrinology_gnomadv3_destination = \"inputs/Endocrinology_gnomadv3.csv\"\n",
    "Endocrinology_gnomadv3 = pd.read_csv(Endocrinology_gnomadv3_destination)\n",
    "print('Endocrinology_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC_gnomadv2_link = \"https://drive.google.com/file/d/1vsFFlEgSKbx1fbzFA4HpHGR3wM1FQtIf/view?usp=sharing\"\n",
    "GenCC_gnomadv2_destination = \"inputs/GenCC_gnomadv2.csv\"\n",
    "GenCC_gnomadv2 = pd.read_csv(GenCC_gnomadv2_destination)\n",
    "print('GenCC_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC_gnomadv3_link = \"https://drive.google.com/file/d/172CIFNbevP8czyykiu5XCveg7ssM6sak/view?usp=sharing\"\n",
    "GenCC_gnomadv3_destination = \"inputs/GenCC_gnomadv3.csv\"\n",
    "GenCC_gnomadv3 = pd.read_csv(GenCC_gnomadv3_destination)\n",
    "print('GenCC_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afc4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hematology_gnomadv2_link = \"https://drive.google.com/file/d/1PdILV35u5TULpz1xXPbFuM3ldn7yRRD8/view?usp=sharing\"\n",
    "Hematology_gnomadv2_destination = \"inputs/Hematology_gnomadv2.csv\"\n",
    "Hematology_gnomadv2 = pd.read_csv(Hematology_gnomadv2_destination)\n",
    "print('Hematology_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hematology_gnomadv3_link = \"https://drive.google.com/file/d/1DkIKE2h3Gu7ktvOy00VGM-EejusJQuvi/view?usp=sharing\"\n",
    "Hematology_gnomadv3_destination = \"inputs/Hematology_gnomadv3.csv\"\n",
    "Hematology_gnomadv3 = pd.read_csv(Hematology_gnomadv3_destination)\n",
    "print('Hematology_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67feddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InbornErrorsOfMetabolism_gnomadv2_link = \"https://drive.google.com/file/d/1p6KsIZOdsQ0ouQGNqpfzRG1vjFbL6Faz/view?usp=sharing\"\n",
    "InbornErrorsOfMetabolism_gnomadv2_destination = \"inputs/InbornErrorsOfMetabolism_gnomadv2.csv\"\n",
    "InbornErrorsOfMetabolism_gnomadv2 = pd.read_csv(InbornErrorsOfMetabolism_gnomadv2_destination)\n",
    "print('InbornErrorsOfMetabolism_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InbornErrorsOfMetabolism_gnomadv3_link = \"https://drive.google.com/file/d/1J1WGjuzta-nR7bImbxavgbK49RULILDG/view?usp=sharing\"\n",
    "InbornErrorsOfMetabolism_gnomadv3_destination = \"inputs/InbornErrorsOfMetabolism_gnomadv3.csv\"\n",
    "InbornErrorsOfMetabolism_gnomadv3 = pd.read_csv(InbornErrorsOfMetabolism_gnomadv3_destination)\n",
    "print('InbornErrorsOfMetabolism_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af955776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAVERegistry_gnomadv2_link = \"https://drive.google.com/file/d/17rC3Ky5kbi0Jp7X0cQPmXkw7Db4o-08/view?usp=sharing\"\n",
    "MAVERegistry_gnomadv2_destination = \"inputs/MAVERegistry_gnomadv2.csv\"\n",
    "MAVERegistry_gnomadv2 = pd.read_csv(MAVERegistry_gnomadv2_destination)\n",
    "print('MAVERegistry_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAVERegistry_gnomadv3_link = \"https://drive.google.com/file/d/1Gn-P_6ovCkkBU9HJaqCL4syLexwDejZR/view?usp=sharing\"\n",
    "MAVERegistry_gnomadv3_destination = \"inputs/MAVERegistry_gnomadv3.csv\"\n",
    "MAVERegistry_gnomadv3 = pd.read_csv(MAVERegistry_gnomadv3_destination)\n",
    "print('MAVERegistry_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newborn_gnomadv2_link = \"https://drive.google.com/file/d/14KzzvdiX1hCxmhmMDtHjhVEAPlEz6QHR/view?usp=sharing\"\n",
    "Newborn_gnomadv2_destination = \"inputs/Newborn_gnomadv2.csv\"\n",
    "Newborn_gnomadv2 = pd.read_csv(Newborn_gnomadv2_destination)\n",
    "print('Newborn_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59aa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newborn_gnomadv3_link = \"https://drive.google.com/file/d/1KSWKCib0h4rEe6KSM3dKQq9bUi4f9Oxg/view?usp=sharing\"\n",
    "Newborn_gnomadv3_destination = \"inputs/Newborn_gnomadv3.csv\"\n",
    "Newborn_gnomadv3 = pd.read_csv(Newborn_gnomadv3_destination)\n",
    "print('Newborn_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14257bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nephrology_gnomadv2_link = \"https://drive.google.com/file/d/1-X4k4ZZX5B5rpS89eMKIGIjvSTgsmrUC/view?usp=sharing\"\n",
    "Nephrology_gnomadv2_destination = \"inputs/Nephrology_gnomadv2.csv\"\n",
    "Nephrology_gnomadv2 = pd.read_csv(Nephrology_gnomadv2_destination)\n",
    "print('Nephrology_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nephrology_gnomadv3_link = \"https://drive.google.com/file/d/1kDqNAwE3PMu1PVCUCb5r53AXQAChsRJt/view?usp=sharing\"\n",
    "Nephrology_gnomadv3_destination = \"inputs/Nephrology_gnomadv3.csv\"\n",
    "Nephrology_gnomadv3 = pd.read_csv(Nephrology_gnomadv3_destination)\n",
    "print('Nephrology_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neurology_gnomadv2_link = \"https://drive.google.com/file/d/1ifnkuLttO3INh7jDQQ4wCEvHJdd4Pg-Z/view?usp=sharing\"\n",
    "Neurology_gnomadv2_destination = \"inputs/Neurology_gnomadv2.csv\"\n",
    "Neurology_gnomadv2 = pd.read_csv(Neurology_gnomadv2_destination)\n",
    "print('Neurology_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neurology_gnomadv3_link = \"https://drive.google.com/file/d/197U12zZo406Tn1T6bdoJA8koE3IwOOjL/view?usp=sharing\"\n",
    "Neurology_gnomadv3_destination = \"inputs/Neurology_gnomadv3.csv\"\n",
    "Neurology_gnomadv3 = pd.read_csv(Neurology_gnomadv3_destination)\n",
    "print('Neurology_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ophthalmology_gnomadv2_link = \"https://drive.google.com/file/d/1QGxa1uQCJf1CmLSYB-88-HE7uKq_yQnT/view?usp=sharing\"\n",
    "Ophthalmology_gnomadv2_destination = \"inputs/Ophthalmology_gnomadv2.csv\"\n",
    "Ophthalmology_gnomadv2 = pd.read_csv(Ophthalmology_gnomadv2_destination)\n",
    "print('Ophthalmology_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6721fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ophthalmology_gnomadv3_link = \"https://drive.google.com/file/d/16BWv4xedFz33AZ956Rb_EdMpc0s7laa6/view?usp=sharing\"\n",
    "Ophthalmology_gnomadv3_destination = \"inputs/Ophthalmology_gnomadv3.csv\"\n",
    "Ophthalmology_gnomadv3 = pd.read_csv(Ophthalmology_gnomadv3_destination)\n",
    "print('Ophthalmology_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b98445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGE_gnomadv2_link = \"https://drive.google.com/file/d/1i0EB6iQ4SXNXIv9WAAVY5v_O13x6mawq/view?usp=sharing\"\n",
    "SGE_gnomadv2_destination = \"inputs/SGE_gnomadv2.csv\"\n",
    "SGE_gnomadv2 = pd.read_csv(SGE_gnomadv2_destination)\n",
    "print('SGE_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGE_gnomadv3_link = \"https://drive.google.com/file/d/1Hn2qRVG8eEZBym0hdf_JaDghG_1GPfsc/view?usp=sharing\"\n",
    "SGE_gnomadv3_destination = \"inputs/SGE_gnomadv3.csv\"\n",
    "SGE_gnomadv3 = pd.read_csv(SGE_gnomadv3_destination)\n",
    "print('SGE_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9973b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAMPSEQ_gnomadv2_link = \"https://drive.google.com/file/d/1hiVl12GB2BPjzPfSxVTugMYnJ6Afi_xK/view?usp=sharing\"\n",
    "VAMPSEQ_gnomadv2_destination = \"inputs/VAMPSEQ_gnomadv2.csv\"\n",
    "VAMPSEQ_gnomadv2 = pd.read_csv(VAMPSEQ_gnomadv2_destination)\n",
    "print('VAMPSEQ_gnomadv2 read in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAMPSEQ_gnomadv3_link = \"https://drive.google.com/file/d/14uQ5nSgWj6l01FxaBinbrRRG0ETgagVA/view?usp=sharing\"\n",
    "VAMPSEQ_gnomadv3_destination = \"inputs/VAMPSEQ_gnomadv3.csv\"\n",
    "VAMPSEQ_gnomadv3 = pd.read_csv(VAMPSEQ_gnomadv3_destination)\n",
    "print('VAMPSEQ_gnomadv3 read in')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564469a",
   "metadata": {},
   "source": [
    "# Step 6: <a class=\"anchor\" id=\"step-6\"></a>Set Main Dictionaries\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52936783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seting up dictionaries of each specialty\n",
    "#used in pie charts and box plots below\n",
    "#key value is used as label in figures\n",
    "\n",
    "inputdictionaryv2 = {\n",
    "    'Cardiology': Cardiac_gnomadv2,\n",
    "    'Endocrinology': Endocrinology_gnomadv2,\n",
    "    'Nephrology': Nephrology_gnomadv2,\n",
    "    'Hematology': Hematology_gnomadv2,\n",
    "    'Oncology': Cancer_gnomadv2,\n",
    "    'Allergy and Immunology': AllergyandImmunology_gnomadv2,\n",
    "    'Ophthalmology': Ophthalmology_gnomadv2,\n",
    "    'Neurology': Neurology_gnomadv2,\n",
    "    'Developmental Disorders (DDG2P)': DDG2P_gnomadv2,\n",
    "    'Carrier Screening': Carrier_gnomadv2,\n",
    "    'Newborn Screening': Newborn_gnomadv2,\n",
    "    'Inborn Errors of Metabolism': InbornErrorsOfMetabolism_gnomadv2,\n",
    "    'All Curated Clinical Genes (GenCC)': GenCC_gnomadv2,\n",
    "    'Reportable Secondary Findings (ACMG)': ACMG78_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictionaryv3 = {\n",
    "    'Cardiology': Cardiac_gnomadv3,\n",
    "    'Endocrinology': Endocrinology_gnomadv3,\n",
    "    'Nephrology': Nephrology_gnomadv3,\n",
    "    'Hematology': Hematology_gnomadv3,\n",
    "    'Oncology': Cancer_gnomadv3,\n",
    "    'Allergy and Immunology': AllergyandImmunology_gnomadv3,\n",
    "    'Ophthalmology': Ophthalmology_gnomadv3,\n",
    "    'Neurology': Neurology_gnomadv3,\n",
    "    'Developmental Disorders (DDG2P)': DDG2P_gnomadv3,\n",
    "    'Carrier Screening': Carrier_gnomadv3,\n",
    "    'Newborn Screening': Newborn_gnomadv3,\n",
    "    'Inborn Errors of Metabolism': InbornErrorsOfMetabolism_gnomadv3,\n",
    "    'All Curated Clinical Genes (GenCC)': GenCC_gnomadv3,\n",
    "    'Reportable Secondary Findings (ACMG)': ACMG78_gnomadv3\n",
    "}\n",
    "\n",
    "\n",
    "inputdictGenCCv2 = {\n",
    "    'All Curated Clinical Genes (GenCC)': GenCC_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictGenCCv3 = {\n",
    "    'All Curated Clinical Genes (GenCC)': GenCC_gnomadv3\n",
    "}\n",
    "\n",
    "\n",
    "inputdictACMG78v2 = {\n",
    "    'Reportable Secondary Findings (ACMG)': ACMG78_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictACMG78v3 = {\n",
    "    'Reportable Secondary Findings (ACMG)': ACMG78_gnomadv3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f308b7c",
   "metadata": {},
   "source": [
    "# Step 7: <a class=\"anchor\" id=\"step-7\"></a>Breakdown Databases By Medical Specialty and Variant Type\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de107f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vepannotations_pie_charts(dataframes_dict, column_name, filter_column=None, filter_value=None):\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
    "    axs = axs.flatten()\n",
    "    wedges_list = []\n",
    "    labels_list = []\n",
    "    variant_label_mapping = {\n",
    "        'missense_variant': 'Missense',\n",
    "        'Other': 'Other'\n",
    "    }\n",
    "    \n",
    "    for i, (df_name, dataframe) in enumerate(dataframes_dict.items()):\n",
    "        if filter_column and filter_value is not None:\n",
    "            dataframe = dataframe[dataframe[filter_column] == filter_value]\n",
    "        \n",
    "        value_counts = dataframe[column_name].value_counts()\n",
    "        other_categories = value_counts.index.difference(['missense_variant'])\n",
    "        other = value_counts[other_categories].sum()\n",
    "        value_counts = value_counts.drop(index=other_categories)\n",
    "        if other > 0:\n",
    "            value_counts['Other'] = other\n",
    "        \n",
    "        wedges, texts, autotexts = axs[i].pie(\n",
    "            value_counts, \n",
    "            autopct=lambda p: f'{p:.1f}%' if p > 1.5 else '', \n",
    "            startangle=90, \n",
    "            colors=plt.cm.Paired(range(len(value_counts))), \n",
    "            wedgeprops=dict(edgecolor='black'),\n",
    "            pctdistance=0.85,\n",
    "            textprops=dict(family='Helvetica', ha='center', va='center', weight='bold', color='white', size=10)\n",
    "        )\n",
    "        \n",
    "        for text in autotexts:\n",
    "            text.set_position((text.get_position()[0]*1.4, text.get_position()[1]*1.4))\n",
    "            text.set_color('black')\n",
    "            text.set_size(12)  \n",
    "    \n",
    "        wedges_list.extend(wedges)\n",
    "        labels_list.extend(value_counts.index.map(variant_label_mapping).tolist())\n",
    "    \n",
    "        axs[i].set(aspect=\"equal\")\n",
    "        axs[i].set_title(df_name, fontsize=12)  \n",
    "    \n",
    "    unique_labels = pd.Series(labels_list).drop_duplicates().tolist()\n",
    "    unique_wedges = [wedges_list[labels_list.index(label)] for label in unique_labels]\n",
    "    \n",
    "    axs[14].legend(unique_wedges, unique_labels, title='Variant Type', loc=\"center\", fontsize=12)  # Setting the font size of the legend\n",
    "    axs[14].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vepannotations_pie_charts(dataframes_dict = inputdictionaryv2, \n",
    "               column_name = 'VEP Annotation', \n",
    "               filter_column = 'ClinVar Clinical Significance', \n",
    "               filter_value = 'Variant of Uncertain Significance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vepannotations_pie_charts(dataframes_dict = inputdictionaryv3, \n",
    "               column_name = 'VEP Annotation', \n",
    "               filter_column = 'ClinVar Clinical Significance', \n",
    "               filter_value = 'Variant of Uncertain Significance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ba2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variant_pie_charts(dataframes_dict, column_name, filter_column=None, filter_value=None):\n",
    "    # Create a figure and an array of axes with 3 rows and 5 columns\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
    "    \n",
    "    # Flatten the array of axes\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Initialize an empty list to store the wedge handles and labels\n",
    "    wedges_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Loop through the list of dataframes and plot the pie chart in each subplot\n",
    "    for i, (df_name, dataframe) in enumerate(dataframes_dict.items()):\n",
    "        # If filter_column and filter_value are provided, filter the dataframe\n",
    "        if filter_column and filter_value is not None:\n",
    "            dataframe = dataframe[dataframe[filter_column] == filter_value]\n",
    "        \n",
    "        # Count the occurrences of each unique value in the specified column\n",
    "        value_counts = dataframe[column_name].value_counts()\n",
    "        total = value_counts.sum()\n",
    "        \n",
    "        # Group values less than or equal to 1.5% of the total under 'Other'\n",
    "        value_counts = value_counts[value_counts / total > 0.015]\n",
    "        other = total - value_counts.sum()\n",
    "        if other > 0:\n",
    "            value_counts['Other'] = other\n",
    "        \n",
    "        # Plot the pie chart\n",
    "        wedges, texts, autotexts = axs[i].pie(\n",
    "            value_counts, \n",
    "            autopct=lambda p: f'{p:.1f}%' if p > 1.5 else '', \n",
    "            startangle=90, \n",
    "            colors=plt.cm.Paired(range(len(value_counts))), \n",
    "            wedgeprops=dict(edgecolor='black'),\n",
    "            pctdistance=0.85,\n",
    "            textprops=dict(family='Helvetica', ha='center', va='center', weight='bold', color='white')\n",
    "        )\n",
    "        \n",
    "        # Move the percentage labels to be outside the pie chart\n",
    "        for text in autotexts:\n",
    "            text.set_position((text.get_position()[0]*1.4, text.get_position()[1]*1.4))\n",
    "            text.set_color('black')\n",
    "    \n",
    "        # Add the wedges and labels to the lists\n",
    "        wedges_list.extend(wedges)\n",
    "        labels_list.extend(value_counts.index)\n",
    "    \n",
    "        axs[i].set(aspect=\"equal\")  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    \n",
    "        # Set the title of the plot to the name of the dataframe\n",
    "        axs[i].set_title(df_name)\n",
    "    \n",
    "    # Make unique labels for the legend\n",
    "    unique_labels = pd.Series(labels_list).drop_duplicates().tolist()\n",
    "    unique_wedges = [wedges_list[labels_list.index(label)] for label in unique_labels]\n",
    "    \n",
    "    # Add a shared legend in the 15th subplot\n",
    "    axs[14].legend(unique_wedges, unique_labels, title=column_name, loc=\"center\")\n",
    "    axs[14].axis('off')  # Turn off the axis for the 15th subplot\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variant_pie_charts(dataframes_dict = inputdictionaryv2, \n",
    "               column_name = 'VEP Annotation', \n",
    "               filter_column = 'ClinVar Clinical Significance', \n",
    "               filter_value = 'Variant of Uncertain Significance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f901fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variant_pie_charts(dataframes_dict = inputdictionaryv3, \n",
    "               column_name = 'VEP Annotation', \n",
    "               filter_column = 'ClinVar Clinical Significance', \n",
    "               filter_value = 'Pathogenic or Likely Pathogenic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b67543",
   "metadata": {},
   "source": [
    "# Step 8: <a class=\"anchor\" id=\"step-8\"></a>Filter out non-clinical genes for SGE and VAMPSEQ sets\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef097d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df1, df2):\n",
    "    # Extract the unique ENSG values from dataframe 2\n",
    "    ensg_values = df2['ENSG'].unique()\n",
    "    \n",
    "    # Filter the rows in dataframe 1 based on the ENSG values\n",
    "    filtered_df = df1[df1['ENSG'].isin(ensg_values)]\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048dbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for SGE and VAMPseq that will only have clinical disease genes\n",
    "SGE_gnomadv2_clinical = filter_dataframe(SGE_gnomadv2, GenCC_gnomadv2)\n",
    "VAMPSEQ_gnomadv2_clinical = filter_dataframe(VAMPSEQ_gnomadv2, GenCC_gnomadv2)\n",
    "SGE_gnomadv3_clinical = filter_dataframe(SGE_gnomadv3, GenCC_gnomadv3)\n",
    "VAMPSEQ_gnomadv3_clinical = filter_dataframe(VAMPSEQ_gnomadv3, GenCC_gnomadv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"gnomadv2 --> Total number of target clinical disease genes for SGE: {len(SGE_gnomadv2_clinical['ENSG'].unique())} genes\")\n",
    "print(f\"gnomadv2 --> Total number of target clinical disease genes for VAMPseq: {len(VAMPSEQ_gnomadv2_clinical['ENSG'].unique())} genes\")\n",
    "print(f\"gnomadv3 --> Total number of target clinical disease genes for SGE: {len(SGE_gnomadv3_clinical['ENSG'].unique())} genes\")\n",
    "print(f\"gnomadv3 --> Total number of target clinical disease genes for VAMPseq: {len(VAMPSEQ_gnomadv3_clinical['ENSG'].unique())} genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6dfc34",
   "metadata": {},
   "source": [
    "# Step 9: <a class=\"anchor\" id=\"step-9\"></a>Establish input dictionaries of the different gene sets\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionaries of dataframes for gnomadv2 and gnomadv3\n",
    "\n",
    "inputdictthreev2 = {\n",
    "    'ACMG78': ACMG78_gnomadv2,\n",
    "    'GenCC': GenCC_gnomadv2,\n",
    "    'Newborn Screening': Newborn_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictthreev3 = {\n",
    "    'ACMG78': ACMG78_gnomadv3,\n",
    "    'GenCC': GenCC_gnomadv3,\n",
    "    'Newborn Screening': Newborn_gnomadv3\n",
    "}\n",
    "\n",
    "inputdictionarymavev2 = {\n",
    "    'SGE': SGE_gnomadv2_clinical,\n",
    "    'VAMPseq': VAMPSEQ_gnomadv2_clinical,\n",
    "    'MAVERegistry': MAVERegistry_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictionarymavev3 = {\n",
    "    'SGE': SGE_gnomadv3_clinical,\n",
    "    'VAMPseq': VAMPSEQ_gnomadv3_clinical,\n",
    "    'MAVERegistry': MAVERegistry_gnomadv3\n",
    "}\n",
    "\n",
    "inputdictionarieslistv2 = {\n",
    "    'Main': inputdictionaryv2,\n",
    "    'MAVE': inputdictionarymavev2\n",
    "}\n",
    "\n",
    "inputdictionarieslistv3 = {\n",
    "    'Main': inputdictionaryv3,\n",
    "    'MAVE': inputdictionarymavev3    \n",
    "}\n",
    "\n",
    "\n",
    "inputdictthreeGenCCv2 = {\n",
    "    'GenCC': GenCC_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictthreeGenCCv3 = {\n",
    "    'GenCC': GenCC_gnomadv3\n",
    "}\n",
    "\n",
    "inputdictthreeACMGv2 = {\n",
    "    'ACMG78': ACMG78_gnomadv2\n",
    "}\n",
    "\n",
    "inputdictthreeACMGv3 = {\n",
    "    'ACMG78': ACMG78_gnomadv3\n",
    "}\n",
    "\n",
    "inputdictionariesvariantmappinglistACMG78v2 = {\n",
    "    'ACMG78': inputdictthreeACMGv2\n",
    "}\n",
    "\n",
    "inputdictionariesvariantmappinglistACMG78v3 = {\n",
    "    'ACMG78': inputdictthreeACMGv3\n",
    "}\n",
    "\n",
    "\n",
    "inputdictionariesvariantmappinglistGenCCv2 = {\n",
    "    'GenCC': inputdictthreeGenCCv2\n",
    "}\n",
    "\n",
    "inputdictionariesvariantmappinglistGenCCv3 = {\n",
    "    'GenCC': inputdictthreeGenCCv3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18420af0",
   "metadata": {},
   "source": [
    "# Part 2: <a class=\"anchor\" id=\"part-2\"></a> Gene by Gene Statistical Testing (Wilcoxon singed-rank, matched-pairs test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b93aa0",
   "metadata": {},
   "source": [
    "# Step 10: <a class=\"anchor\" id=\"step-10\"></a>Function for Filtering by Variant Type\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for variant types\n",
    "def filterforvarianttypes(df, varianttypes):\n",
    "    filtereddf = df[df['VEP Annotation'].isin(varianttypes)]\n",
    "    return filtereddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9dbde1",
   "metadata": {},
   "source": [
    "# Step 11: <a class=\"anchor\" id=\"step-11\"></a>Functions for Grouping Genes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc156bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by gene and clinical significance for each input df\n",
    "def groupbygeneandsignificance(df):\n",
    "    new_df = df.groupby(['ENSG', 'ClinVar Clinical Significance']).sum().reset_index()\n",
    "    \n",
    "    new_df['Total Variants in Gene'] = new_df['Total Variants in Gene'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Coding Variants in Gene'] = new_df['Total Coding Variants in Gene'] / new_df['Unique Variants']\n",
    "    new_df['Total Noncoding Variants in Gene'] = new_df['Total Noncoding Variants in Gene'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Unique Variants in popdescrips1'] = new_df['Total Unique Variants in popdescrips1'] / new_df['Unique Variants']\n",
    "    new_df['Total Unique Variants in popdescrips2'] = new_df['Total Unique Variants in popdescrips2'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Unique Coding Variants for popdescrips1'] = new_df['Total Unique Coding Variants for popdescrips1'] / new_df['Unique Variants']\n",
    "    new_df['Total Unique Coding Variants for popdescrips2'] = new_df['Total Unique Coding Variants for popdescrips2'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Unique Noncoding Variants for popdescrips1'] = new_df['Total Unique Noncoding Variants for popdescrips1'] / new_df['Unique Variants']\n",
    "    new_df['Total Unique Noncoding Variants for popdescrips2'] = new_df['Total Unique Noncoding Variants for popdescrips2'] / new_df['Unique Variants']\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b867eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by gene and clinical significance for each input df\n",
    "def groupbygeneandsignificanceandvepannotation(df):\n",
    "    new_df = df.groupby(['ENSG', 'ClinVar Clinical Significance', 'VEP Annotation']).sum().reset_index()\n",
    "    \n",
    "    new_df['Total Variants in Gene'] = new_df['Total Variants in Gene'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Coding Variants in Gene'] = new_df['Total Coding Variants in Gene'] / new_df['Unique Variants']\n",
    "    new_df['Total Noncoding Variants in Gene'] = new_df['Total Noncoding Variants in Gene'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Unique Variants in popdescrips1'] = new_df['Total Unique Variants in popdescrips1'] / new_df['Unique Variants']\n",
    "    new_df['Total Unique Variants in popdescrips2'] = new_df['Total Unique Variants in popdescrips2'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Unique Coding Variants for popdescrips1'] = new_df['Total Unique Coding Variants for popdescrips1'] / new_df['Unique Variants']\n",
    "    new_df['Total Unique Coding Variants for popdescrips2'] = new_df['Total Unique Coding Variants for popdescrips2'] / new_df['Unique Variants']\n",
    "    \n",
    "    new_df['Total Unique Noncoding Variants for popdescrips1'] = new_df['Total Unique Noncoding Variants for popdescrips1'] / new_df['Unique Variants']\n",
    "    new_df['Total Unique Noncoding Variants for popdescrips2'] = new_df['Total Unique Noncoding Variants for popdescrips2'] / new_df['Unique Variants']\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f10c7",
   "metadata": {},
   "source": [
    "# Step 12: <a class=\"anchor\" id=\"step-12\"></a>Functions for Calculating Allele Prevalence\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate allele prevalence\n",
    "def calculate_allele_count_per_individual_twobars(inputdf, version, popdescrips1, popdescrips2, normalizationterm):\n",
    "    df = inputdf.drop('Allele Count', axis=1, errors='ignore')\n",
    "\n",
    "    if version == 'v2':\n",
    "        ancestry_num = ancestry_v2\n",
    "    if version == 'v3':\n",
    "        ancestry_num = ancestry_v3nonv2\n",
    "\n",
    "    #prevent division by zero\n",
    "    #doesn't actually matter because if there are no unique variants then there are no alleles so the allele prevalence would be zero anyway\n",
    "    df['Total Unique Coding Variants for popdescrips1'] = df['Total Unique Coding Variants for popdescrips1'].replace(0, 1)\n",
    "    df['Total Unique Coding Variants for popdescrips2'] = df['Total Unique Coding Variants for popdescrips2'].replace(0, 1)\n",
    "    df['Total Unique Noncoding Variants for popdescrips1'] = df['Total Unique Noncoding Variants for popdescrips1'].replace(0, 1)\n",
    "    df['Total Unique Noncoding Variants for popdescrips2'] = df['Total Unique Noncoding Variants for popdescrips2'].replace(0, 1)\n",
    "\n",
    "    # Calculate the allele count per individual for popdescrips1 ancestry and normalize by number of variants\n",
    "    df['Summed Allele Count popdescrips1'] = sum([df[f'Allele Count {ancestry}'] for ancestry in popdescrips1])\n",
    "    popdescrips1_ind_count = sum([ancestry_num[ancestry] for ancestry in popdescrips1])\n",
    "    \n",
    "    # Calculate the allele count per individual for popdescrips2 ancestry and normalize by number of variants\n",
    "    df['Summed Allele Count popdescrips2'] = sum([df[f'Allele Count {ancestry}'] for ancestry in popdescrips2])\n",
    "    popdescrips2_ind_count = sum([ancestry_num[ancestry] for ancestry in popdescrips2])\n",
    "    \n",
    "    df['Allele Prevalence of ClinSigGroup popdescrips1'] = df['Summed Allele Count popdescrips1'] / (popdescrips1_ind_count)\n",
    "    df['Allele Prevalence of ClinSigGroup popdescrips2'] = df['Summed Allele Count popdescrips2'] / (popdescrips2_ind_count)\n",
    "    df['Allele Prevalence Difference'] = df['Allele Prevalence of ClinSigGroup popdescrips1'] - df['Allele Prevalence of ClinSigGroup popdescrips2']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate allele prevalence\n",
    "def popbreakcalculate_allele_count_per_individual_twobars(inputdf, version, popdescrips1, popdescrips2, normalizationterm):\n",
    "    df = inputdf.drop('Allele Count', axis=1, errors='ignore')\n",
    "\n",
    "    if version == 'v2':\n",
    "        ancestry_num = ancestry_v2\n",
    "    if version == 'v3':\n",
    "        ancestry_num = ancestry_v3nonv2\n",
    "\n",
    "    #prevent division by zero\n",
    "    #doesn't actually matter because if there are no unique variants then there are no alleles so the allele prevalence would be zero anyway\n",
    "    df['Total Unique Coding Variants for popdescrips1'] = df['Total Unique Coding Variants for popdescrips1'].replace(0, 1)\n",
    "    df['Total Unique Coding Variants for popdescrips2'] = df['Total Unique Coding Variants for popdescrips2'].replace(0, 1)\n",
    "    df['Total Unique Noncoding Variants for popdescrips1'] = df['Total Unique Noncoding Variants for popdescrips1'].replace(0, 1)\n",
    "    df['Total Unique Noncoding Variants for popdescrips2'] = df['Total Unique Noncoding Variants for popdescrips2'].replace(0, 1)\n",
    "\n",
    "    for ancestry, num in ancestry_num.items():\n",
    "        df[f'Allele Prevalence of ClinSigGroup {ancestry}'] = df[f'Allele Count {ancestry}'] / num\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168beae9",
   "metadata": {},
   "source": [
    "# Step 13: <a class=\"anchor\" id=\"step-13\"></a>Functions for Subsetting Needed Columns\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the dataframe to feed for plotting and statistical testing\n",
    "def variantsubsetdf(df, inputterm):\n",
    "    #'Summed Allele Count popdescrips1', 'Variant in popdescrips1', 'Summed Allele Count popdescrips2', 'Variant in popdescrips2', \n",
    "    filtereddf = df[['ENSG', 'ClinVar Clinical Significance', 'VEP Annotation', 'Allele Prevalence of ClinSigGroup popdescrips1', 'Allele Prevalence of ClinSigGroup popdescrips2','Group']]\n",
    "    subsetdf = filtereddf[df['ClinVar Clinical Significance'] == inputterm]\n",
    "    return subsetdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013960a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the dataframe to feed for plotting and statistical testing\n",
    "def popbreaksubsetdf(df, inputterm, version):\n",
    "    #'Summed Allele Count popdescrips1', 'Variant in popdescrips1', 'Summed Allele Count popdescrips2', 'Variant in popdescrips2', \n",
    "    if version == 'v2':\n",
    "        ancestry_num = ancestry_v2\n",
    "    if version == 'v3':\n",
    "        ancestry_num = ancestry_v3nonv2\n",
    "    \n",
    "    originalcolumns = ['ENSG', 'ClinVar Clinical Significance', 'Group']\n",
    "    \n",
    "    ancestrylist = []\n",
    "    for ancestry, num in ancestry_num.items():\n",
    "        ancestrylist.append(f'Allele Prevalence of ClinSigGroup {ancestry}')\n",
    "    \n",
    "    columnstokeep = ancestrylist + originalcolumns\n",
    "    filtereddf = df[columnstokeep]\n",
    "    subsetdf = filtereddf[df['ClinVar Clinical Significance'] == inputterm]\n",
    "    \n",
    "    return subsetdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55894541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the dataframe to feed for plotting and statistical testing\n",
    "def subsetdf(df, inputterm):\n",
    "    #'Summed Allele Count popdescrips1', 'Variant in popdescrips1', 'Summed Allele Count popdescrips2', 'Variant in popdescrips2', \n",
    "    filtereddf = df[['ENSG', 'ClinVar Clinical Significance', 'Allele Prevalence of ClinSigGroup popdescrips1', 'Allele Prevalence of ClinSigGroup popdescrips2','Group']]\n",
    "    subsetdf = filtereddf[df['ClinVar Clinical Significance'] == inputterm]\n",
    "    return subsetdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5e331",
   "metadata": {},
   "source": [
    "# Step 14: <a class=\"anchor\" id=\"step-14\"></a>Functions for Combining Allele Counts\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_allele_counts(df):\n",
    "    combined_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        popdescrips1_count = row['Allele Prevalence of ClinSigGroup popdescrips1']\n",
    "        popdescrips2_count = row['Allele Prevalence of ClinSigGroup popdescrips2']\n",
    "        \n",
    "        popdescrips1_row = row.copy()\n",
    "        popdescrips1_row['Allele Prevalence'] = popdescrips1_count\n",
    "        popdescrips1_row['Population'] = 'European'\n",
    "        \n",
    "        popdescrips2_row = row.copy()\n",
    "        popdescrips2_row['Allele Prevalence'] = popdescrips2_count\n",
    "        popdescrips2_row['Population'] = 'non-European'\n",
    "        \n",
    "        combined_rows.append(popdescrips1_row)\n",
    "        combined_rows.append(popdescrips2_row)\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_rows)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd014951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popbreakcombine_allele_counts(df, version):\n",
    "    if version == 'v2':\n",
    "        ancestry_num = ancestry_v2\n",
    "    if version == 'v3':\n",
    "        ancestry_num = ancestry_v3nonv2\n",
    "    \n",
    "    combined_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        for ancestry, num in ancestry_num.items():\n",
    "            popdescrips_row = row.copy()\n",
    "            popdescrips_row['Allele Prevalence'] = row[f'Allele Prevalence of ClinSigGroup {ancestry}']\n",
    "            popdescrips_row['Population'] = f'{ancestry}'\n",
    "            combined_rows.append(popdescrips_row)\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_rows)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c7b99",
   "metadata": {},
   "source": [
    "# Step 15: <a class=\"anchor\" id=\"step-15\"></a> Trigger Functions For Setting Up Plotting Dataframe: Filter Variant Type --> Group by Gene --> Calculate Allele Prevalence --> Concatenate --> Subset by Significance --> Combine Allele Counts\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run all the above functions to make the final tables for usage and for plotting\n",
    "def dfforfinaltable(inputdictionarylist, version, varianttype, popdescrips1, popdescrips2, normalizationterm):\n",
    "    finaldf = pd.DataFrame()\n",
    "    \n",
    "    for key, df in inputdictionarylist.items():\n",
    "        #print(f\"{key} dataframe variants in each popdescrip group counted; ready to be summed\")\n",
    "        \n",
    "        variantselecteddf = filterforvarianttypes(df, varianttype)\n",
    "        #print(f\"{key} dataframe selected \" + ', '.join(varianttype) + \" variants selected\")\n",
    "        \n",
    "        geneclindf = groupbygeneandsignificance(variantselecteddf)\n",
    "        #print(f\"{key} dataframe grouped by ENSG and clinical significance\")\n",
    "        \n",
    "        countdf = calculate_allele_count_per_individual_twobars(geneclindf, version, popdescrips1, popdescrips2, normalizationterm)\n",
    "        #print(f\"{key} dataframe allele prevalence tabulated and normalized\")\n",
    "        \n",
    "        countdf['Group'] = key\n",
    "        \n",
    "        finaltable = remove_columns_by_name(countdf, columns_to_remove)\n",
    "        #print(f\"{key} dataframe with nonsensical columns removed\")\n",
    "\n",
    "        finaldf = pd.concat([finaldf,finaltable])\n",
    "        #print(f\"{key} dataframe appended to final\")\n",
    "        \n",
    "    return finaldf\n",
    "\n",
    "\n",
    "def dfforplots(inputdictionarylist, version, varianttype, clinicalsignificance, popdescrips1, popdescrips2, normalizationterm):\n",
    "    finaltable = dfforfinaltable(inputdictionarylist, version, varianttype, popdescrips1, popdescrips2, normalizationterm)\n",
    "        \n",
    "    subsetteddf = subsetdf(finaltable, clinicalsignificance)\n",
    "    #print(f\"dataframe selected only 1 clinical significance value and only ENSG, ClinVar Clinical Significance, Allele Count Per Individual Non-European, Allele Count Per Individual European columns\")\n",
    "        \n",
    "    finaldf = combine_allele_counts(subsetteddf)\n",
    "    #print(f\"dataframe moved alleles to single column\")\n",
    "\n",
    "    return finaltable, finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7581b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run all the above functions to make the final tables for usage and for plotting\n",
    "def variantdfforfinaltable(inputdictionarylist, version, varianttype, popdescrips1, popdescrips2, normalizationterm):\n",
    "    finaldf = pd.DataFrame()\n",
    "    \n",
    "    for key, df in inputdictionarylist.items():\n",
    "        #print(f\"{key} dataframe variants in each popdescrip group counted; ready to be summed\")\n",
    "        \n",
    "        variantselecteddf = filterforvarianttypes(df, varianttype)\n",
    "        #print(f\"{key} dataframe selected \" + ', '.join(varianttype) + \" variants selected\")\n",
    "        \n",
    "        geneclindf = groupbygeneandsignificanceandvepannotation(variantselecteddf)\n",
    "        #print(f\"{key} dataframe grouped by ENSG and clinical significance\")\n",
    "        \n",
    "        countdf = calculate_allele_count_per_individual_twobars(geneclindf, version, popdescrips1, popdescrips2, normalizationterm)\n",
    "        #print(f\"{key} dataframe allele representation tabulated and normalized\")\n",
    "        \n",
    "        countdf['Group'] = countdf['VEP Annotation']\n",
    "        values_to_replace = [\"inframe_deletion\", \"inframe_insertion\"]\n",
    "    \n",
    "        # Replace the values in the \"Group\" column with \"inframe\"\n",
    "        countdf[\"Group\"] = countdf[\"Group\"].apply(lambda x: \"inframe\" if x in values_to_replace else x)\n",
    "        \n",
    "        finaltable = remove_columns_by_name(countdf, columns_to_remove)\n",
    "        #print(f\"{key} dataframe with nonsensical columns removed\")\n",
    "\n",
    "        finaldf = pd.concat([finaldf,finaltable])\n",
    "        #print(f\"{key} dataframe appended to final\")\n",
    "        \n",
    "    return finaldf\n",
    "\n",
    "\n",
    "def variantdfforplots(inputdictionarylist, version, varianttype, clinicalsignificance, popdescrips1, popdescrips2, normalizationterm):\n",
    "    finaltable = variantdfforfinaltable(inputdictionarylist, version, varianttype, popdescrips1, popdescrips2, normalizationterm)\n",
    "        \n",
    "    subsetteddf = variantsubsetdf(finaltable, clinicalsignificance)\n",
    "    #print(f\"dataframe selected only 1 clinical significance value and only ENSG, ClinVar Clinical Significance, Allele Count Per Individual Non-European, Allele Count Per Individual European columns\")\n",
    "        \n",
    "    finaldf = combine_allele_counts(subsetteddf)\n",
    "    #print(f\"dataframe moved alleles to single column\")\n",
    "\n",
    "    return finaltable, finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run all the above functions to make the final tables for usage and for plotting\n",
    "def popbreakdfforfinaltable(inputdictionarylist, version, varianttype, popdescrips1, popdescrips2, normalizationterm):\n",
    "    finaldf = pd.DataFrame()\n",
    "    \n",
    "    for key, df in inputdictionarylist.items():\n",
    "        #print(f\"{key} dataframe variants in each popdescrip group counted; ready to be summed\")\n",
    "        \n",
    "        variantselecteddf = filterforvarianttypes(df, varianttype)\n",
    "        #print(f\"{key} dataframe selected \" + ', '.join(varianttype) + \" variants selected\")\n",
    "        \n",
    "        geneclindf = groupbygeneandsignificance(variantselecteddf)\n",
    "        #print(f\"{key} dataframe grouped by ENSG and clinical significance\")\n",
    "        \n",
    "        countdf = popbreakcalculate_allele_count_per_individual_twobars(geneclindf, version, popdescrips1, popdescrips2, normalizationterm)\n",
    "        #print(f\"{key} dataframe allele prevalence tabulated and normalized\")\n",
    "        \n",
    "        countdf['Group'] = key\n",
    "        \n",
    "        finaltable = remove_columns_by_name(countdf, columns_to_remove)\n",
    "        #print(f\"{key} dataframe with nonsensical columns removed\")\n",
    "\n",
    "        finaldf = pd.concat([finaldf,finaltable])\n",
    "        #print(f\"{key} dataframe appended to final\")\n",
    "        \n",
    "    return finaldf\n",
    "\n",
    "\n",
    "def popbreakdfforplots(inputdictionarylist, version, varianttype, clinicalsignificance, popdescrips1, popdescrips2, normalizationterm):\n",
    "    finaltable = popbreakdfforfinaltable(inputdictionarylist, version, varianttype, popdescrips1, popdescrips2, normalizationterm)\n",
    "        \n",
    "    subsetteddf = popbreaksubsetdf(finaltable, clinicalsignificance, version)\n",
    "    #print(f\"dataframe selected only 1 clinical significance value and only ENSG, ClinVar Clinical Significance, Allele Count Per Individual Non-European, Allele Count Per Individual European columns\")\n",
    "        \n",
    "    finaldf = popbreakcombine_allele_counts(subsetteddf, version)\n",
    "    #print(f\"dataframe moved alleles to single column\")\n",
    "\n",
    "    return finaltable, finaldf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b567e",
   "metadata": {},
   "source": [
    "# Step 16: <a class=\"anchor\" id=\"step-16\"></a> Miscellaneous Helper Functions\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70549230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to remove; these columns after all the summation don't make any more numerical sense\n",
    "columns_to_remove = ['Position', 'ClinVar Variation ID','Allele Number','Allele Frequency',\n",
    "                    'Allele Number Latino/Admixed American',\n",
    "                    'Allele Number Ashkenazi Jewish',\n",
    "                    'Allele Number East Asian',\n",
    "                    'Allele Number European (Finnish)',\n",
    "                    'Allele Number European (non-Finnish)',\n",
    "                    'Allele Number Other',\n",
    "                    'Allele Number South Asian',\n",
    "                    'Allele Number African/African American']\n",
    "\n",
    "def remove_columns_by_name(dataframe, columns_to_remove):\n",
    "    return dataframe.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gene_column(original_df, biomart_filepath):\n",
    "    # Load the BioMart table\n",
    "    biodf = pd.read_csv(biomart_filepath,sep='\\t')\n",
    "    \n",
    "    # Check if 'Ensembl gene ID' and 'Approved symbol' columns exist in the BioMart table\n",
    "    if 'Ensembl gene ID' not in biodf.columns or 'Approved symbol' not in biodf.columns:\n",
    "        raise ValueError(\"BioMart table must contain 'Ensembl gene ID' and 'Approved symbol' columns.\")\n",
    "    \n",
    "    biodf.drop_duplicates(subset='Ensembl gene ID', keep='first', inplace=True)\n",
    "    \n",
    "    # Merge the original DataFrame with the BioMart DataFrame based on the 'ENSG' column\n",
    "    geneaddeddf = pd.merge(original_df, biodf[['Ensembl gene ID', 'Approved symbol']], left_on='ENSG', right_on='Ensembl gene ID', how='left')\n",
    "    finalgeneaddeddf = geneaddeddf.drop('Ensembl gene ID', axis=1)\n",
    "    finalgeneaddeddf.rename(columns={'Approved symbol': 'Gene'}, inplace=True)\n",
    "\n",
    "    # If 'Gene' column is not found for a row, it will be filled with NaN in the resulting DataFrame\n",
    "    return finalgeneaddeddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_significant_figures(number, significant_figures):\n",
    "    \"\"\"\n",
    "    Rounds a number to the specified number of significant figures.\n",
    "\n",
    "    :param number: The number to be rounded.\n",
    "    :param significant_figures: The number of significant figures.\n",
    "    :return: The number rounded to the specified significant figures.\n",
    "    \"\"\"\n",
    "    if number == 0:\n",
    "        return 0  # Handle the special case where the number is zero\n",
    "    \n",
    "    # Format the number to the specified number of significant figures\n",
    "    formatted_number = '{:.{p}g}'.format(number, p=significant_figures)\n",
    "    \n",
    "    # Convert the formatted number back to a float\n",
    "    rounded_number = float(formatted_number)\n",
    "    \n",
    "    return rounded_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variant_value(df, group_value, clinvar_value, colname):\n",
    "    \"\"\"\n",
    "    Returns the value from the 'Variant in popdescrips1' column based on the provided conditions.\n",
    "\n",
    "    :param df: DataFrame containing the relevant columns.\n",
    "    :param group_value: Value in the 'Group' column to match.\n",
    "    :param clinvar_value: Value in the 'ClinVar Clinical Significance' column to match.\n",
    "    :return: The corresponding value from the 'Variant in popdescrips1' column.\n",
    "    \"\"\"\n",
    "    # Use boolean indexing to filter the DataFrame based on the conditions\n",
    "    filtered_df = df[(df['Group'] == group_value) & (df['ClinVar Clinical Significance'] == clinvar_value)]\n",
    "\n",
    "    # Check if there is a match\n",
    "    if filtered_df.empty:\n",
    "        return None  # or raise an exception, or return a default value, depending on your needs\n",
    "\n",
    "    # Extract and return the value from the 'Variant in popdescrips1' column\n",
    "    return filtered_df[colname].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_png(df, filename):\n",
    "    \"\"\"\n",
    "    Save the given DataFrame to a PNG file.\n",
    "\n",
    "    :param df: DataFrame to be saved as PNG.\n",
    "    :param filename: The name of the output PNG file.\n",
    "    \"\"\"\n",
    "    # Set up the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(6, 4)) # Set the size of your figure\n",
    "\n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Render the DataFrame as a table\n",
    "    tbl = plt.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "\n",
    "    # Set the font size\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(12)\n",
    "\n",
    "    # Auto size the columns\n",
    "    tbl.auto_set_column_width(col=list(range(len(df.columns))))\n",
    "\n",
    "    # Remove gridlines by setting the visibility of cell edges to False\n",
    "    for _, cell in tbl.get_celld().items():\n",
    "        cell.set_edgecolor('white')\n",
    "    \n",
    "    # Save the image\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Optionally display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_pdf(df, filename, figsize=(6, 4), fontsize=12):\n",
    "    \"\"\"\n",
    "    Save the given DataFrame to a PDF file.\n",
    "\n",
    "    :param df: DataFrame to be saved as PDF.\n",
    "    :param filename: The name of the output PDF file.\n",
    "    :param figsize: Tuple representing the size of the figure.\n",
    "    :param fontsize: Font size of the table text.\n",
    "    \"\"\"\n",
    "    # Set up the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Render DataFrame as a table\n",
    "    tbl = plt.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "    \n",
    "    # Set font size\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(fontsize)\n",
    "    \n",
    "    # Auto size columns\n",
    "    tbl.auto_set_column_width(col=list(range(len(df.columns))))\n",
    "    \n",
    "    # Remove gridlines \n",
    "    for _, cell in tbl.get_celld().items():\n",
    "        cell.set_edgecolor('white')\n",
    "    \n",
    "    # Save the figure to PDF\n",
    "    pdf_pages = PdfPages(filename)\n",
    "    pdf_pages.savefig(fig, bbox_inches='tight', dpi=300)\n",
    "    pdf_pages.close()\n",
    "    \n",
    "    # Close the matplotlib plot display\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b58def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values(df: pd.DataFrame, column_name: str, filter_column_name: str, filter_value: any) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of unique values in a column of a DataFrame,\n",
    "    after subsetting the DataFrame based on a value in a different column.\n",
    "    \n",
    "    :param df: DataFrame to perform the operation on\n",
    "    :param column_name: Name of the column to count unique values in\n",
    "    :param filter_column_name: Name of the column to filter the DataFrame on\n",
    "    :param filter_value: Value in the filter_column_name to retain the rows\n",
    "    :return: Count of unique values in the specified column after filtering\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on the value in the filter_column_name\n",
    "    filtered_df = df[df[filter_column_name] == filter_value]\n",
    "    \n",
    "    # Count the number of unique values in the specified column\n",
    "    num_unique_values = filtered_df[column_name].nunique()\n",
    "    \n",
    "    return num_unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08482056",
   "metadata": {},
   "source": [
    "# Step 17: <a class=\"anchor\" id=\"step-17\"></a> Filtering Zeroes For Plotting\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_zeros(df, col1, col2, col3, col4):\n",
    "    filtered_df = df[(df[col1] != 0) & (df[col2] != 0)]\n",
    "\n",
    "    # Concatenate the two specified columns with an underscore separator\n",
    "    new_col_name = f\"{col3}_{col4}\"\n",
    "    filtered_df[new_col_name] = filtered_df[col3].astype(str) + \"_\" + filtered_df[col4].astype(str)\n",
    "    \n",
    "    # Count the occurrences of each value in the new column\n",
    "    value_counts = filtered_df[new_col_name].value_counts()\n",
    "    \n",
    "    # Filter rows based on the count of each value\n",
    "    secondfilterdf = filtered_df[filtered_df[new_col_name].map(value_counts) == 2]\n",
    "    \n",
    "    # Drop the temporary new column\n",
    "    finaldf = secondfilterdf.drop(columns=[new_col_name])\n",
    "    \n",
    "    return finaldf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936719cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_zeros_multicolumn(df, col1, col2, col3, col4, col5, col6, col7, col8):\n",
    "    filtered_df = df[(df[col1] != 0) & (df[col2] != 0) & (df[col3] != 0) & (df[col4] != 0) & (df[col5] != 0) & (df[col6] != 0)]\n",
    "    \n",
    "    # Concatenate the two specified columns with an underscore separator\n",
    "    #new_col_name = f\"{col7}_{col8}\"\n",
    "    #filtered_df[new_col_name] = filtered_df[col7].astype(str) + \"_\" + filtered_df[col8].astype(str)\n",
    "    \n",
    "    # Count the occurrences of each value in the new column\n",
    "    #value_counts = filtered_df[new_col_name].value_counts()\n",
    "    \n",
    "    # Filter rows based on the count of each value\n",
    "    #secondfilterdf = filtered_df[filtered_df[new_col_name].map(value_counts) == 6]\n",
    "    \n",
    "    # Drop the temporary new column\n",
    "    #finaldf = secondfilterdf.drop(columns=[new_col_name])\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b91b0",
   "metadata": {},
   "source": [
    "# Step 18: <a class=\"anchor\" id=\"step-18\"></a> Estimating Statistical Power\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6471e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_wilcoxon_power(effect_size, sample_size, alpha, bonferroni_correction, num_simulations=50000):\n",
    "    \"\"\"\n",
    "    Simulate the power of a Wilcoxon signed-rank test with Bonferroni correction.\n",
    "\n",
    "    Parameters:\n",
    "    effect_size (float): The expected effect size.\n",
    "    sample_size (int): The number of matched pairs.\n",
    "    alpha (float): The significance level (without correction).\n",
    "    num_simulations (int): The number of simulations to run.\n",
    "    bonferroni_correction (int): The number of tests for Bonferroni correction.\n",
    "\n",
    "    Returns:\n",
    "    float: Estimated power of the test.\n",
    "    \"\"\"\n",
    "    corrected_alpha = alpha / bonferroni_correction\n",
    "    significant_results = 0\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Generate data with the specified effect size\n",
    "        global data1\n",
    "        data1 = np.random.normal(0, 1, sample_size)\n",
    "        global data2\n",
    "        data2 = data1 + np.random.normal(effect_size, 1, sample_size)\n",
    "        \n",
    "        \n",
    "        # Perform the Wilcoxon signed-rank test\n",
    "        _, p_value = stats.wilcoxon(data1, data2)\n",
    "\n",
    "        if p_value < corrected_alpha:\n",
    "            significant_results += 1\n",
    "\n",
    "    # Estimate the power\n",
    "    power = significant_results / num_simulations\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cebb3b",
   "metadata": {},
   "source": [
    "# Step 19: <a class=\"anchor\" id=\"step-19\"></a> Functions to Generate Plots\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_log_formatter(val, pos=None):\n",
    "    if val != 0:\n",
    "        log_val = np.log10(val)\n",
    "        #if log_val % 2 == 0:  # Check if the log value is an even number\n",
    "        return r'$10^{%d}$' % int(log_val)\n",
    "        #else:\n",
    "        #    return ''  # Return an empty string for odd powers\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "\n",
    "def plot_boxplot_gridwithtest(inputdictionarylistname, inputdictionarylist, version, varianttype, varianttypename, popdescrips1, popdescrips2, makeppt, normalizationterm, outputdir):\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\n",
    "            'Group', 'Population Description(s)', 'ClinVar Clinical Significance', 'Total Unique Variants', 'Total Allele Count', 'p-value',\n",
    "        ])\n",
    "    \n",
    "    #fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    figheight = 4*len(inputdictionarylist)\n",
    "    fig, axes = plt.subplots(6, 1, figsize=(9, figheight))  # Change the number of rows and columns\n",
    "   \n",
    "    significancelist = [\n",
    "        \"Variant of Uncertain Significance\",\n",
    "        \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Benign or Likely Benign\",\n",
    "        \"Conflicting Interpretations\",\n",
    "        \"No Designation\"]\n",
    "\n",
    "    figuretitle = inputdictionarylistname + '_' + varianttypename + \"_gnomad\" + version + '_'\n",
    "    fig.suptitle(figuretitle, fontsize=12)\n",
    "    color_palette = {'non-European': '#1f77b4', 'European': '#ff7f0e'}\n",
    "\n",
    "    for row, significance in enumerate(significancelist):\n",
    "\n",
    "        finaltable, dfforplot = dfforplots(inputdictionarylist, version, varianttype, significance, popdescrips1, popdescrips2, normalizationterm)\n",
    "        print('finished for ' + significance)\n",
    "        line_color = 'black'\n",
    "        finaltablewithgenenames = add_gene_column(finaltable, 'inputs/biomartensg.txt')\n",
    "        groupeddf = finaltablewithgenenames.groupby(['Group', 'ClinVar Clinical Significance']).sum().reset_index()\n",
    "        finaltablewithgenenames['Difference in Variants in popdescrips'] = finaltablewithgenenames['Variant in popdescrips1'] - finaltablewithgenenames['Variant in popdescrips2']\n",
    "                \n",
    "        parameters = {\n",
    "                    'data': dfforplot,\n",
    "                    'order': list(inputdictionarylist.keys()),\n",
    "                    'y': 'Group',  \n",
    "                    'x': 'Allele Prevalence',  \n",
    "                    'hue': 'Population',\n",
    "                    'showfliers': False,\n",
    "                    'notch': False,\n",
    "                    'orient': 'h',\n",
    "                    'saturation': 1,\n",
    "                    'width': 0.7}\n",
    "        \n",
    "        dfnozero = filter_zeros(dfforplot, \n",
    "                                col1='Allele Prevalence of ClinSigGroup popdescrips1',\n",
    "                                col2='Allele Prevalence of ClinSigGroup popdescrips2',\n",
    "                                col3='ENSG',\n",
    "                                col4='Group')\n",
    "               \n",
    "        #ax = axes[row, col]\n",
    "        ax = axes[row]\n",
    "        \n",
    "        sns.stripplot(ax=ax, \n",
    "                      data=dfnozero, \n",
    "                      x=\"Allele Prevalence\", \n",
    "                      y=\"Group\", \n",
    "                      size=3, \n",
    "                      hue='Population',\n",
    "                      jitter=True,\n",
    "                      dodge=True,\n",
    "                      marker='o',\n",
    "                      palette=color_palette,\n",
    "                      alpha=0.5)\n",
    "        \n",
    "        parameters['data'] = dfnozero\n",
    "        sns.boxplot(ax=ax, **parameters, color='white', linewidth = 2,\n",
    "                    whiskerprops={'color': line_color},\n",
    "                    capprops={'color': line_color},\n",
    "                    medianprops={'color': line_color},\n",
    "                    boxprops={'edgecolor': line_color})\n",
    "        \n",
    "        parameters['data'] = dfforplot\n",
    "        ax.set_xscale('symlog', linthresh=1e-5)\n",
    "        \n",
    "        # Apply the custom tick formatter to the x-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(custom_log_formatter))\n",
    "        ax.set_xlim(left=1e-5)                \n",
    "        ax.grid(False)\n",
    "\n",
    "\n",
    "        global testylabels\n",
    "        testylabels = ax.get_yticklabels()\n",
    "        \n",
    "        for i, label in enumerate(testylabels):\n",
    "            current_text = label.get_text()\n",
    "            new_text = f\"{current_text}\\n({count_unique_values(dfforplot, 'ENSG', 'Group', current_text)} genes)\"\n",
    "            #new_text = ''\n",
    "            label.set_text(new_text)\n",
    "\n",
    "        ax.set_title(significance)\n",
    "        ax.set_yticklabels(testylabels)  # Change to y-tick labels\n",
    "        ax.set_xlabel('Allele Prevalence', fontsize=14)  # Change to x-label\n",
    "        ax.set_ylabel('', fontsize=0)\n",
    "        \n",
    "        # Set font to Helvetica for all elements\n",
    "        #prop = fm.FontProperties(fname=font_path)\n",
    "        plt.setp(ax.get_xticklabels())\n",
    "        plt.setp(ax.get_yticklabels())\n",
    "        ax.set_xlabel(ax.get_xlabel())\n",
    "        ax.set_ylabel(ax.get_ylabel())  # Use y-label as x-label\n",
    "\n",
    "\n",
    "        \n",
    "        groupsxaxis = dfforplot['Group'].unique()\n",
    "        pair_list = []\n",
    "\n",
    "        for group in groupsxaxis:\n",
    "            populations = dfforplot['Population'].unique()\n",
    "            pairs = list(combinations(populations, 2))\n",
    "            group_pairs = []\n",
    "            for pair in pairs:\n",
    "                group_pairs.extend([[(group, pair[0]), (group, pair[1])]])\n",
    "            pair_list.extend(group_pairs)\n",
    "\n",
    "        #formatted_pairs = \"[\" + \",\".join(map(str, pair_list)) + \"]\"\n",
    "        #print(formatted_pairs)\n",
    "        #annotator = Annotator(ax, formatted_pairs, plot='boxplot', **parameters)\n",
    "        #annotator.configure(test=\"Wilcoxon\", comparisons_correction=\"BF\", verbose=False, loc=\"outside\").apply_and_annotate()\n",
    "\n",
    "        annotator = Annotator(ax, pairs = pair_list, plot='boxplot', **parameters)\n",
    "\n",
    "        #adjusting alpha value for Bonferroni correction\n",
    "        #starting with alpha of 0.05 and dividing by total number of statistical tests\n",
    "        #total of 1000 statistical tests done in this paper\n",
    "        #using n of 1000\n",
    "        totaltests=210\n",
    "        \n",
    "        if varianttypename == 'codingwithoutmissense':\n",
    "            totaltests=126\n",
    "        \n",
    "        \n",
    "        if varianttypename == 'noncoding':\n",
    "            totaltests=140\n",
    "        \n",
    "        alpha=0.05\n",
    "        onestar=alpha/totaltests\n",
    "        twostar=onestar/2\n",
    "        threestar=twostar/2\n",
    "        fourstar=threestar/10\n",
    "        \n",
    "        annotator._pvalue_format.pvalue_thresholds =  [[fourstar, '****'], [threestar, '***'], [twostar, '**'], [onestar, '*'], [1, 'ns']]\n",
    "        annotator.configure(test=\"Wilcoxon\", correction_format='replace', loc=\"outside\", alpha=alpha)\n",
    "     \n",
    "        global fromannotationother, test_results\n",
    "        fromannotationother, test_results = annotator.apply_and_annotate()\n",
    "#         for pair,res in zip(pair_list, test_results): \n",
    "#             print(pair[0][0])\n",
    "#             print(pair[0][1])\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[0][0], clinvar_value = significance, colname = 'Variant in popdescrips1'))\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[0][0], clinvar_value = significance, colname = 'Summed Allele Count popdescrips1'))\n",
    "#             print(round_to_significant_figures(res.data.pvalue, 3))\n",
    "            \n",
    "#             print(pair[1][0])\n",
    "#             print(pair[1][1])\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[1][0], clinvar_value = significance, colname = 'Variant in popdescrips2'))\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[1][0], clinvar_value = significance, colname = 'Summed Allele Count popdescrips2'))\n",
    "#             print(round_to_significant_figures(res.data.pvalue, 3))\n",
    "            \n",
    "        # Define an empty DataFrame with the desired column names\n",
    "\n",
    "        #seems to be a weird bug in the annotation package such that the second Wilcoxon test gets placed a the first test\n",
    "        #thus it mismatche the order in pair_list\n",
    "        #just switching the first two element in pair_list to match\n",
    "        #pair_list[0], pair_list[1] = pair_list[1], pair_list[0]\n",
    "        \n",
    "        for pair, res in zip(pair_list, test_results):\n",
    "#             group1, group2 = pair\n",
    "#             data1 = dfforplot[(dfforplot['Group'] == group1[0]) & (dfforplot['Population'] == group1[1])]['Allele Prevalence']\n",
    "#             data2 = dfforplot[(dfforplot['Group'] == group2[0]) & (dfforplot['Population'] == group2[1])]['Allele Prevalence']\n",
    "    \n",
    "#             # Ensure data is ranked\n",
    "#             data1_ranked = rankdata(data1)\n",
    "#             data2_ranked = rankdata(data2)\n",
    "\n",
    "#             # Now, calculate the Rank Biserial Correlation for paired samples\n",
    "#             x = [0 if d1 < d2 else 1 for d1, d2 in zip(data1, data2)]\n",
    "#             y = data1_ranked + data2_ranked  # Rank of the paired samples\n",
    "\n",
    "#             rankbiserialcoeff = rankbiserial(x, y)\n",
    "            \n",
    "            newdfforplot = dfforplot[(dfforplot['Group'] == pair[0][0])]\n",
    "            result = run_r_ggstatsplot(newdfforplot)\n",
    "                       \n",
    "            pvalue = result[4][0]\n",
    "            rankbiserialcoeff = result[0][0]\n",
    "            cilow = result[1][0]\n",
    "            cihigh = result[2][0]\n",
    "            effectsize = result[3][0]\n",
    "            \n",
    "            numgenepairs = int(len(newdfforplot)/2)\n",
    "            \n",
    "            totaluniquevariants1 = get_variant_value(df=groupeddf, group_value=pair[0][0], clinvar_value=significance, colname='Variant in popdescrips1')\n",
    "            totaluniquevariants2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant in popdescrips2')\n",
    "            \n",
    "            totalallelecount1 = get_variant_value(df=groupeddf, group_value=pair[0][0], clinvar_value=significance, colname='Summed Allele Count popdescrips1')\n",
    "            totalallelecount2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Summed Allele Count popdescrips2')\n",
    "            \n",
    "            uniquevariants1 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant only in popdescrips1')\n",
    "            uniquevariants2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant only in popdescrips2')\n",
    "            uniquevariantsinboth = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant in both popdescrips')\n",
    "            percentuniquevariants1 = round_to_significant_figures(uniquevariants1 / totaluniquevariants1 * 100, 3)\n",
    "            percentuniquevariants2 = round_to_significant_figures(uniquevariants2 / totaluniquevariants2 * 100, 3)\n",
    "            percentuniquevariantsinboth1 = round_to_significant_figures(uniquevariantsinboth / totaluniquevariants1 * 100, 3)\n",
    "            percentuniquevariantsinboth2 = round_to_significant_figures(uniquevariantsinboth / totaluniquevariants2 * 100, 3)\n",
    "            \n",
    "            \n",
    "            rawpower = simulate_wilcoxon_power(effect_size = rankbiserialcoeff, \n",
    "                                                    sample_size = numgenepairs, \n",
    "                                                    alpha = alpha, \n",
    "                                                    bonferroni_correction = totaltests, \n",
    "                                                    num_simulations=50000)\n",
    "            \n",
    "            powerestimate = round_to_significant_figures(rawpower, 3)\n",
    "            \n",
    "            pvaluerow1 = round_to_significant_figures(pvalue, 3)\n",
    "            \n",
    "            if pvalue <= onestar:\n",
    "                significantornot = 'Yes'\n",
    "            else:\n",
    "                significantornot = 'No'\n",
    "            \n",
    "            # Assign the values to the corresponding cells in the DataFrame\n",
    "            row1_data = {\n",
    "                'Group': pair[0][0],\n",
    "                'Population Description(s)': pair[0][1],\n",
    "                'ClinVar Clinical Significance': significance,\n",
    "                '# of Gene Pairs': numgenepairs,\n",
    "                'Total Unique Variants': format(totaluniquevariants1, ','),\n",
    "                'Total Allele Count': format(totalallelecount1, ','),\n",
    "                'Unique Variants in Only One Group': format(uniquevariants1, ',')+' ('+format(percentuniquevariants1, ',')+'%)',\n",
    "                'Unique Variants in Both Groups': format(uniquevariantsinboth, ',')+' ('+format(percentuniquevariantsinboth1, ',')+'%)', \n",
    "                'p-value': pvaluerow1,\n",
    "                'Rank Biserial Coefficient': round_to_significant_figures(rankbiserialcoeff, 3),\n",
    "                '95% CI Lower Bound': str(round_to_significant_figures(cilow, 3)),\n",
    "                '95% CI Upper Bound': str(round_to_significant_figures(cihigh, 3)),\n",
    "                'Effect Size': effectsize,\n",
    "                'Estimated Statistical Power': powerestimate,\n",
    "                'Statistically Significant Difference': significantornot}\n",
    "            output_df = output_df.append(row1_data, ignore_index=True)\n",
    "            \n",
    "            \n",
    "            row2_data = {\n",
    "                'Group': ' ',\n",
    "                'Population Description(s)': pair[1][1],\n",
    "                'ClinVar Clinical Significance': ' ',\n",
    "                '# of Gene Pairs': ' ',\n",
    "                'Total Unique Variants': format(totaluniquevariants2, ','),\n",
    "                'Total Allele Count': format(totalallelecount2, ','),\n",
    "                'Unique Variants in Only One Group': format(uniquevariants2, ',')+' ('+format(percentuniquevariants2, ',')+'%)',\n",
    "                'Unique Variants in Both Groups': format(uniquevariantsinboth, ',')+' ('+format(percentuniquevariantsinboth2, ',')+'%)', \n",
    "                'p-value': ' ',\n",
    "                'Rank Biserial Coefficient': ' ',\n",
    "                '95% CI Lower Bound': ' ',\n",
    "                '95% CI Upper Bound': ' ',                \n",
    "                'Effect Size': ' ',\n",
    "                'Estimated Statistical Power': ' ',\n",
    "                'Statistically Significant Difference': ' '}\n",
    "            output_df = output_df.append(row2_data, ignore_index=True)             \n",
    "            \n",
    "            row3_data = {\n",
    "                'Group': ' ',\n",
    "                'Population Description(s)': ' ',\n",
    "                'ClinVar Clinical Significance': ' ',\n",
    "                '# of Gene Pairs': ' ',\n",
    "                'Total Unique Variants': ' ',\n",
    "                'Total Allele Count': ' ',\n",
    "                'Unique Variants in Both Groups': ' ',\n",
    "                'p-value': ' ',\n",
    "                'Rank Biserial Coefficient': ' ',\n",
    "                '95% CI Lower Bound': ' ',\n",
    "                '95% CI Upper Bound': ' ',\n",
    "                'Unique Variants in Only One Group': ' ',\n",
    "                'Effect Size': ' ',\n",
    "                'Estimated Statistical Power': ' ',\n",
    "                'Statistically Significant Difference': ' '}\n",
    "            output_df = output_df.append(row3_data, ignore_index=True)\n",
    "        \n",
    "        ax.legend().set_visible(False)  # Remove the legend from each plot\n",
    "        column_order = ['Group', \n",
    "                'Population Description(s)',\n",
    "                'ClinVar Clinical Significance',\n",
    "                '# of Gene Pairs',\n",
    "                'Total Allele Count',\n",
    "                'Total Unique Variants',\n",
    "                'p-value',\n",
    "                'Statistically Significant Difference',\n",
    "                'Rank Biserial Coefficient',\n",
    "                '95% CI Lower Bound',\n",
    "                '95% CI Upper Bound',\n",
    "                'Effect Size',\n",
    "                'Estimated Statistical Power',\n",
    "                'Unique Variants in Both Groups',\n",
    "                'Unique Variants in Only One Group']                       \n",
    "        output_df = output_df.reindex(columns = column_order)\n",
    "    \n",
    "    \n",
    "    # Create a separate legend for the entire set of plots\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.5)  # Adjust top spacing between subplots\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot to a pdf\n",
    "    pdfimagepath = os.path.join(outputdir,figuretitle+'plots.pdf')\n",
    "    fig.savefig(pdfimagepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    #Redefine same variables to allow for usage in ppt cause it needs png\n",
    "    pngimagepath = 'temp_plot.png'\n",
    "    fig.savefig(pngimagepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    output_df.to_csv(os.path.join(outputdir, figuretitle+'dataframe.csv'), index = False)\n",
    "    save_dataframe_to_png(df = output_df, filename = os.path.join(outputdir,figuretitle+'dataframe.png'))\n",
    "    save_dataframe_to_pdf(df = output_df, filename = os.path.join(outputdir,figuretitle+'dataframe.pdf'))\n",
    "    \n",
    "        \n",
    "    \n",
    "    if makeppt:\n",
    "        # Create a new PowerPoint presentation and a slide\n",
    "        prs = Presentation(os.path.join(outputdir,'everyvarianttype_boxplots_presentation.pptx'))\n",
    "        slide_layout = prs.slide_layouts[5]  # Use the title only layout\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        title = slide.shapes.title\n",
    "        title.text = figuretitle\n",
    "\n",
    "        # Define image path and insert it to the slide\n",
    "        left = Inches(0.5)\n",
    "        top = Inches(1.5)\n",
    "        width = Inches(6)\n",
    "        height = Inches(45)\n",
    "        slide.shapes.add_picture(pngimagepath, left, top, width, height)\n",
    "\n",
    "\n",
    "        # Define image path and insert it to the slide\n",
    "        left = Inches(6.5)\n",
    "        top = Inches(1.5)\n",
    "        width = Inches(30)\n",
    "        height = Inches(45)\n",
    "        slide.shapes.add_picture(os.path.join(outputdir,figuretitle+'dataframe.png'), left, top, width, height)\n",
    "\n",
    "        # Save the PowerPoint presentation\n",
    "        prs.save(os.path.join(outputdir,'everyvarianttype_boxplots_presentation.pptx'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_log_formatter(val, pos=None):\n",
    "    if val != 0:\n",
    "        log_val = np.log10(val)\n",
    "        #if log_val % 2 == 0:  # Check if the log value is an even number\n",
    "        return r'$10^{%d}$' % int(log_val)\n",
    "        #else:\n",
    "        #    return ''  # Return an empty string for odd powers\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "\n",
    "ancestry_v2 = {\n",
    "    'European (non-Finnish)': 64603,\n",
    "    'African/African American': 12487,\n",
    "    'Latino/Admixed American': 17720,\n",
    "    'East Asian': 9977,\n",
    "    'South Asian': 15308,\n",
    "    'Other': 3614}\n",
    "\n",
    "\n",
    "ancestry_v3nonv2 = {\n",
    "    'European (non-Finnish)': 25988,\n",
    "    'African/African American': 14377,\n",
    "    'Latino/Admixed American': 6878,\n",
    "    'East Asian': 1414,\n",
    "    'South Asian': 1946,\n",
    "    'Other': 932}\n",
    "\n",
    "ancestry_aou = {\n",
    "    'European': 123072,\n",
    "    'African/African American': 53944,\n",
    "    'Latino/Admixed American': 40838,\n",
    "    'East Asian': 5381,\n",
    "    'South Asian': 2342,\n",
    "    'Other': 19289\n",
    "}    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def popbreakplot_boxplot_gridwithtest(inputdictionarylistname, inputdictionarylist, version, varianttype, varianttypename, popdescrips1, popdescrips2, makeppt, normalizationterm, outputdir, pair_list=[]):\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\n",
    "            'Group', 'Population Description(s)', 'ClinVar Clinical Significance', 'Total Unique Variants', 'Total Allele Count', 'p-value'])\n",
    "    \n",
    "    #fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    figheight = 20*len(inputdictionarylist)\n",
    "    fig, axes = plt.subplots(6, 1, figsize=(12, figheight))  # Change the number of rows and columns\n",
    "   \n",
    "    #fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    #figheight = 5*len(varianttype)\n",
    "    #fig, axes = plt.subplots(6, 1, figsize=(12, figheight))  # Change the number of rows and columns\n",
    "\n",
    "\n",
    "    significancelist = [\n",
    "        \"Variant of Uncertain Significance\",\n",
    "        \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Benign or Likely Benign\",\n",
    "        \"Conflicting Interpretations\",\n",
    "        \"No Designation\"]\n",
    "\n",
    "    figuretitle = inputdictionarylistname + '_' + varianttypename + \"_gnomad\" + version + '_'\n",
    "    fig.suptitle(figuretitle, fontsize=12)\n",
    "    color_palette = {'European (non-Finnish)': '#ff7f0e',\n",
    "                     'African/African American': '#000fff',\n",
    "                     'Latino/Admixed American': '#4079b9',\n",
    "                     'East Asian': '#5067b2',\n",
    "                     'South Asian': '#54c2ff',\n",
    "                     'Other': '#418fff'}\n",
    "\n",
    "    for row, significance in enumerate(significancelist):\n",
    "        global dfforplot\n",
    "        global finaltable\n",
    "        finaltable, dfforplot = popbreakdfforplots(inputdictionarylist, version, varianttype, significance, popdescrips1, popdescrips2, normalizationterm)\n",
    "        print('finished for ' + significance)\n",
    "        line_color = 'black'\n",
    "        finaltablewithgenenames = add_gene_column(finaltable, 'inputs/biomartensg.txt')\n",
    "        groupeddf = finaltablewithgenenames.groupby(['Group', 'ClinVar Clinical Significance']).sum().reset_index()\n",
    "        #finaltablewithgenenames['Difference in Variants in popdescrips'] = finaltablewithgenenames['Variant in popdescrips1'] - finaltablewithgenenames['Variant in popdescrips2']\n",
    "                \n",
    "        parameters = {\n",
    "                    'data': dfforplot,\n",
    "                    'order': list(inputdictionarylist.keys()),\n",
    "                    'y': 'Group',  \n",
    "                    'x': 'Allele Prevalence',  \n",
    "                    'hue': 'Population',\n",
    "                    'showfliers': False,\n",
    "                    'notch': False,\n",
    "                    'orient': 'h',\n",
    "                    'saturation': 1,\n",
    "                    'width': 0.8}\n",
    "        \n",
    "        global dfnozero\n",
    "        dfnozero = filter_zeros_multicolumn(dfforplot, \n",
    "                                col1='Allele Prevalence of ClinSigGroup European (non-Finnish)',\n",
    "                                col2='Allele Prevalence of ClinSigGroup African/African American',\n",
    "                                col3='Allele Prevalence of ClinSigGroup Latino/Admixed American',\n",
    "                                col4='Allele Prevalence of ClinSigGroup East Asian',\n",
    "                                col5='Allele Prevalence of ClinSigGroup South Asian',\n",
    "                                col6='Allele Prevalence of ClinSigGroup Other',\n",
    "                                col7='ENSG',\n",
    "                                col8='Group')\n",
    "               \n",
    "        #ax = axes[row, col]\n",
    "        ax = axes[row]\n",
    "        \n",
    "        sns.stripplot(ax=ax, \n",
    "                      data=dfnozero, \n",
    "                      x=\"Allele Prevalence\", \n",
    "                      y=\"Group\", \n",
    "                      size=3, \n",
    "                      hue='Population',\n",
    "                      jitter=True,\n",
    "                      dodge=True,\n",
    "                      marker='o',\n",
    "                      palette=color_palette,\n",
    "                      alpha=0.5)\n",
    "        \n",
    "        parameters['data'] = dfnozero\n",
    "        sns.boxplot(ax=ax, **parameters, color='white', linewidth = 2,\n",
    "                    whiskerprops={'color': line_color},\n",
    "                    capprops={'color': line_color},\n",
    "                    medianprops={'color': line_color},\n",
    "                    boxprops={'edgecolor': line_color})\n",
    "        \n",
    "        parameters['data'] = dfforplot\n",
    "        ax.set_xscale('symlog', linthresh=1e-5)\n",
    "        \n",
    "        # Apply the custom tick formatter to the x-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(custom_log_formatter))\n",
    "        ax.set_xlim(left=1e-5)                \n",
    "        ax.grid(False)\n",
    "\n",
    "        global testylabels\n",
    "        testylabels = ax.get_yticklabels()\n",
    "        \n",
    "        for i, label in enumerate(testylabels):\n",
    "            current_text = label.get_text()\n",
    "            new_text = f\"{current_text}\\n({count_unique_values(dfforplot, 'ENSG', 'Group', current_text)} genes)\"\n",
    "            #new_text = ''\n",
    "            label.set_text(new_text)\n",
    "\n",
    "        ax.set_title(significance)\n",
    "        ax.set_yticklabels(testylabels)  # Change to y-tick labels\n",
    "        ax.set_xlabel('Allele Prevalence', fontsize=14)  # Change to x-label\n",
    "        ax.set_ylabel('', fontsize=0)\n",
    "        \n",
    "        # Set font to Helvetica for all elements\n",
    "        #prop = fm.FontProperties(fname=font_path)\n",
    "        plt.setp(ax.get_xticklabels())\n",
    "        plt.setp(ax.get_yticklabels())\n",
    "        ax.set_xlabel(ax.get_xlabel())\n",
    "        ax.set_ylabel(ax.get_ylabel())  # Use y-label as x-label\n",
    "        \n",
    "        groupsxaxis = dfforplot['Group'].unique()\n",
    "        \n",
    "        if not pair_list:\n",
    "            pair_list = []\n",
    "\n",
    "            for group in groupsxaxis:\n",
    "                populations = dfforplot['Population'].unique()\n",
    "                pairs = list(combinations(populations, 2))\n",
    "                group_pairs = []\n",
    "                for pair in pairs:\n",
    "                    group_pairs.extend([[(group, pair[0]), (group, pair[1])]])\n",
    "                pair_list.extend(group_pairs)\n",
    "\n",
    "        \n",
    "#         pair_list = [[('ACMG78', 'African/African American'), ('ACMG78', 'European (non-Finnish)')],\n",
    "#              [('ACMG78', 'Latino/Admixed American'), ('ACMG78', 'European (non-Finnish)')],\n",
    "#              [('ACMG78', 'East Asian'), ('ACMG78', 'European (non-Finnish)')],\n",
    "#              [('ACMG78', 'European (non-Finnish)'), ('ACMG78', 'South Asian')],\n",
    "#              [('ACMG78', 'European (non-Finnish)'), ('ACMG78', 'Other')]]\n",
    "        \n",
    "        #formatted_pairs = \"[\" + \",\".join(map(str, pair_list)) + \"]\"\n",
    "        #print(formatted_pairs)\n",
    "        #annotator = Annotator(ax, formatted_pairs, plot='boxplot', **parameters)\n",
    "        #annotator.configure(test=\"Wilcoxon\", comparisons_correction=\"BF\", verbose=False, loc=\"outside\").apply_and_annotate()\n",
    "\n",
    "        annotator = Annotator(ax, pairs = pair_list, plot='boxplot', **parameters)\n",
    "\n",
    "        #adjusting alpha value for Bonferroni correction\n",
    "        #starting with alpha of 0.05 and dividing by total number of statistical tests\n",
    "        #total of 1000 statistical tests done in this paper\n",
    "        #using n of 1000\n",
    "        totaltests=75\n",
    "        \n",
    "        alpha=0.05\n",
    "        onestar=alpha/totaltests\n",
    "        twostar=onestar/2\n",
    "        threestar=twostar/2\n",
    "        fourstar=threestar/10\n",
    "        \n",
    "        annotator._pvalue_format.pvalue_thresholds =  [[fourstar, '****'], [threestar, '***'], [twostar, '**'], [onestar, '*'], [1, 'ns']]\n",
    "        annotator.configure(test=\"Wilcoxon\", correction_format='replace', loc=\"outside\", alpha=alpha)\n",
    "     \n",
    "        global fromannotationother, test_results\n",
    "        fromannotationother, test_results = annotator.apply_and_annotate()\n",
    "        \n",
    "        ax.legend().set_visible(False)  # Remove the legend from each plot\n",
    "        \n",
    "#         for pair, res in zip(pair_list, test_results):\n",
    "\n",
    "#             print(pair)\n",
    "#             newdfforplot = dfforplot[(dfforplot['Group'] == pair[0][0])]\n",
    "#             result = popbreak_run_r_ggstatsplot(newdfforplot)\n",
    "                       \n",
    "#             pvalue = result[4][0]\n",
    "#             rankbiserialcoeff = result[0][0]\n",
    "#             cilow = result[1][0]\n",
    "#             cihigh = result[2][0]\n",
    "#             effectsize = result[3][0]\n",
    "            \n",
    "#             numgenepairs = int(len(newdfforplot)/2)\n",
    "            \n",
    "#             totaluniquevariants1 = get_variant_value(df=groupeddf, group_value=pair[0][0], clinvar_value=significance, colname='Variant in popdescrips1')\n",
    "#             totaluniquevariants2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant in popdescrips2')\n",
    "            \n",
    "#             totalallelecount1 = get_variant_value(df=groupeddf, group_value=pair[0][0], clinvar_value=significance, colname='Summed Allele Count popdescrips1')\n",
    "#             totalallelecount2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Summed Allele Count popdescrips2')\n",
    "            \n",
    "#             uniquevariants1 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant only in popdescrips1')\n",
    "#             uniquevariants2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant only in popdescrips2')\n",
    "#             uniquevariantsinboth = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant in both popdescrips')\n",
    "#             percentuniquevariants1 = round_to_significant_figures(uniquevariants1 / totaluniquevariants1 * 100, 3)\n",
    "#             percentuniquevariants2 = round_to_significant_figures(uniquevariants2 / totaluniquevariants2 * 100, 3)\n",
    "#             percentuniquevariantsinboth1 = round_to_significant_figures(uniquevariantsinboth / totaluniquevariants1 * 100, 3)\n",
    "#             percentuniquevariantsinboth2 = round_to_significant_figures(uniquevariantsinboth / totaluniquevariants2 * 100, 3)\n",
    "            \n",
    "#             pvaluerow1 = round_to_significant_figures(pvalue, 3)\n",
    "            \n",
    "#             if pvalue <= onestar:\n",
    "#                 significantornot = 'Yes'\n",
    "#             else:\n",
    "#                 significantornot = 'No'\n",
    "            \n",
    "#             # Assign the values to the corresponding cells in the DataFrame\n",
    "#             row1_data = {\n",
    "#                 'Group': pair[0][0],\n",
    "#                 'Population Description(s)': pair[0][1],\n",
    "#                 'ClinVar Clinical Significance': significance,\n",
    "#                 '# of Gene Pairs': numgenepairs,\n",
    "#                 'Total Unique Variants': format(totaluniquevariants1, ','),\n",
    "#                 'Total Allele Count': format(totalallelecount1, ','),\n",
    "#                 'Unique Variants in Only One Group': format(uniquevariants1, ',')+' ('+format(percentuniquevariants1, ',')+'%)',\n",
    "#                 'Unique Variants in Both Groups': format(uniquevariantsinboth, ',')+' ('+format(percentuniquevariantsinboth1, ',')+'%)', \n",
    "#                 'p-value': pvaluerow1,\n",
    "#                 'Rank Biserial Coefficient': round_to_significant_figures(rankbiserialcoeff, 3),\n",
    "#                 '95% CI Lower Bound': str(round_to_significant_figures(cilow, 3)),\n",
    "#                 '95% CI Upper Bound': str(round_to_significant_figures(cihigh, 3)),\n",
    "#                 'Effect Size': effectsize,\n",
    "#                 'Statistically Significant Difference': significantornot}\n",
    "#             output_df = output_df.append(row1_data, ignore_index=True)\n",
    "            \n",
    "            \n",
    "#             row2_data = {\n",
    "#                 'Group': ' ',\n",
    "#                 'Population Description(s)': pair[1][1],\n",
    "#                 'ClinVar Clinical Significance': ' ',\n",
    "#                 '# of Gene Pairs': ' ',\n",
    "#                 'Total Unique Variants': format(totaluniquevariants2, ','),\n",
    "#                 'Total Allele Count': format(totalallelecount2, ','),\n",
    "#                 'Unique Variants in Only One Group': format(uniquevariants2, ',')+' ('+format(percentuniquevariants2, ',')+'%)',\n",
    "#                 'Unique Variants in Both Groups': format(uniquevariantsinboth, ',')+' ('+format(percentuniquevariantsinboth2, ',')+'%)', \n",
    "#                 'p-value': ' ',\n",
    "#                 'Rank Biserial Coefficient': ' ',\n",
    "#                 '95% CI Lower Bound': ' ',\n",
    "#                 '95% CI Upper Bound': ' ',                \n",
    "#                 'Effect Size': ' ',\n",
    "#                 'Statistically Significant Difference': ' '}\n",
    "#             output_df = output_df.append(row2_data, ignore_index=True)             \n",
    "            \n",
    "#             row3_data = {\n",
    "#                 'Group': ' ',\n",
    "#                 'Population Description(s)': ' ',\n",
    "#                 'ClinVar Clinical Significance': ' ',\n",
    "#                 '# of Gene Pairs': ' ',\n",
    "#                 'Total Unique Variants': ' ',\n",
    "#                 'Total Allele Count': ' ',\n",
    "#                 'Unique Variants in Both Groups': ' ',\n",
    "#                 'p-value': ' ',\n",
    "#                 'Rank Biserial Coefficient': ' ',\n",
    "#                 '95% CI Lower Bound': ' ',\n",
    "#                 '95% CI Upper Bound': ' ',\n",
    "#                 'Unique Variants in Only One Group': ' ',\n",
    "#                 'Effect Size': ' ',\n",
    "#                 'Statistically Significant Difference': ' '}\n",
    "#             output_df = output_df.append(row3_data, ignore_index=True)\n",
    "        \n",
    "        \n",
    "#         column_order = ['Group', \n",
    "#                 'Population Description(s)',\n",
    "#                 'ClinVar Clinical Significance',\n",
    "#                 '# of Gene Pairs',\n",
    "#                 'Total Allele Count',\n",
    "#                 'Total Unique Variants',\n",
    "#                 'p-value',\n",
    "#                 'Statistically Significant Difference',\n",
    "#                 'Rank Biserial Coefficient',\n",
    "#                 '95% CI Lower Bound',\n",
    "#                 '95% CI Upper Bound',\n",
    "#                 'Effect Size',\n",
    "#                 'Unique Variants in Both Groups',\n",
    "#                 'Unique Variants in Only One Group']                       \n",
    "#         output_df = output_df.reindex(columns = column_order)\n",
    "          \n",
    "        \n",
    "    \n",
    "    # Create a separate legend for the entire set of plots\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    # Adding black borders to each subplot\n",
    "    for ax in axes:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('black')\n",
    "            spine.set_linewidth(1)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.5)  # Adjust top spacing between subplots\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot to a pdf\n",
    "    pdfimagepath = os.path.join(outputdir,figuretitle+'popbreakplots.pdf')\n",
    "    fig.savefig(pdfimagepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    #Redefine same variables to allow for usage in ppt cause it needs png\n",
    "    pngimagepath = 'temp_plot.png'\n",
    "    fig.savefig(pngimagepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "#     output_df.to_csv(os.path.join(outputdir, figuretitle+'popbreaksdataframe.csv'), index = False)\n",
    "#     save_dataframe_to_png(df = output_df, filename = os.path.join(outputdir,figuretitle+'popbreakdataframe.png'))\n",
    "#     save_dataframe_to_pdf(df = output_df, filename = os.path.join(outputdir,figuretitle+'popbreakdataframe.pdf'))\n",
    "    \n",
    "        \n",
    "    \n",
    "#     if makeppt:\n",
    "#         # Create a new PowerPoint presentation and a slide\n",
    "#         prs = Presentation(os.path.join(outputdir,'everyvarianttype_boxplots_presentation.pptx'))\n",
    "#         slide_layout = prs.slide_layouts[5]  # Use the title only layout\n",
    "#         slide = prs.slides.add_slide(slide_layout)\n",
    "#         title = slide.shapes.title\n",
    "#         title.text = figuretitle\n",
    "\n",
    "#         # Define image path and insert it to the slide\n",
    "#         left = Inches(0.5)\n",
    "#         top = Inches(1.5)\n",
    "#         width = Inches(6)\n",
    "#         height = Inches(45)\n",
    "#         slide.shapes.add_picture(pngimagepath, left, top, width, height)\n",
    "\n",
    "\n",
    "#         # Define image path and insert it to the slide\n",
    "#         left = Inches(6.5)\n",
    "#         top = Inches(1.5)\n",
    "#         width = Inches(30)\n",
    "#         height = Inches(45)\n",
    "#         slide.shapes.add_picture(os.path.join(outputdir,figuretitle+'dataframe.png'), left, top, width, height)\n",
    "\n",
    "#         # Save the PowerPoint presentation\n",
    "#         prs.save(os.path.join(outputdir,'everyvarianttype_boxplots_presentation.pptx'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_log_formatter(val, pos=None):\n",
    "    if val != 0:\n",
    "        log_val = np.log10(val)\n",
    "        #if log_val % 2 == 0:  # Check if the log value is an even number\n",
    "        return r'$10^{%d}$' % int(log_val)\n",
    "        #else:\n",
    "        #    return ''  # Return an empty string for odd powers\n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "\n",
    "def variantplot_boxplot_gridwithtest(inputdictionarylistname, inputdictionarylist, version, varianttype, varianttypename, popdescrips1, popdescrips2, makeppt, normalizationterm, outputdir):\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\n",
    "            'Group', 'Population Description(s)', 'ClinVar Clinical Significance', 'Total Unique Variants', 'Total Allele Count', 'p-value',\n",
    "        ])\n",
    "    \n",
    "    #fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    figheight = 5*len(varianttype)\n",
    "    fig, axes = plt.subplots(6, 1, figsize=(12, figheight))  # Change the number of rows and columns\n",
    "   \n",
    "    significancelist = [\n",
    "        \"Variant of Uncertain Significance\",\n",
    "        \"Pathogenic or Likely Pathogenic\",\n",
    "        \"Benign or Likely Benign\",\n",
    "        \"Conflicting Interpretations\",\n",
    "        \"No Designation\"]\n",
    "\n",
    "    figuretitle = inputdictionarylistname + '_' + varianttypename + \"_gnomad\" + version + '_'\n",
    "    #figuretitle = \" gnomad\" + version\n",
    "    fig.suptitle(figuretitle, fontsize=6)\n",
    "    color_palette = {'non-European': '#1f77b4', 'European': '#ff7f0e'}\n",
    "\n",
    "\n",
    "    #positions = [(row, col) for row in range(2) for col in range(3)]\n",
    "    #for (row, col), significance in zip(positions, significancelist):\n",
    "    for row, significance in enumerate(significancelist):\n",
    "        \n",
    "        finaltable, dfforplot = variantdfforplots(inputdictionarylist, version, varianttype, significance, popdescrips1, popdescrips2, normalizationterm)\n",
    "        print('finished for ' + significance)\n",
    "        line_color = 'black'\n",
    "        finaltablewithgenenames = add_gene_column(finaltable, 'inputs/biomartensg.txt')\n",
    "        \n",
    "        \n",
    "        groupeddf = finaltablewithgenenames.groupby(['Group', 'ClinVar Clinical Significance']).sum().reset_index()\n",
    "        \n",
    "        finaltablewithgenenames['Difference in Variants in popdescrips'] = finaltablewithgenenames['Variant in popdescrips1'] - finaltablewithgenenames['Variant in popdescrips2']\n",
    "\n",
    "        # Loop through each variant type and check if it exists in the data\n",
    "        valid_varianttype = []\n",
    "        for vt in varianttype:\n",
    "            if vt in dfforplot['Group'].unique():\n",
    "                valid_varianttype.append(vt)  # Add the variant type to the valid list if it exists\n",
    "\n",
    "        \n",
    "        parameters = {\n",
    "                    'data': dfforplot,\n",
    "                    'order': valid_varianttype,\n",
    "                    'y': 'Group',  \n",
    "                    'x': 'Allele Prevalence',  \n",
    "                    'hue': 'Population',\n",
    "                    'showfliers': False,\n",
    "                    'notch': False,\n",
    "                    'orient': 'h',\n",
    "                    'saturation': 1,\n",
    "                    'width': 0.7\n",
    "        }\n",
    "        \n",
    "        dfnozero = filter_zeros(dfforplot, \n",
    "                                col1='Allele Prevalence of ClinSigGroup popdescrips1',\n",
    "                                col2='Allele Prevalence of ClinSigGroup popdescrips2',\n",
    "                                col3='ENSG',\n",
    "                                col4='Group')\n",
    "               \n",
    "        #ax = axes[row, col]\n",
    "        ax = axes[row]\n",
    "        \n",
    "        sns.stripplot(ax=ax, \n",
    "                      data=dfnozero,\n",
    "                      order=valid_varianttype, \n",
    "                      x=\"Allele Prevalence\", \n",
    "                      y=\"Group\", \n",
    "                      size=3, \n",
    "                      hue='Population',\n",
    "                      jitter=True,\n",
    "                      dodge=True,\n",
    "                      marker='o',\n",
    "                      palette=color_palette,\n",
    "                      alpha=0.5)\n",
    "        \n",
    "        parameters['data'] = dfnozero\n",
    "        sns.boxplot(ax=ax, **parameters, color='white', linewidth = 2,\n",
    "                    whiskerprops={'color': line_color},\n",
    "                    capprops={'color': line_color},\n",
    "                    medianprops={'color': line_color},\n",
    "                    boxprops={'edgecolor': line_color})\n",
    "        \n",
    "        parameters['data'] = dfforplot\n",
    "        ax.set_xscale('symlog', linthresh=1e-5)\n",
    "        \n",
    "        #box = sns.boxplot(ax=ax, **parameters)\n",
    "        #ax.set_xscale(\"symlog\")\n",
    "        #ax.set_xlim([1e-8, ax.get_xlim()[1]])\n",
    "        # Set symlog scale for the x-axis\n",
    "        #ax.set_xscale('symlog', linthresh=1e-10, subs=[2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        #ax.set_xscale('symlog', linthresh=1e-10)\n",
    "\n",
    "        # Apply the custom tick formatter to the x-axis\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(custom_log_formatter))\n",
    "        ax.set_xlim(left=1e-5)        \n",
    "        ax.grid(False)\n",
    "        \n",
    "        #count_unique_values(df, 'ENSG', 'Group', 'label')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #finaltable\n",
    "        \n",
    "        #need to group before iterating and enumerating; this will save time\n",
    "        #for i, count in enumerate(dfforplot['Allele Representation']):\n",
    "        #    ax.text(count+1, i, f'1', va='center')\n",
    "        \n",
    "\n",
    "            \n",
    "#         current_labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "#         new_labels = [variantmappingdictforylabels.get(label, label) for label in current_labels]\n",
    "#         ax.set_yticklabels(new_labels)    \n",
    "        \n",
    "        global testylabelsoriginal\n",
    "        global testylabelstomodify\n",
    "        testylabelsoriginal = [label.get_text() for label in ax.get_yticklabels()]\n",
    "        testylabelstomodify = ax.get_yticklabels()\n",
    "    \n",
    "        for i, label in enumerate(testylabelstomodify):\n",
    "            current_text = label.get_text()\n",
    "            newlabel = variantmappingdictforylabels.get(current_text, current_text) \n",
    "            new_text = f\"{newlabel} ({count_unique_values(dfforplot, 'ENSG', 'Group', current_text)} genes)\\n({format(get_variant_value(df=groupeddf, group_value=current_text, clinvar_value=significance, colname='Summed Allele Count popdescrips1'), ',')} vs. {format(get_variant_value(df=groupeddf, group_value=current_text, clinvar_value=significance, colname='Summed Allele Count popdescrips2'), ',')})\"\n",
    "            label.set_text(new_text)\n",
    "        \n",
    "        ax.set_title(significance)        \n",
    "        ax.set_yticklabels(testylabelstomodify)  # Change to y-tick labels\n",
    "        ax.set_xlabel('Allele Prevalence')  # Change to x-label\n",
    "        ax.set_ylabel('', fontsize=0)\n",
    "\n",
    "        # Set font to Helvetica for all elements\n",
    "        #prop = fm.FontProperties(fname=font_path)\n",
    "        plt.setp(ax.get_xticklabels())\n",
    "        plt.setp(ax.get_yticklabels())\n",
    "        ax.set_xlabel(ax.get_xlabel())\n",
    "        ax.set_ylabel(ax.get_ylabel())  # Use y-label as x-label\n",
    "\n",
    "#         # Calculate Wilcoxon matched pairs signed rank test for each pair of populations\n",
    "#         groupsxaxis = dfforplot['Group'].unique()\n",
    "#         for group in groupsxaxis:\n",
    "#             populations = dfforplot['Population'].unique()\n",
    "#             pairs = combinations(populations, 2)\n",
    "#             for pair in pairs:\n",
    "#                 pop1 = pair[0]\n",
    "#                 pop2 = pair[1]\n",
    "#                 subsetdf = dfforplot[dfforplot['Group'] == group]\n",
    "#                 data1 = subsetdf[subsetdf['Population'] == pop1]['Allele Representation']\n",
    "#                 #print(sum(data1))\n",
    "#                 data2 = subsetdf[subsetdf['Population'] == pop2]['Allele Representation']\n",
    "#                 #print(sum(data2))\n",
    "#                 statistic, p_value = wilcoxon(data1, data2)\n",
    "#                 is_significant = p_value < 0.00005\n",
    "#                 #print(f\"Wilcoxon test using Bonferroni correction for {pop1} vs {pop2}\") \n",
    "#                 print(f\"{group}: statistic={statistic}, p-value={p_value}, is significant={is_significant}\")\n",
    "\n",
    "#[('GenCC', 'Non-European'),('GenCC', 'European'),('ACMG78', 'Non-European'),('ACMG78', 'European'),('Cancer', 'Non-European'),('Cancer', 'European'),('Cardiac', 'Non-European'),('Cardiac', 'European'),('DDG2P', 'Non-European'),('DDG2P', 'European')]\n",
    "\n",
    "        groupsxaxis = testylabelsoriginal\n",
    "        pair_list = []\n",
    "\n",
    "        for group in groupsxaxis:\n",
    "            populations = dfforplot['Population'].unique()\n",
    "            pairs = list(combinations(populations, 2))\n",
    "            group_pairs = []\n",
    "            for pair in pairs:\n",
    "                group_pairs.extend([[(group, pair[0]), (group, pair[1])]])\n",
    "            pair_list.extend(group_pairs)\n",
    "\n",
    "        #formatted_pairs = \"[\" + \",\".join(map(str, pair_list)) + \"]\"\n",
    "        #print(formatted_pairs)\n",
    "        #annotator = Annotator(ax, formatted_pairs, plot='boxplot', **parameters)\n",
    "        #annotator.configure(test=\"Wilcoxon\", comparisons_correction=\"BF\", verbose=False, loc=\"outside\").apply_and_annotate()\n",
    "\n",
    "        annotator = Annotator(ax, pairs = pair_list, plot='boxplot', **parameters)\n",
    "\n",
    "        #adjusting alpha value for Bonferroni correction\n",
    "        #starting with alpha of 0.05 and dividing by total number of statistical tests\n",
    "        #total of 1000 statistical tests done in this paper\n",
    "        #using n of 1000\n",
    "        totaltests=165\n",
    "        \n",
    "        if varianttypename == 'noncoding':\n",
    "            totaltests=120\n",
    "        \n",
    "        alpha=0.05\n",
    "        onestar=alpha/totaltests\n",
    "        twostar=onestar/2\n",
    "        threestar=twostar/2\n",
    "        fourstar=threestar/10\n",
    "        \n",
    "        annotator._pvalue_format.pvalue_thresholds =  [[fourstar, '****'], [threestar, '***'], [twostar, '**'], [onestar, '*'], [1, 'ns']]\n",
    "        annotator.configure(test=\"Wilcoxon\", correction_format='replace', loc=\"outside\", alpha=alpha)\n",
    "     \n",
    "        global fromannotationother, test_results\n",
    "        fromannotationother, test_results = annotator.apply_and_annotate()\n",
    "#         for pair,res in zip(pair_list, test_results): \n",
    "#             print(pair[0][0])\n",
    "#             print(pair[0][1])\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[0][0], clinvar_value = significance, colname = 'Variant in popdescrips1'))\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[0][0], clinvar_value = significance, colname = 'Summed Allele Count popdescrips1'))\n",
    "#             print(round_to_significant_figures(res.data.pvalue, 3))\n",
    "            \n",
    "#             print(pair[1][0])\n",
    "#             print(pair[1][1])\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[1][0], clinvar_value = significance, colname = 'Variant in popdescrips2'))\n",
    "#             print(get_variant_value(df = groupeddf, group_value = pair[1][0], clinvar_value = significance, colname = 'Summed Allele Count popdescrips2'))\n",
    "#             print(round_to_significant_figures(res.data.pvalue, 3))\n",
    "            \n",
    "        # Define an empty DataFrame with the desired column names\n",
    "\n",
    "        #seems to be a weird bug in the annotation package such that the second Wilcoxon test gets placed a the first test\n",
    "        #thus it mismatche the order in pair_list\n",
    "        #just switching the first two element in pair_list to match\n",
    "        #pair_list[0], pair_list[1] = pair_list[1], pair_list[0]\n",
    "        \n",
    "        for pair, res in zip(pair_list, test_results):\n",
    "#             group1, group2 = pair\n",
    "#             data1 = dfforplot[(dfforplot['Group'] == group1[0]) & (dfforplot['Population'] == group1[1])]['Allele Representation']\n",
    "#             data2 = dfforplot[(dfforplot['Group'] == group2[0]) & (dfforplot['Population'] == group2[1])]['Allele Representation']\n",
    "    \n",
    "#             # Ensure data is ranked\n",
    "#             data1_ranked = rankdata(data1)\n",
    "#             data2_ranked = rankdata(data2)\n",
    "\n",
    "#             # Now, calculate the Rank Biserial Correlation for paired samples\n",
    "#             x = [0 if d1 < d2 else 1 for d1, d2 in zip(data1, data2)]\n",
    "#             y = data1_ranked + data2_ranked  # Rank of the paired samples\n",
    "\n",
    "#             rankbiserialcoeff = rankbiserial(x, y)\n",
    "            \n",
    "            newdfforplot = dfforplot[(dfforplot['Group'] == pair[0][0])]\n",
    "            result = run_r_ggstatsplot(newdfforplot)\n",
    "                       \n",
    "            pvalue = result[4][0]\n",
    "            rankbiserialcoeff = result[0][0]\n",
    "            cilow = result[1][0]\n",
    "            cihigh = result[2][0]\n",
    "            effectsize = result[3][0]\n",
    "            \n",
    "            numgenepairs = int(len(newdfforplot)/2)\n",
    "            \n",
    "            totaluniquevariants1 = get_variant_value(df=groupeddf, group_value=pair[0][0], clinvar_value=significance, colname='Variant in popdescrips1')\n",
    "            totaluniquevariants2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant in popdescrips2')\n",
    "            \n",
    "            totalallelecount1 = get_variant_value(df=groupeddf, group_value=pair[0][0], clinvar_value=significance, colname='Summed Allele Count popdescrips1')\n",
    "            totalallelecount2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Summed Allele Count popdescrips2')\n",
    "            \n",
    "            uniquevariants1 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant only in popdescrips1')\n",
    "            uniquevariants2 = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant only in popdescrips2')\n",
    "            uniquevariantsinboth = get_variant_value(df=groupeddf, group_value=pair[1][0], clinvar_value=significance, colname='Variant in both popdescrips')\n",
    "            percentuniquevariants1 = round_to_significant_figures(uniquevariants1 / totaluniquevariants1 * 100, 3)\n",
    "            percentuniquevariants2 = round_to_significant_figures(uniquevariants2 / totaluniquevariants2 * 100, 3)\n",
    "            percentuniquevariantsinboth1 = round_to_significant_figures(uniquevariantsinboth / totaluniquevariants1 * 100, 3)\n",
    "            percentuniquevariantsinboth2 = round_to_significant_figures(uniquevariantsinboth / totaluniquevariants2 * 100, 3)\n",
    "            \n",
    "            pvaluerow1 = round_to_significant_figures(pvalue, 3)\n",
    "            \n",
    "            if pvalue <= onestar:\n",
    "                significantornot = 'Yes'\n",
    "            else:\n",
    "                significantornot = 'No'\n",
    "            \n",
    "            \n",
    "            rawpower = simulate_wilcoxon_power(effect_size = rankbiserialcoeff, \n",
    "                                                    sample_size = numgenepairs, \n",
    "                                                    alpha = alpha, \n",
    "                                                    bonferroni_correction = totaltests, \n",
    "                                                    num_simulations=50000)\n",
    "            \n",
    "            powerestimate = round_to_significant_figures(rawpower, 3)\n",
    "            \n",
    "            \n",
    "            # Assign the values to the corresponding cells in the DataFrame\n",
    "            row1_data = {\n",
    "                'Group': pair[0][0],\n",
    "                'Population Description(s)': pair[0][1],\n",
    "                'ClinVar Clinical Significance': significance,\n",
    "                '# of Gene Pairs': numgenepairs,\n",
    "                'Total Unique Variants': format(totaluniquevariants1, ','),\n",
    "                'Total Allele Count': format(totalallelecount1, ','),\n",
    "                'Unique Variants in Only One Group': format(uniquevariants1, ',')+' ('+format(percentuniquevariants1, ',')+'%)',\n",
    "                'Unique Variants in Both Groups': format(uniquevariantsinboth, ',')+' ('+format(percentuniquevariantsinboth1, ',')+'%)', \n",
    "                'p-value': pvaluerow1,\n",
    "                'Rank Biserial Coefficient': round_to_significant_figures(rankbiserialcoeff, 3),\n",
    "                '95% CI Lower Bound': str(round_to_significant_figures(cilow, 3)),\n",
    "                '95% CI Upper Bound': str(round_to_significant_figures(cihigh, 3)),\n",
    "                'Effect Size': effectsize,\n",
    "                'Estimated Statistical Power': powerestimate,\n",
    "                'Statistically Significant Difference': significantornot}\n",
    "            output_df = output_df.append(row1_data, ignore_index=True)\n",
    "            \n",
    "            row2_data = {\n",
    "                'Group': ' ',\n",
    "                'Population Description(s)': pair[1][1],\n",
    "                'ClinVar Clinical Significance': ' ',\n",
    "                '# of Gene Pairs': ' ',\n",
    "                'Total Unique Variants': format(totaluniquevariants2, ','),\n",
    "                'Total Allele Count': format(totalallelecount2, ','),\n",
    "                'Unique Variants in Only One Group': format(uniquevariants2, ',')+' ('+format(percentuniquevariants2, ',')+'%)',\n",
    "                'Unique Variants in Both Groups': format(uniquevariantsinboth, ',')+' ('+format(percentuniquevariantsinboth2, ',')+'%)', \n",
    "                'p-value': ' ',\n",
    "                'Rank Biserial Coefficient': ' ',\n",
    "                '95% CI Lower Bound': ' ',\n",
    "                '95% CI Upper Bound': ' ',                \n",
    "                'Effect Size': ' ',\n",
    "                'Estimated Statistical Power': ' ',\n",
    "                'Statistically Significant Difference': ' '}\n",
    "            output_df = output_df.append(row2_data, ignore_index=True)             \n",
    "            \n",
    "            row3_data = {\n",
    "                'Group': ' ',\n",
    "                'Population Description(s)': ' ',\n",
    "                'ClinVar Clinical Significance': ' ',\n",
    "                '# of Gene Pairs': ' ',\n",
    "                'Total Unique Variants': ' ',\n",
    "                'Total Allele Count': ' ',\n",
    "                'Unique Variants in Both Groups': ' ',\n",
    "                'p-value': ' ',\n",
    "                'Rank Biserial Coefficient': ' ',\n",
    "                '95% CI Lower Bound': ' ',\n",
    "                '95% CI Upper Bound': ' ',\n",
    "                'Unique Variants in Only One Group': ' ',\n",
    "                'Effect Size': ' ',\n",
    "                'Estimated Statistical Power': ' ',\n",
    "                'Statistically Significant Difference': ' '}\n",
    "            output_df = output_df.append(row3_data, ignore_index=True)\n",
    "        \n",
    "        ax.legend().set_visible(False)  # Remove the legend from each plot\n",
    "        column_order = ['Group', \n",
    "                'Population Description(s)',\n",
    "                'ClinVar Clinical Significance',\n",
    "                '# of Gene Pairs',\n",
    "                'Total Allele Count',\n",
    "                'Total Unique Variants',\n",
    "                'p-value',\n",
    "                'Statistically Significant Difference',\n",
    "                'Rank Biserial Coefficient',\n",
    "                '95% CI Lower Bound',\n",
    "                '95% CI Upper Bound',\n",
    "                'Effect Size',\n",
    "                'Estimated Statistical Power',\n",
    "                'Unique Variants in Both Groups',\n",
    "                'Unique Variants in Only One Group']                       \n",
    "        output_df = output_df.reindex(columns = column_order)\n",
    "\n",
    "        \n",
    "   \n",
    "    # Create a separate legend for the entire set of plots\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.5)  # Adjust top spacing between subplots\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save the plot to a pdf\n",
    "    pdfimagepath = os.path.join(outputdir,figuretitle+'plots.pdf')\n",
    "    fig.savefig(pdfimagepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    #Redefine same variables to allow for usage in ppt cause it needs png\n",
    "    pngimagepath = 'temp_plot.png'\n",
    "    fig.savefig(pngimagepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    output_df.to_csv(os.path.join(outputdir, figuretitle+'dataframe.csv'), index = False)\n",
    "    save_dataframe_to_png(df = output_df, filename = os.path.join(outputdir,figuretitle+'dataframe.png'))\n",
    "    save_dataframe_to_pdf(df = output_df, filename = os.path.join(outputdir,figuretitle+'dataframe.pdf'))\n",
    "    \n",
    "        \n",
    "    \n",
    "    if makeppt:\n",
    "        # Create a new PowerPoint presentation and a slide\n",
    "        prs = Presentation(os.path.join(outputdir,'everyvarianttype_boxplots_presentation.pptx'))\n",
    "        slide_layout = prs.slide_layouts[5]  # Use the title only layout\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        title = slide.shapes.title\n",
    "        title.text = figuretitle\n",
    "\n",
    "        # Define image path and insert it to the slide\n",
    "        left = Inches(0.5)\n",
    "        top = Inches(1.5)\n",
    "        width = Inches(6)\n",
    "        height = Inches(45)\n",
    "        slide.shapes.add_picture(pngimagepath, left, top, width, height)\n",
    "\n",
    "\n",
    "        # Define image path and insert it to the slide\n",
    "        left = Inches(6.5)\n",
    "        top = Inches(1.5)\n",
    "        width = Inches(30)\n",
    "        height = Inches(45)\n",
    "        slide.shapes.add_picture(os.path.join(outputdir,figuretitle+'dataframe.png'), left, top, width, height)\n",
    "\n",
    "        # Save the PowerPoint presentation\n",
    "        prs.save(os.path.join(outputdir,'everyvarianttype_boxplots_presentation.pptx'))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93016ab4",
   "metadata": {},
   "source": [
    "# Step 20: <a class=\"anchor\" id=\"step-20\"></a> Functions For Stats in R\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce213e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pandas_df_to_r(df):\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        r_df = robjects.conversion.py2rpy(df)\n",
    "    return r_df\n",
    "\n",
    "def convert_r_list_to_python(r_list):\n",
    "    # Convert R list elements to Python objects\n",
    "    python_dict = {}\n",
    "    for name, item in r_list.items():\n",
    "        if isinstance(item, robjects.vectors.DataFrame):\n",
    "            with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "                python_dict[name] = robjects.conversion.rpy2py(item)\n",
    "        else:\n",
    "            python_dict[name] = item\n",
    "    return python_dict\n",
    "\n",
    "def run_r_ggstatsplot(df):\n",
    "    # Enable the conversion between pandas DataFrame and R DataFrame\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    #if running into errors with this function try using the below line to start a with loop\n",
    "    #and indent all the code below and up to the return statement\n",
    "    #with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "    \n",
    "    # Import the R packages that are required\n",
    "    ggstatsplot = importr('ggstatsplot')\n",
    "\n",
    "    # Convert the pandas DataFrame to R DataFrame\n",
    "    df_r = convert_pandas_df_to_r(df)\n",
    "\n",
    "    # Define the R code\n",
    "    r_code1 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"non-European\")\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        estimate <- result$subtitle_data$estimate[1]\n",
    "        return (estimate)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code3 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"non-European\")\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        cilow <- result$subtitle_data$conf.low[1]\n",
    "        return (cilow)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code4 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"non-European\")\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        cihigh <- result$subtitle_data$conf.high[1]\n",
    "        return (cihigh)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code5 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"non-European\")\n",
    "        library(effectsize)\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        estimate <- result$subtitle_data$estimate[1]\n",
    "        interpret <- interpret_rank_biserial(estimate)\n",
    "        return (interpret)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code6 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"non-European\")\n",
    "        library(effectsize)\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        pvalue <- result$subtitle_data$p.value[1]\n",
    "        return (pvalue)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an R function from the string containing the R code\n",
    "    r_function1 = robjects.r(r_code1)\n",
    "    r_function3 = robjects.r(r_code3)\n",
    "    r_function4 = robjects.r(r_code4)\n",
    "    r_function5 = robjects.r(r_code5)\n",
    "    r_function6 = robjects.r(r_code6)\n",
    "\n",
    "\n",
    "    # Call the R function with the R DataFrame\n",
    "    rankcoeff = r_function1(df_r)\n",
    "    cilow = r_function3(df_r)\n",
    "    cihigh = r_function4(df_r)\n",
    "    interpret = r_function5(df_r)\n",
    "    pvalue = r_function6(df_r)\n",
    "    \n",
    "\n",
    "    # Return the Python result\n",
    "    return rankcoeff, cilow, cihigh, interpret, pvalue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def popbreak_run_r_ggstatsplot(df):\n",
    "    # Enable the conversion between pandas DataFrame and R DataFrame\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    # Import the R packages that are required\n",
    "    ggstatsplot = importr('ggstatsplot')\n",
    "\n",
    "    # Convert the pandas DataFrame to R DataFrame\n",
    "    df_r = convert_pandas_df_to_r(df)\n",
    "\n",
    "    # Define the R code\n",
    "    r_code1 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"European (non-Finnish)\")\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        estimate <- result$subtitle_data$estimate[1]\n",
    "        return (estimate)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code3 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"European (non-Finnish)\")\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        cilow <- result$subtitle_data$conf.low[1]\n",
    "        return (cilow)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code4 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"European (non-Finnish)\")\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        cihigh <- result$subtitle_data$conf.high[1]\n",
    "        return (cihigh)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code5 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"European (non-Finnish)\")\n",
    "        library(effectsize)\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        estimate <- result$subtitle_data$estimate[1]\n",
    "        interpret <- interpret_rank_biserial(estimate)\n",
    "        return (interpret)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    r_code6 = \"\"\"\n",
    "    function(data) {\n",
    "        library(ggstatsplot)\n",
    "        data$Population <- as.factor(data$Population)\n",
    "        data$Population <- relevel(data$Population, ref = \"European (non-Finnish)\")\n",
    "        library(effectsize)\n",
    "        p <- ggwithinstats(\n",
    "            data = data,\n",
    "            x = 'Population',\n",
    "            y = 'Allele Prevalence',\n",
    "            type = 'nonparametric'\n",
    "        )\n",
    "        result <- extract_stats(p)\n",
    "        pvalue <- result$subtitle_data$p.value[1]\n",
    "        return (pvalue)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an R function from the string containing the R code\n",
    "    r_function1 = robjects.r(r_code1)\n",
    "    r_function3 = robjects.r(r_code3)\n",
    "    r_function4 = robjects.r(r_code4)\n",
    "    r_function5 = robjects.r(r_code5)\n",
    "    r_function6 = robjects.r(r_code6)\n",
    "\n",
    "\n",
    "    # Call the R function with the R DataFrame\n",
    "    rankcoeff = r_function1(df_r)\n",
    "    cilow = r_function3(df_r)\n",
    "    cihigh = r_function4(df_r)\n",
    "    interpret = r_function5(df_r)\n",
    "    pvalue = r_function6(df_r)\n",
    "    \n",
    "\n",
    "    # Return the Python result\n",
    "    return rankcoeff, cilow, cihigh, interpret, pvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca91b9",
   "metadata": {},
   "source": [
    "# Step 21: <a class=\"anchor\" id=\"step-21\"></a> Functions to Trigger Generating Combination Boxplots\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations_and_plot(inputdictionarylists, versions, varianttype_dict, makeppt, normalizationterm, outputdir):\n",
    "    for (inputdict_name, inputdict_dicts), version, (varianttype_name, varianttype_list) in product(inputdictionarylists.items(), versions, varianttype_dict.items()):\n",
    "        parameters = {\n",
    "            'inputdictionarylistname': inputdict_name,\n",
    "            'inputdictionarylist': inputdict_dicts, \n",
    "            'version': version, \n",
    "            'varianttype': varianttype_list,\n",
    "            'varianttypename': varianttype_name,\n",
    "            'popdescrips1': popdescrips1, \n",
    "            'popdescrips2': popdescrips2,\n",
    "            'makeppt': makeppt,\n",
    "            'normalizationterm': normalizationterm,\n",
    "            'outputdir': outputdir\n",
    "        }\n",
    "        plot_boxplot_gridwithtest(**parameters)\n",
    "\n",
    "v2version = ['v2']  # List of versions\n",
    "v3version = ['v3']  # List of versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variantgenerate_combinations_and_plot(inputdictionarylists, versions, varianttype_dict, makeppt, normalizationterm, outputdir):\n",
    "    for (inputdict_name, inputdict_dicts), version, (varianttype_name, varianttype_list) in product(inputdictionarylists.items(), versions, varianttype_dict.items()):\n",
    "        parameters = {\n",
    "            'inputdictionarylistname': inputdict_name,\n",
    "            'inputdictionarylist': inputdict_dicts, \n",
    "            'version': version, \n",
    "            'varianttype': varianttype_list,\n",
    "            'varianttypename': varianttype_name,\n",
    "            'popdescrips1': popdescrips1, \n",
    "            'popdescrips2': popdescrips2,\n",
    "            'makeppt': makeppt,\n",
    "            'normalizationterm': normalizationterm,\n",
    "            'outputdir': outputdir\n",
    "        }\n",
    "        variantplot_boxplot_gridwithtest(**parameters)\n",
    "\n",
    "v2version = ['v2']  # List of versions\n",
    "v3version = ['v3']  # List of versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46594c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popbreakgenerate_combinations_and_plot(inputdictionarylists, versions, varianttype_dict, makeppt, normalizationterm, outputdir, pair_list = []):\n",
    "    for (inputdict_name, inputdict_dicts), version, (varianttype_name, varianttype_list) in product(inputdictionarylists.items(), versions, varianttype_dict.items()):\n",
    "        parameters = {\n",
    "            'inputdictionarylistname': inputdict_name,\n",
    "            'inputdictionarylist': inputdict_dicts, \n",
    "            'version': version, \n",
    "            'varianttype': varianttype_list,\n",
    "            'varianttypename': varianttype_name,\n",
    "            'popdescrips1': popdescrips1, \n",
    "            'popdescrips2': popdescrips2,\n",
    "            'makeppt': makeppt,\n",
    "            'normalizationterm': normalizationterm,\n",
    "            'outputdir': outputdir,\n",
    "            'pair_list': pair_list\n",
    "        }\n",
    "        popbreakplot_boxplot_gridwithtest(**parameters)\n",
    "\n",
    "v2version = ['v2']  # List of versions\n",
    "v3version = ['v3']  # List of versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6911cf",
   "metadata": {},
   "source": [
    "# Part 3: <a class=\"anchor\" id=\"part-3\"></a>Figures for Gene by Gene Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e425416",
   "metadata": {},
   "source": [
    "# Step 22: <a class=\"anchor\" id=\"step-22\"></a> Population Breakdown Plots\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffaa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair lists for comparisons\n",
    "genccpairlist = [[('GenCC', 'African/African American'), ('GenCC', 'European (non-Finnish)')],\n",
    "             [('GenCC', 'Latino/Admixed American'), ('GenCC', 'European (non-Finnish)')],\n",
    "             [('GenCC', 'East Asian'), ('GenCC', 'European (non-Finnish)')],\n",
    "             [('GenCC', 'South Asian'), ('GenCC', 'European (non-Finnish)')],\n",
    "             [('GenCC', 'Other'), ('GenCC', 'European (non-Finnish)')]]\n",
    "\n",
    "\n",
    "acmg78pairlist = [[('ACMG78', 'African/African American'), ('ACMG78', 'European (non-Finnish)')],\n",
    "             [('ACMG78', 'Latino/Admixed American'), ('ACMG78', 'European (non-Finnish)')],\n",
    "             [('ACMG78', 'East Asian'), ('ACMG78', 'European (non-Finnish)')],\n",
    "             [('ACMG78', 'South Asian'), ('ACMG78', 'European (non-Finnish)')],\n",
    "             [('ACMG78', 'Other'), ('ACMG78', 'European (non-Finnish)')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1480f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sixth plot for the majority of the boxplots made below will be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97276249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with all pairwise combinations that include euro v2 ACMG78 \n",
    "#popbreakgenerate_combinations_and_plot(inputdictionariesvariantmappinglistACMG78v2, v2version, varianttypemappingjustcoding, makeppt=False, normalizationterm='coding', outputdir = 'output', pair_list = acmg78pairlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with all pairwise combinations that include euro v3 ACMG78 \n",
    "#popbreakgenerate_combinations_and_plot(inputdictionariesvariantmappinglistACMG78v3, v3version, varianttypemappingjustcoding, makeppt=False, normalizationterm='coding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce595f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with all pairwise combinations that include euro v2 GenCC\n",
    "popbreakgenerate_combinations_and_plot(inputdictionariesvariantmappinglistGenCCv2, v2version, varianttypemappingjustcoding, makeppt=False, normalizationterm='coding', outputdir = 'output', pair_list = genccpairlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with all pairwise combinations that include euro v3 GenCC\n",
    "popbreakgenerate_combinations_and_plot(inputdictionariesvariantmappinglistGenCCv3, v3version, varianttypemappingjustcoding, makeppt=False, normalizationterm='coding', outputdir = 'output', pair_list = genccpairlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79a956",
   "metadata": {},
   "source": [
    "# Step 23: <a class=\"anchor\" id=\"step-23\"></a> Variant Type Breakdown Plots\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acecc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC VUS Coding gnomad v2 combined inframes \n",
    "variantgenerate_combinations_and_plot(inputdictionariesvariantmappinglistGenCCv2, v2version, varianttypemappingall, makeppt=False, normalizationterm='coding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC VUS Coding gnomad v3 combined inframes \n",
    "variantgenerate_combinations_and_plot(inputdictionariesvariantmappinglistGenCCv3, v3version, varianttypemappingall, makeppt=False, normalizationterm='coding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC VUS Noncoding gnomad v2 \n",
    "variantgenerate_combinations_and_plot(inputdictionariesvariantmappinglistGenCCv2, v2version, varianttypemappingnoncoding, makeppt=False, normalizationterm='noncoding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70335582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC VUS Noncoding gnomad v3\n",
    "variantgenerate_combinations_and_plot(inputdictionariesvariantmappinglistGenCCv3, v3version, varianttypemappingnoncoding, makeppt=False, normalizationterm='noncoding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffafcbde",
   "metadata": {},
   "source": [
    "# Step 24: <a class=\"anchor\" id=\"step-24\"></a> Allele Prevalence and Clinical Significance Breakdown Plots\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 noncoding only \n",
    "generate_combinations_and_plot(inputdictionarieslistv2, v2version, varianttypemappingnoncoding, makeppt=False, normalizationterm='noncoding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v3 noncoding only \n",
    "generate_combinations_and_plot(inputdictionarieslistv3, v3version, varianttypemappingnoncoding, makeppt=False, normalizationterm='noncoding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 coding\n",
    "generate_combinations_and_plot(inputdictionarieslistv2, v2version, varianttypemappingforpaper, makeppt=False, normalizationterm='coding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v3 coding\n",
    "generate_combinations_and_plot(inputdictionarieslistv3, v3version, varianttypemappingforpaper, makeppt=False, normalizationterm='coding', outputdir = 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bc8d3",
   "metadata": {},
   "source": [
    "# Step 25: <a class=\"anchor\" id=\"step-25\"></a> Make Forest Plots for Effect Size Comparisons\n",
    "* [Back Up to Table of Contents](#step-toc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_remove_empty(df, column_name):\n",
    "    # Remove empty rows from the DataFrame\n",
    "    df = df.dropna(subset=['Group']).reset_index(drop=True)\n",
    "\n",
    "    # Create a new column with the \"Concatenated\" suffix\n",
    "    new_column_name = f\"{column_name} Concatenated\"\n",
    "\n",
    "    # Initialize an empty list to store the concatenated values\n",
    "    concatenated_values = []\n",
    "\n",
    "    # Iterate through the DataFrame and concatenate values in pairs\n",
    "    for i in range(0, len(df), 2):\n",
    "        if i + 1 < len(df):\n",
    "            value1 = df.iloc[i][column_name]\n",
    "            value2 = df.iloc[i + 1][column_name]\n",
    "            concatenated_value = f\"({value1} vs. {value2})\"\n",
    "            concatenated_values.extend([concatenated_value, concatenated_value])\n",
    "        else:\n",
    "            # If there's an odd number of rows, keep the last row unchanged\n",
    "            concatenated_values.append(df.iloc[i][column_name])\n",
    "\n",
    "    # Add the concatenated values as a new column\n",
    "    df[new_column_name] = concatenated_values\n",
    "\n",
    "    return df\n",
    "\n",
    "def processdfforinterlacing(df, Missenseboolean=True):\n",
    "    # Skip every third row (0-indexed)\n",
    "    df_skipped = df[df.index % 3 != 2]\n",
    "    df_skipped.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Remove empty rows and concatenate values in 'Total Allele Count' column\n",
    "    finaldf = concatenate_and_remove_empty(df_skipped, 'Total Allele Count')\n",
    "    \n",
    "    # Add a new column 'MissensePresent' with the value 'With Missense'\n",
    "    if Missenseboolean:\n",
    "        finaldf['MissensePresent'] = 'With Missense'\n",
    "        finaldf['Total Allele Count Concatenated'] = finaldf['Group'] + ' - ' + finaldf['Total Allele Count Concatenated']\n",
    "    else:\n",
    "        finaldf['MissensePresent'] = 'Without Missense'\n",
    "        \n",
    "    return finaldf\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataframe(input_df, clinicalsignificance):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame by filtering rows, concatenating columns, and splitting a column.\n",
    "\n",
    "    Parameters:\n",
    "    input_df (DataFrame): The input DataFrame containing columns 'Rank Biserial Coefficient',\n",
    "                          'Group', 'ClinVar Clinical Significance', and '95% Confidence Interval'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The preprocessed DataFrame with filtered rows and new columns.\n",
    "    \"\"\"\n",
    "    # Filter out rows with empty values in 'Rank Biserial Coefficient' column\n",
    "    #input_df['Rank Biserial Coefficient'] = pd.to_numeric(input_df['Rank Biserial Coefficient'], errors='coerce')\n",
    "    #filtered_df = input_df.dropna(subset=['Rank Biserial Coefficient'])\n",
    "    global filtered_df\n",
    "    input_df['p-value'] = pd.to_numeric(input_df['p-value'], errors='coerce')\n",
    "    filtered_df = input_df.dropna(subset=['p-value'])\n",
    "    \n",
    "    #print(filtered_df)\n",
    "    #input_df['Total Allele Count'] = pd.to_numeric(input_df['Total Allele Count'], errors='coerce')\n",
    "    #filtered_df = input_df.dropna(subset=['Total Allele Count'])\n",
    "    \n",
    "    #print(filtered_df)\n",
    "    # Concatenate 'Group' and 'ClinVar Clinical Significance' into 'Group_Clinvar'\n",
    "    filtered_df['Group_Clinvar'] = filtered_df['Group'] + ' ' + filtered_df['ClinVar Clinical Significance']\n",
    "    #filtered_df = filtered_df[filtered_df['ClinVar Clinical Significance'] == 'Variant of Uncertain Significance']\n",
    "    # Split '95% Confidence Interval' into '95lower' and '95higher' columns\n",
    "    global confidence_interval_split\n",
    "    #confidence_interval_split = filtered_df['95% Confidence Interval'].str.strip('[]').str.split(',', expand=True)\n",
    "    #filtered_df['95% Confidence Interval'] = filtered_df['95% Confidence Interval'].fillna('0')\n",
    "\n",
    "    #print(confidence_interval_split[0])\n",
    "    #print(confidence_interval_split[1])\n",
    "    #filtered_df['95lower'] = confidence_interval_split[0].astype(float)\n",
    "    #filtered_df['95higher'] = confidence_interval_split[1].astype(float)\n",
    "    finaldf = filtered_df[filtered_df['ClinVar Clinical Significance'] == clinicalsignificance]\n",
    "    \n",
    "    return finaldf.reset_index(drop=True)\n",
    "\n",
    "def interlacedandreversed_dataframes(df1, df2):\n",
    "    # Get the number of rows in each DataFrame\n",
    "    len_df1, len_df2 = len(df1), len(df2)\n",
    "    \n",
    "    # Determine the maximum length and create an empty DataFrame\n",
    "    max_len = max(len_df1, len_df2)\n",
    "    result_df = pd.DataFrame(columns=df1.columns)\n",
    "    \n",
    "    # Interlace the rows from both DataFrames\n",
    "    for i in range(max_len):\n",
    "        if i < len_df1:\n",
    "            result_df = result_df.append(df1.iloc[i], ignore_index=True)\n",
    "        if i < len_df2:\n",
    "            result_df = result_df.append(df2.iloc[i], ignore_index=True)\n",
    "    \n",
    "    finalresultdf = result_df[::-1].reset_index(drop=True)\n",
    "    \n",
    "    return finalresultdf\n",
    "\n",
    "\n",
    "def create_forest_plot(data, clinicalsignificance):\n",
    "    \"\"\"\n",
    "    Create a forest plot from a DataFrame containing effect sizes, lower bounds,\n",
    "    upper bounds, and labels.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): A DataFrame with columns 'effect_size', 'lower_bound',\n",
    "                      'upper_bound', and 'label'.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays the forest plot.\n",
    "    \"\"\"\n",
    "    # Set the font to Helvetica\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Helvetica']\n",
    "    \n",
    "    # Sample data\n",
    "    effect_sizes = data['Rank Biserial Coefficient']\n",
    "    lower_bounds = data['95% CI Lower Bound']\n",
    "    upper_bounds = data['95% CI Upper Bound']\n",
    "    labels = data['Total Allele Count Concatenated']\n",
    "    significancelabel = data['Statistically Significant Difference']    \n",
    "      \n",
    "    global pathmultiplier\n",
    "    global ha\n",
    "    global effectciposition\n",
    "    ha='left'\n",
    "    effectciposition=1.02\n",
    "    if clinicalsignificance == 'Pathogenic or Likely Pathogenic':\n",
    "        pathmultiplier=-1\n",
    "        ha='right'\n",
    "        effectciposition=0.12\n",
    "        sizelabelpos = len(labels) - .5\n",
    "    else:    \n",
    "        pathmultiplier=1\n",
    "        sizelabelpos = len(labels) + .5\n",
    "\n",
    "    # Create a vertical bar plot for effect sizes with error bars\n",
    "    plt.figure(figsize=(12, len(labels)*.25))\n",
    "   \n",
    "    # Custom y-axis labels with colors\n",
    "    #custom_labels = []\n",
    "\n",
    "    # Add alternating gray and white shading every two rows\n",
    "    for i in range(len(effect_sizes)):\n",
    "        #print(i)\n",
    "        if i % 4 < 2:\n",
    "            plt.axhspan(i - 0.5, i + 0.5, facecolor='lightgray', alpha=0.5)\n",
    "\n",
    "        # Set the color for 'a' rows to red\n",
    "        if i % 2 != 1:\n",
    "            color = '#0000FF'\n",
    "        else:\n",
    "            color = 'black'\n",
    "        \n",
    "        # Check if 'effect_size' is 'NS' (not significant)\n",
    "        if significancelabel[i] == 'No' and clinicalsignificance != 'Variant of Uncertain Significance':\n",
    "            # Label it as 'NS' and skip error bar plot              \n",
    "            plt.text(0.5*pathmultiplier, i, 'No Statistically Significant Difference', ha=ha, va='center_baseline', fontsize=10, color='black')\n",
    "            continue\n",
    "        else:\n",
    "            effectsizevalue = float(effect_sizes[i])\n",
    "            low95civalue = float(lower_bounds[i])\n",
    "            upper95civalue = float(upper_bounds[i])\n",
    "            \n",
    "        plt.errorbar(effectsizevalue, i, xerr=[[effectsizevalue - low95civalue], [upper95civalue - effectsizevalue]], fmt='o', markersize=5, capsize=5, color=color)\n",
    "\n",
    "    # Add labels to the right of the plot\n",
    "    for i, label in enumerate(labels):\n",
    "        if significancelabel[i] == 'No' and clinicalsignificance != 'Variant of Uncertain Significance':\n",
    "            continue\n",
    "        else:\n",
    "            plt.text(effectciposition, i, f'{float(effect_sizes[i]):.2f} ({float(lower_bounds[i]):.2f}, {float(upper_bounds[i]):.2f})', va='center', fontsize=10, color='black')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.yticks(range(len(labels)), labels, color='black')  # Set custom y-axis labels with colors\n",
    "    plt.xlabel('Effect Size')\n",
    "    \n",
    "    if clinicalsignificance == 'Pathogenic or Likely Pathogenic':\n",
    "        plt.xlim([-1, 0.1])  \n",
    "    else:    \n",
    "        plt.xlim([-0.1, 1])  # Set the x-axis limits from -0.1 to 1\n",
    "         \n",
    "    plt.axvline(0, color='gray', linestyle='--')  # Add a vertical line at zero\n",
    "    \n",
    "    # Add vertical lines and labels at 0.2, 0.4, and 0.6\n",
    "    plt.axvline(0.1*pathmultiplier, color='#FF4444', linestyle='--')\n",
    "    plt.text(0.1*pathmultiplier, sizelabelpos, 'small', va='bottom', fontsize=12, color='#FF4444', rotation=45)\n",
    "    \n",
    "    plt.axvline(0.2*pathmultiplier, color='#FF0000', linestyle='--')\n",
    "    plt.text(0.2*pathmultiplier, sizelabelpos, 'medium', va='bottom', fontsize=12, color='#FF0000', rotation=45)\n",
    "    \n",
    "    plt.axvline(0.3*pathmultiplier, color='#CC0000', linestyle='--')\n",
    "    plt.text(0.3*pathmultiplier, sizelabelpos, 'large', va='bottom', fontsize=12, color='#CC0000', rotation=45)\n",
    "    \n",
    "    plt.axvline(0.4*pathmultiplier, color='#660000', linestyle='--')\n",
    "    plt.text(0.4*pathmultiplier, sizelabelpos, 'very large', va='bottom', fontsize=12, color='#660000', rotation=45)\n",
    "\n",
    "    # Remove legend\n",
    "    plt.legend().set_visible(False)\n",
    "    \n",
    "    if clinicalsignificance == 'Pathogenic or Likely Pathogenic':\n",
    "        plt.text(-1.01, len(labels)+0.25, 'Specialty - (Total Alleles European vs non-European)', va='center', ha='right', fontsize=12, color='black')\n",
    "        plt.text(0.11, len(labels)+0.25, 'Effect Size (95% CI)', va='center', ha='left', fontsize=12, color='black')\n",
    "    else:\n",
    "        plt.text(-0.11, len(labels)+0.25, 'Specialty - (Total Alleles European vs non-European)', va='center', ha='right', fontsize=12, color='black')\n",
    "        plt.text(1.01, len(labels)+0.25, 'Effect Size (95% CI)', va='center', ha='left', fontsize=12, color='black')\n",
    "        \n",
    "    # Hide grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bfb87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputdfwithmissense = pd.read_csv('output/Main_coding_gnomadv2_dataframe.csv')\n",
    "outputdfwithoutmissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv2_dataframe.csv')\n",
    "clinicalsignificance = 'Variant of Uncertain Significance'\n",
    "\n",
    "dfwithmissense = processdfforinterlacing(outputdfwithmissense, Missenseboolean=True)\n",
    "dfwithoutmissense = processdfforinterlacing(outputdfwithoutmissense, Missenseboolean=False)\n",
    "forestinputdf = interlacedandreversed_dataframes(dfwithmissense, dfwithoutmissense)\n",
    "create_forest_plot(preprocess_dataframe(forestinputdf, clinicalsignificance), clinicalsignificance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29118c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdfwithmissense = pd.read_csv('output/Main_coding_gnomadv2_dataframe.csv')\n",
    "outputdfwithoutmissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv2_dataframe.csv')\n",
    "clinicalsignificance = 'Pathogenic or Likely Pathogenic'\n",
    "\n",
    "dfwithmissense = processdfforinterlacing(outputdfwithmissense, Missenseboolean=True)\n",
    "dfwithoutmissense = processdfforinterlacing(outputdfwithoutmissense, Missenseboolean=False)\n",
    "forestinputdf = interlacedandreversed_dataframes(dfwithmissense, dfwithoutmissense)\n",
    "create_forest_plot(preprocess_dataframe(forestinputdf, clinicalsignificance), clinicalsignificance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb46c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdfwithmissense = pd.read_csv('output/Main_coding_gnomadv2_dataframe.csv')\n",
    "outputdfwithoutmissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv2_dataframe.csv')\n",
    "clinicalsignificance = 'Benign or Likely Benign'\n",
    "\n",
    "dfwithmissense = processdfforinterlacing(outputdfwithmissense, Missenseboolean=True)\n",
    "dfwithoutmissense = processdfforinterlacing(outputdfwithoutmissense, Missenseboolean=False)\n",
    "forestinputdf = interlacedandreversed_dataframes(dfwithmissense, dfwithoutmissense)\n",
    "create_forest_plot(preprocess_dataframe(forestinputdf, clinicalsignificance), clinicalsignificance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81717532",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdfwithmissense = pd.read_csv('output/Main_coding_gnomadv3_dataframe.csv')\n",
    "outputdfwithoutmissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv3_dataframe.csv')\n",
    "clinicalsignificance = 'Variant of Uncertain Significance'\n",
    "\n",
    "dfwithmissense = processdfforinterlacing(outputdfwithmissense, Missenseboolean=True)\n",
    "dfwithoutmissense = processdfforinterlacing(outputdfwithoutmissense, Missenseboolean=False)\n",
    "forestinputdf = interlacedandreversed_dataframes(dfwithmissense, dfwithoutmissense)\n",
    "create_forest_plot(preprocess_dataframe(forestinputdf, clinicalsignificance), clinicalsignificance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da30ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdfwithmissense = pd.read_csv('output/Main_coding_gnomadv3_dataframe.csv')\n",
    "outputdfwithoutmissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv3_dataframe.csv')\n",
    "clinicalsignificance = 'Pathogenic or Likely Pathogenic'\n",
    "\n",
    "dfwithmissense = processdfforinterlacing(outputdfwithmissense, Missenseboolean=True)\n",
    "dfwithoutmissense = processdfforinterlacing(outputdfwithoutmissense, Missenseboolean=False)\n",
    "forestinputdf = interlacedandreversed_dataframes(dfwithmissense, dfwithoutmissense)\n",
    "create_forest_plot(preprocess_dataframe(forestinputdf, clinicalsignificance), clinicalsignificance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ea8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdfwithmissense = pd.read_csv('output/Main_coding_gnomadv3_dataframe.csv')\n",
    "outputdfwithoutmissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv3_dataframe.csv')\n",
    "clinicalsignificance = 'Benign or Likely Benign'\n",
    "\n",
    "dfwithmissense = processdfforinterlacing(outputdfwithmissense, Missenseboolean=True)\n",
    "dfwithoutmissense = processdfforinterlacing(outputdfwithoutmissense, Missenseboolean=False)\n",
    "forestinputdf = interlacedandreversed_dataframes(dfwithmissense, dfwithoutmissense)\n",
    "create_forest_plot(preprocess_dataframe(forestinputdf, clinicalsignificance), clinicalsignificance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18101007",
   "metadata": {},
   "source": [
    "# Step 26: <a class=\"anchor\" id=\"step-26\"></a> Bar Graphs of Allele Prevalence\n",
    "* [Back Up to Table of Contents](#step-toc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC\n",
    "finaltableforGenCCbargraphsv25sigs, dfforplotfinaltableforGenCCbargraphsv25sigs = dfforplots(inputdictionarylist = inputdictGenCCv2, \n",
    "                                   version = 'v2', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GenCC\n",
    "finaltableforGenCCbargraphsv35sigs, dfforplotfinaltableforGenCCbargraphsv35sigs = dfforplots(inputdictionarylist = inputdictGenCCv3, \n",
    "                                   version = 'v3', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5116cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACMG78\n",
    "finaltableforACMG78bargraphsv25sigs, dfforplotfinaltableforACMG78bargraphsv25sigs = dfforplots(inputdictionarylist = inputdictACMG78v2, \n",
    "                                   version = 'v2', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACMG78\n",
    "finaltableforACMG78bargraphsv35sigs, dfforplotfinaltableforACMG78bargraphsv35sigs = dfforplots(inputdictionarylist = inputdictACMG78v3, \n",
    "                                   version = 'v3', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fe6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_v2 = {\n",
    "    'African/African American': 12487,\n",
    "    'Latino/Admixed American': 17720,\n",
    "    'East Asian': 9977,\n",
    "    'European (non-Finnish)': 64603,\n",
    "    'South Asian': 15308,\n",
    "    'Other': 3614}\n",
    "\n",
    "ancestry_v3nonv2 = {\n",
    "    'African/African American': 14377,\n",
    "    'Latino/Admixed American': 6878,\n",
    "    'East Asian': 1414,\n",
    "    'European (non-Finnish)': 25988,\n",
    "    'South Asian': 1946,\n",
    "    'Other': 932}\n",
    "\n",
    "def plot_allele_stat(inputdf, ancestry_num):\n",
    "    # Create a dictionary of ancestry group abbreviations\n",
    "    ancestry_abbr = {\n",
    "        'European (non-Finnish)': 'EUR',\n",
    "        'African/African American': 'AFR',\n",
    "        'Latino/Admixed American': 'AMR',\n",
    "        'East Asian': 'EAS',\n",
    "        'Other': 'OTH',\n",
    "        'South Asian': 'SAS'\n",
    "    }\n",
    "\n",
    "    inputdfcopy = inputdf.copy()\n",
    "    #print(len(df))\n",
    "    \n",
    "    df = inputdfcopy.groupby(['ClinVar Clinical Significance']).sum().reset_index()\n",
    "    \n",
    "    for ancestry, num in ancestry_num.items():\n",
    "        df[f'Allele Count Per Individual {ancestry}'] = df[f'Allele Count {ancestry}'] / num\n",
    "        \n",
    "    # Create a dictionary of color values for each clinical significance group\n",
    "    color_dict = {\n",
    "        \"Variant of Uncertain Significance\": \"lightgray\",\n",
    "        \"Pathogenic or Likely Pathogenic\": \"red\",\n",
    "        \"Benign or Likely Benign\": \"blue\",\n",
    "        \"Conflicting Interpretations\": \"darkgray\",\n",
    "        \"No Designation\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Set Helvetica font\n",
    "    font_path = \"/System/Library/Fonts/Helvetica.ttc\"\n",
    "    prop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "    # Create the subplots\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(60, 8))\n",
    "\n",
    "    # Flatten the axes for easier iteration\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Loop through each clinical significance group and create a bar chart for the allele frequency\n",
    "    dfs_by_clin_sig = {}\n",
    "    for i, (clin_sig, color) in enumerate(color_dict.items()):\n",
    "        # Filter the dataframe to only include the current clinical significance group\n",
    "        #df_filtered = df.loc[[clin_sig], :]\n",
    "        df_filtered = df[df['ClinVar Clinical Significance'] == clin_sig]\n",
    "        #print('filtering for clinical significance call: Number of rows:', df_filtered.shape[0], \"   Number of columns:\", df_filtered.shape[1])\n",
    "        \n",
    "        # Filter the dataframe to only include the allele frequency columns\n",
    "        df_filtered = df_filtered.filter(regex='Allele Count Per Individual')\n",
    "        #print('filtering for allele count per individual columns: Number of rows:', df_filtered.shape[0], \"   Number of columns:\", df_filtered.shape[1])\n",
    "        \n",
    "        # Get a list of the column names for the allele frequency columns\n",
    "        global allele_freq_cols\n",
    "        allele_freq_cols = df_filtered.columns\n",
    "        \n",
    "        # Extract the part of the column name that comes after the term and use it as the label\n",
    "        allele_freq_cols = [ancestry_abbr[c.split('Allele Count Per Individual')[-1].strip()] for c in allele_freq_cols]\n",
    "\n",
    "        # Plot a bar chart of the allele frequency for each ancestry\n",
    "        axs[i].bar(allele_freq_cols, df_filtered.values[0], color=color, edgecolor=\"black\")\n",
    "        axs[i].set_title(clin_sig, fontsize=24, fontproperties=prop)\n",
    "        axs[i].set_ylabel('Allele Count Per Individual', fontsize=22, fontproperties=prop)\n",
    "        axs[i].tick_params(axis=\"both\", which=\"major\", labelsize=22)\n",
    "        axs[i].set_xticklabels(allele_freq_cols, rotation=45, ha='right', fontsize=22,fontproperties=prop)\n",
    "        axs[i].spines[\"right\"].set_visible(False)\n",
    "        axs[i].spines[\"top\"].set_visible(False)\n",
    "\n",
    "        # Store the filtered dataframe in a dictionary\n",
    "        dfs_by_clin_sig[clin_sig] = df_filtered\n",
    "    \n",
    "    # Add space between subplots\n",
    "    fig.tight_layout(pad=2.0)\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig('Allele Count Per Individual'+\"ClinvarSignificance\"+\".png\", dpi=1000, bbox_inches=\"tight\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the dataframes by clinical significance designation\n",
    "    #return dfs_by_clin_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4893a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_allele_stat(finaltableforGenCCbargraphsv25sigs, ancestry_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allele_stat(finaltableforGenCCbargraphsv35sigs, ancestry_v3nonv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac62147",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allele_stat(finaltableforACMG78bargraphsv25sigs, ancestry_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5500f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allele_stat(finaltableforACMG78bargraphsv35sigs, ancestry_v3nonv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67898728",
   "metadata": {},
   "source": [
    "# Part 4: <a class=\"anchor\" id=\"part-4\"></a>Unique Variant Examination (Orthogonal Chi-Square Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e310e75",
   "metadata": {},
   "source": [
    "# Step 27: <a class=\"anchor\" id=\"step-27\"></a> Functions to Set Up Unique Variants\n",
    "* [Back Up to Table of Contents](#step-toc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdfg2 = pd.read_csv('output/Main_coding_gnomadv2_dataframe.csv')\n",
    "outputdfg2nomissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv2_dataframe.csv')\n",
    "\n",
    "outputdfg3 = pd.read_csv('output/Main_coding_gnomadv3_dataframe.csv')\n",
    "outputdfg3nomissense = pd.read_csv('output/Main_codingwithoutmissense_gnomadv3_dataframe.csv')\n",
    "\n",
    "outputdfaou = pd.read_csv('inputs/Main_coding_AoUAoU_dataframe.csv')\n",
    "outputdfaounomissense = pd.read_csv('inputs/Main_codingwithoutmissense_AoUAoU_dataframe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db041cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_contingency_custom(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if row['ClinVar Clinical Significance'] == 'No Designation':\n",
    "            df.at[index, 'chipvalue'] = ''\n",
    "            continue\n",
    "        \n",
    "        filtering_df = []        \n",
    "        filtering_df = df[(df['Group'] == row['Group']) & (df['ClinVar Clinical Significance'] == 'No Designation')]\n",
    "        filtering_df = filtering_df.reset_index(drop=True)\n",
    "\n",
    "        x = filtering_df.at[0, 'Unique Variants in Both Groups Number']\n",
    "        y = filtering_df.at[0, 'Unique nonEuro']\n",
    "        \n",
    "        contingency_table = [[x, y], [row['Unique Variants in Both Groups Number'], row['Unique nonEuro']]]\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Check if p-value is extremely small and adjust its representation\n",
    "        if p > 0.05/336:\n",
    "            p = 'ns'\n",
    "            \n",
    "        elif p < 1e-300:  # This threshold can be adjusted\n",
    "            p = f\"****\"\n",
    "            \n",
    "        else:\n",
    "            p = f\"p = {p:.2e}\"  # Format p-value in scientific notation for readability\n",
    "        \n",
    "        df.at[index, 'chipvalue'] = p\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def uniquegroupvariantanalysis(input_df, column_to_split='Unique Variants in Only One Group', popcolumn='Population Description(s)'):\n",
    "    # Filter the DataFrame to keep only rows with 'European' or 'non-European'\n",
    "    input_df = input_df[input_df[popcolumn].isin(['European', 'non-European'])]\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    \n",
    "    # Split the specified column and create 'Unique Variants in One Group Number Only'\n",
    "    input_df['Unique Variants in One Group Number Only'] = input_df[column_to_split].str.split('(').str[0]\n",
    "    input_df['Unique Variants in One Group Percent Only'] = input_df[column_to_split].str.split('(').str[1]\n",
    "    input_df['Unique Variants in Both Groups Number'] = input_df['Unique Variants in Both Groups'].str.split('(').str[0]\n",
    "    input_df['Unique Variants in Both Groups Percent'] = input_df['Unique Variants in Both Groups'].str.split('(').str[1]\n",
    "\n",
    "    # Split 'Unique Variants in One Group Number Only' into 'Unique Euro' and 'Unique nonEuro'\n",
    "    split_result = input_df['Unique Variants in One Group Number Only'].str.split('###').apply(pd.Series)\n",
    "\n",
    "    # Initialize empty lists to store 'Unique Euro' and 'Unique nonEuro' values\n",
    "    unique_euro_values = []\n",
    "    unique_non_euro_values = []\n",
    "\n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in input_df.iterrows():\n",
    "        if 'non-European' in row[popcolumn]:\n",
    "            unique_euro_values.append(input_df.loc[index - 1]['Unique Variants in One Group Number Only'])\n",
    "            unique_non_euro_values.append(row['Unique Variants in One Group Number Only'])\n",
    "        else:\n",
    "            unique_euro_values.append(row['Unique Variants in One Group Number Only'])\n",
    "            unique_non_euro_values.append(input_df.loc[index + 1]['Unique Variants in One Group Number Only'])\n",
    "        \n",
    "    # Assign the lists as new columns\n",
    "    input_df['Unique Euro'] = unique_euro_values\n",
    "    input_df['Unique nonEuro'] = unique_non_euro_values\n",
    "    \n",
    "    input_df['Unique Variants in One Group Number Only'] = input_df['Unique Variants in One Group Number Only'].str.replace(' ', '').str.replace(',', '')\n",
    "    input_df['Unique Euro'] = input_df['Unique Euro'].str.replace(' ', '').str.replace(',', '')\n",
    "    input_df['Unique nonEuro'] = input_df['Unique nonEuro'].str.replace(' ', '').str.replace(',', '')\n",
    "    input_df['Unique Variants in Both Groups Number'] = input_df['Unique Variants in Both Groups Number'].str.replace(' ', '').str.replace(',', '').str.replace(')', '').str.replace('%', '')\n",
    "    input_df['Unique Variants in Both Groups Percent'] = input_df['Unique Variants in Both Groups Percent'].str.replace(' ', '').str.replace(',', '').str.replace(')', '').str.replace('%', '')\n",
    "    input_df['Unique Variants in One Group Percent Only'] = input_df['Unique Variants in One Group Percent Only'].str.replace(' ', '').str.replace(',', '').str.replace(')', '').str.replace('%', '')\n",
    "    \n",
    "    input_df['Unique Euro'] = input_df['Unique Euro'].astype(int)\n",
    "    input_df['Unique nonEuro'] = input_df['Unique nonEuro'].astype(int)  \n",
    "    input_df['Unique Variants in Both Groups Number'] = input_df['Unique Variants in Both Groups Number'].astype(int)\n",
    "    input_df['Unique Variants in Both Groups Percent'] = input_df['Unique Variants in Both Groups Percent'].astype(float)\n",
    "    \n",
    "    input_df['chipvalue'] = ''\n",
    "    \n",
    "    input_df = input_df[input_df['Population Description(s)'] == 'European']\n",
    "    input_df = input_df.reset_index(drop=True)\n",
    "    chidf = chi2_contingency_custom(input_df)\n",
    "    \n",
    "    return chidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquegnomad2 = uniquegroupvariantanalysis(input_df=outputdfg2)\n",
    "uniquegnomad2nomissense = uniquegroupvariantanalysis(input_df=outputdfg2nomissense)\n",
    "\n",
    "uniquegnomad3 = uniquegroupvariantanalysis(input_df=outputdfg3)\n",
    "uniquegnomad3nomissense = uniquegroupvariantanalysis(input_df=outputdfg3nomissense)\n",
    "\n",
    "uniqueaou = uniquegroupvariantanalysis(input_df=outputdfaou)\n",
    "uniqueaounomissense = uniquegroupvariantanalysis(input_df=outputdfaounomissense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ad8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_venn_by_category_in_columns(df, col_left, col_right, col_overlap, category_col, chipvalue='chipvalue',order=None):\n",
    "    # Define color palette for the circles\n",
    "    color_palette = {'European': '#ff7f0e', 'non-European': '#1f77b4'}   \n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "    \n",
    "    # Group the DataFrame by the clinical significance category\n",
    "    global category_groups\n",
    "    category_groups = df.groupby(category_col)\n",
    "    \n",
    "    # If a specific order is provided, reorder the groups accordingly\n",
    "    if order:\n",
    "        category_groups = {category: category_groups.get_group(category) for category in order if category in category_groups.groups}\n",
    "    else:\n",
    "        # Convert groupby object to a dict for consistent handling\n",
    "        category_groups = {category: group for category, group in category_groups}  \n",
    "    \n",
    "    # Determine the maximum number of entries in any category to set the number of rows\n",
    "    max_entries = max(len(group) for group in category_groups.values())\n",
    "    \n",
    "    # Number of categories will determine the number of columns\n",
    "    n_categories = len(category_groups)\n",
    "    \n",
    "    # Create a figure with subplots: one row per entry, one column per category\n",
    "    fig, axes = plt.subplots(max_entries, n_categories, figsize=(n_categories*4, max_entries*4 + 1), squeeze=False)\n",
    "    \n",
    "    # Adjust subplot parameters for tighter layout and closer columns\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.5, bottom=0.1, wspace=0.0, hspace=0)\n",
    "    \n",
    "    # Store the x-position for each category title\n",
    "    x_positions = [0.18 + i*(0.8/n_categories) for i in range(n_categories)]\n",
    "    \n",
    "    for col_index, (category, group) in enumerate(category_groups.items()):\n",
    "        # Place category name at the top of each column\n",
    "        fig.text(x_positions[col_index], 0.5, category, ha='center', va='center')\n",
    "        \n",
    "        for row_index, (_, row) in enumerate(group.iterrows()):\n",
    "            # Extract the values for the left, right, and overlap sets\n",
    "            left = row[col_left]\n",
    "            right = row[col_right]\n",
    "            overlap = row[col_overlap]\n",
    "            chi = row[chipvalue]\n",
    "            \n",
    "            \n",
    "            # Plot the Venn diagram on the current axes\n",
    "            ax = axes[row_index][col_index]\n",
    "            venn_diagram = venn2(subsets=(left, right, overlap), set_labels=('',f'{chi}'), ax=ax)\n",
    "            #venn_diagram = venn2(subsets=(left, right, overlap), set_labels=('',''), ax=ax)\n",
    "            \n",
    "            # Apply the color palette and draw a black border around each circle\n",
    "            venn_diagram.get_patch_by_id('10').set_color(color_palette['European'])\n",
    "            venn_diagram.get_patch_by_id('10').set_edgecolor('black')\n",
    "            venn_diagram.get_patch_by_id('01').set_color(color_palette['non-European'])\n",
    "            venn_diagram.get_patch_by_id('01').set_edgecolor('black')\n",
    "            # For the overlap, only set the border color\n",
    "            if venn_diagram.get_patch_by_id('11'):  # Check if the overlap patch exists \n",
    "                venn_diagram.get_patch_by_id('11').set_color('#e7c9a9')\n",
    "                venn_diagram.get_patch_by_id('11').set_edgecolor('black')\n",
    "                \n",
    "                \n",
    "            # Manually adjust the position of the set labels\n",
    "            for label in venn_diagram.set_labels:\n",
    "                if label:  # Check if the label exists\n",
    "                    label.set_horizontalalignment('left')\n",
    "                    label.set_x(label.get_position()[1] - 0.02)  # Adjust y position\n",
    "            \n",
    "            \n",
    "            # Rotate text labels inside the Venn diagram\n",
    "            for text in venn_diagram.subset_labels:\n",
    "                if text:  # Check if the label exists\n",
    "                    text.set_text(format(int(text.get_text()), \",\"))\n",
    "                    text.set_rotation(45)\n",
    "        \n",
    "        # Clear the unused axes\n",
    "        for clear_index in range(len(group), max_entries):\n",
    "            axes[clear_index][col_index].axis('off')\n",
    "\n",
    "    # Show the plot for all categories\n",
    "    plt.show()\n",
    "\n",
    "desired_order = ['Variant of Uncertain Significance', 'Pathogenic or Likely Pathogenic', 'Benign or Likely Benign', 'Conflicting Interpretations', 'No Designation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6335819",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_venn_by_category_in_columns(uniquegnomad2, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_venn_by_category_in_columns(uniquegnomad3, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a592a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueaougencc = uniqueaou[uniqueaou['Group'] == 'All Curated Clinical Genes (GenCC)']\n",
    "plot_venn_by_category_in_columns(uniqueaougencc, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa22495",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_venn_by_category_in_columns(uniqueaou, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_venn_by_category_in_columns(uniquegnomad2nomissense, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_venn_by_category_in_columns(uniquegnomad3nomissense, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21548fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_venn_by_category_in_columns(uniqueaounomissense, 'Unique Euro', 'Unique nonEuro', 'Unique Variants in Both Groups Number', 'ClinVar Clinical Significance', order=desired_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ebe10",
   "metadata": {},
   "source": [
    "# Step 28: <a class=\"anchor\" id=\"step-28\"></a> Functions to Set Up Bar Graphs and Orthogonal Chi-Square Tests\n",
    "* [Back Up to Table of Contents](#step-toc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d93865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unique_variants(df, significance_value, databaselabel):\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "    \n",
    "    # Filter the DataFrame based on the Group column and Clinical Significance column\n",
    "    filtered_df = df[(df['Group'] != '') & (df['ClinVar Clinical Significance'] == significance_value)]\n",
    "\n",
    "    # Define the color palette\n",
    "    color_palette = {'non-European': '#1f77b4', 'European': '#ff7f0e'}\n",
    "\n",
    "    # Initialize lists to store data for plotting\n",
    "    unique_euro_values = []\n",
    "    unique_non_euro_values = []\n",
    "\n",
    "    # Iterate through the filtered DataFrame\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        unique_euro_values.append(row['Unique Euro'])\n",
    "        unique_non_euro_values.append(row['Unique nonEuro'])\n",
    "\n",
    "    # Create a figure and axis for plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Generate the x-axis labels (use 'Group' column values)\n",
    "    x_labels = filtered_df['Group']\n",
    "\n",
    "    # Plot the paired bar graphs\n",
    "    width = 0.35\n",
    "    ax.bar([x - width/2 for x in range(len(x_labels))], unique_euro_values, width, label='European-like genetic ancestry', color=[color_palette['European']]*len(unique_euro_values))\n",
    "    ax.bar([x + width/2 for x in range(len(x_labels))], unique_non_euro_values, width, label='non-European-like genetic ancestry', color=[color_palette['non-European']]*len(unique_non_euro_values))\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel('Group')\n",
    "    ax.set_ylabel('Unique Variant Count')\n",
    "    ax.set_title(f'Unique Variants In Only One Genetic Ancestry for {databaselabel} - {significance_value}')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    ax.set_xticks(range(len(x_labels)))\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "\n",
    "    # Format the y-axis as 10K\n",
    "    ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{int(x/1000)}K'))\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    ax.grid(False)\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "\n",
    "    # Perform a chi-square test for independence\n",
    "    contingencytable = [unique_non_euro_values, unique_euro_values]\n",
    "    chi2, p, _, _ = chi2_contingency(contingencytable)\n",
    "    pvalue = round_to_significant_figures(p, 4)  \n",
    "    \n",
    "    rawpower = calculate_power(unique_non_euro_values, unique_euro_values)\n",
    "    power = round_to_significant_figures(rawpower, 3) \n",
    "    \n",
    "    # Annotate the chi-square test result on the graph\n",
    "    ax.annotate(f'p-value: {pvalue}', xy=(0.02, 0.75), xycoords='axes fraction', fontsize=12)\n",
    "    ax.annotate(f'Estimated Statistical Power: {power}', xy=(0.02, 0.68), xycoords='axes fraction', fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_effect_size(contingency_table):\n",
    "    \"\"\"\n",
    "    Calculate the effect size for the chi-square test.\n",
    "    \"\"\"\n",
    "    chi2, _, _, expected = chi2_contingency(contingency_table)\n",
    "    n = np.sum(contingency_table)\n",
    "    return np.sqrt(chi2 / n)\n",
    "\n",
    "def calculate_power(unique_euro_values, unique_non_euro_values, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the statistical power of a chi-square test.\n",
    "\n",
    "    Parameters:\n",
    "    - unique_euro_values: list, unique variant counts for European ancestry.\n",
    "    - unique_non_euro_values: list, unique variant counts for non-European ancestry.\n",
    "    - alpha: float, the significance level.\n",
    "\n",
    "    Returns:\n",
    "    - power: float, the statistical power of the test.\n",
    "    \"\"\"\n",
    "    # Construct the contingency table\n",
    "    contingency_table = np.array([unique_euro_values, unique_non_euro_values])\n",
    "\n",
    "    effect_size = calculate_effect_size(contingency_table)\n",
    "    n = np.sum(contingency_table)\n",
    "    power_analysis = GofChisquarePower()\n",
    "    power = power_analysis.solve_power(effect_size=effect_size, nobs=n, alpha=alpha, n_bins=contingency_table.shape[0] * contingency_table.shape[1])\n",
    "\n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83971276",
   "metadata": {},
   "source": [
    "# Step 29: <a class=\"anchor\" id=\"step-29\"></a> Plotting Bar Graphs and Orthogonal Chi-Square Tests\n",
    "* [Back Up to Table of Contents](#step-toc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c70ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VUS\n",
    "plot_unique_variants(uniquegnomad2, 'Variant of Uncertain Significance', 'gnomAD v2.1.1')\n",
    "plot_unique_variants(uniquegnomad3, 'Variant of Uncertain Significance', 'gnomAD v3.1.2 (non v2)')\n",
    "plot_unique_variants(uniqueaou, 'Variant of Uncertain Significance', 'All of Us v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b69241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path\n",
    "plot_unique_variants(uniquegnomad2, 'Pathogenic or Likely Pathogenic', 'gnomAD v2.1.1')\n",
    "plot_unique_variants(uniquegnomad3, 'Pathogenic or Likely Pathogenic', 'gnomAD v3.1.2 (non v2)')\n",
    "plot_unique_variants(uniqueaou, 'Pathogenic or Likely Pathogenic', 'All of Us v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benign\n",
    "plot_unique_variants(uniquegnomad2, 'Benign or Likely Benign', 'gnomAD v2.1.1')\n",
    "plot_unique_variants(uniquegnomad3, 'Benign or Likely Benign', 'gnomAD v3.1.2 (non v2)')\n",
    "plot_unique_variants(uniqueaou, 'Benign or Likely Benign', 'All of Us v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CI\n",
    "plot_unique_variants(uniquegnomad2, 'Conflicting Interpretations', 'gnomAD v2.1.1')\n",
    "plot_unique_variants(uniquegnomad3, 'Conflicting Interpretations', 'gnomAD v3.1.2 (non v2)')\n",
    "plot_unique_variants(uniqueaou, 'Conflicting Interpretations', 'All of Us v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ND\n",
    "plot_unique_variants(uniquegnomad2, 'No Designation', 'gnomAD v2.1.1')\n",
    "plot_unique_variants(uniquegnomad3, 'No Designation', 'gnomAD v3.1.2 (non v2)')\n",
    "plot_unique_variants(uniqueaou, 'No Designation', 'All of Us v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f404aca",
   "metadata": {},
   "source": [
    "# Step 30: <a class=\"anchor\" id=\"step-30\"></a>Bar Graphs of Top Genes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36330bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topgenebargraph(df, x_column, y_column, z_column, x_labels_column, outputdf, top_n=10, output_filename=None, filter_column_m=None, filter_value_m=None, filter_column_n=None, filter_value_n=None, y_label=None, z_label=None):\n",
    "    \"\"\"\n",
    "    Create a publication-quality bar graph from a DataFrame with optional filtering.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - x_column: Name of the column for the X-axis values.\n",
    "    - y_column: Name of the column for the first set of bars.\n",
    "    - z_column: Name of the column for the second set of bars.\n",
    "    - x_labels_column: Name of the column for the X-axis labels.\n",
    "    - top_n: Number of top rows to plot (default is 10).\n",
    "    - output_filename: File path to save the plot as an image (optional).\n",
    "    - filter_column_m: Name of the column to filter on for 'm'.\n",
    "    - filter_value_m: Value to filter on for 'm'.\n",
    "    - filter_column_n: Name of the column to filter on for 'n'.\n",
    "    - filter_value_n: Value to filter on for 'n'.\n",
    "    - y_label: Label for the 'y' column (optional).\n",
    "    - z_label: Label for the 'z' column (optional).\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot or saves it to the specified file).\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on filter_column_m and filter_value_m if provided\n",
    "    if filter_column_m and filter_value_m is not None:\n",
    "        df = df[df[filter_column_m] == filter_value_m]\n",
    "\n",
    "    # Filter the DataFrame based on filter_column_n and filter_value_n if provided\n",
    "    if filter_column_n and filter_value_n is not None:\n",
    "        df = df[df[filter_column_n] == filter_value_n]\n",
    "\n",
    "    # Sort the DataFrame by 'x_column' in descending order and select the top 'top_n' rows\n",
    "    df_sortedeverything = df.sort_values(by=x_column, ascending=True)\n",
    "    \n",
    "    df_sorted = df_sortedeverything.head(top_n)\n",
    "    \n",
    "    # Add a 'Rank' column to the sorted DataFrame\n",
    "    df_sortedeverything['Rank'] = range(1, len(df_sortedeverything) + 1)\n",
    "    \n",
    "    # Create the bar graph\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Define figure size\n",
    "    bar_width = 0.3  # Width of the bars\n",
    "\n",
    "    # X-axis values (top 'top_n' rows)\n",
    "    x_values = df_sorted[x_column]\n",
    "    x_labels = df_sorted[x_labels_column][:top_n]\n",
    "\n",
    "    # Y and Z values\n",
    "    y_values = df_sorted[y_column]\n",
    "    z_values = df_sorted[z_column]\n",
    "\n",
    "    # Position for each bar group\n",
    "    x_pos = range(len(x_values))\n",
    "\n",
    "    # Create bars for 'y' and 'z' with custom labels\n",
    "    y_legend_label = y_label if y_label else y_column\n",
    "    z_legend_label = z_label if z_label else z_column\n",
    "\n",
    "    plt.bar(x_pos, y_values, width=bar_width, label=y_legend_label, alpha=1, edgecolor='black', color='#ff7f0e')\n",
    "    plt.bar([x + bar_width for x in x_pos], z_values, width=bar_width, label=z_legend_label, alpha=1, edgecolor='black', color='#1f77b4')\n",
    "\n",
    "    # Set X-axis labels and ticks\n",
    "    plt.xlabel(x_labels_column)\n",
    "    plt.xticks(x_pos, x_labels, rotation=45, ha='right')\n",
    "\n",
    "    # Set Y-axis label\n",
    "    plt.ylabel(x_column)\n",
    "\n",
    "    # Add a legend with custom labels\n",
    "    #plt.legend()\n",
    "\n",
    "    # Set a title with custom labels\n",
    "    title_label = output_filename\n",
    "    plt.title(title_label)\n",
    "\n",
    "    # Disable grid lines\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # Save the plot as a high-resolution image\n",
    "    plt.savefig(output_filename+'.pdf', dpi=1000, bbox_inches='tight')\n",
    "    plt.savefig(output_filename+'.png', dpi=1000, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    if outputdf == True:\n",
    "        df_sortedeverything.to_csv(output_filename+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltableforbargraphsv2, dfforplot = dfforplots(inputdictionarylist = inputdictionaryv2, \n",
    "                                   version = 'v2', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42075b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltableforbargraphsv3, dfforplot = dfforplots(inputdictionarylist = inputdictionaryv3, \n",
    "                                   version = 'v3', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3025c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltableforbargraphsmavev2, dfforplot = dfforplots(inputdictionarylist = inputdictionarymavev2, \n",
    "                                   version = 'v2', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltableforbargraphsmavev3, dfforplot = dfforplots(inputdictionarylist = inputdictionarymavev3, \n",
    "                                   version = 'v3', \n",
    "                                   varianttype = coding, \n",
    "                                   clinicalsignificance = \"Variant of Uncertain Significance\", \n",
    "                                   popdescrips1 = popdescrips1, \n",
    "                                   popdescrips2 = popdescrips2, \n",
    "                                   normalizationterm = 'coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makebargraphsoftopgenes(finaltable, numgenes, grouping, significancelist, outputdir, outputdf, version):\n",
    "    for significance in significancelist:\n",
    "        finaltablewithgenenames = add_gene_column(finaltable, 'inputs/biomartensg.txt')\n",
    "        finaltablewithgenenames['Absolute Difference in Variant Counts'] = finaltablewithgenenames['Variant in popdescrips1'] - finaltablewithgenenames['Variant in popdescrips2']\n",
    "\n",
    "        allelerepbargraph = topgenebargraph(finaltablewithgenenames, 'Allele Prevalence Difference', \n",
    "                                             'Allele Prevalence of ClinSigGroup popdescrips1', \n",
    "                                             'Allele Prevalence of ClinSigGroup popdescrips2', \n",
    "                                             'Gene', top_n=numgenes, \n",
    "                                             output_filename=os.path.join(outputdir,'Top'+str(numgenes)+'genes'+grouping+significance+'AllelePrevalenceDifference'+version), \n",
    "                                             filter_column_m='Group', \n",
    "                                             filter_value_m=grouping, \n",
    "                                             filter_column_n='ClinVar Clinical Significance', \n",
    "                                             filter_value_n=significance,\n",
    "                                             y_label='European genetic ancestry', \n",
    "                                             z_label='non-European genetic ancestry',\n",
    "                                             outputdf=outputdf)\n",
    "\n",
    "\n",
    "        diffvariantbargraph = topgenebargraph(finaltablewithgenenames, \n",
    "                        'Absolute Difference in Variant Counts', \n",
    "                        'Variant in popdescrips1', \n",
    "                        'Variant in popdescrips2', \n",
    "                        'Gene', top_n=numgenes, output_filename=os.path.join(outputdir,'Top'+str(numgenes)+'genes'+grouping+significance+'AbsoluteDifferenceinAlleleCounts'+version), \n",
    "                        filter_column_m='Group', \n",
    "                        filter_value_m=grouping, \n",
    "                        filter_column_n='ClinVar Clinical Significance', \n",
    "                        filter_value_n= significance,\n",
    "                        y_label='European genetic ancestry', \n",
    "                        z_label='non-European genetic ancestry',\n",
    "                        outputdf=outputdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "significancelist = [\"Variant of Uncertain Significance\", \"Pathogenic or Likely Pathogenic\", \"Conflicting Interpretations\", \"No Designation\", \"Benign or Likely Benign\"]\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsv2,\n",
    "                        version = 'v2',\n",
    "                        numgenes = 20, \n",
    "                        grouping = 'All Curated Clinical Genes (GenCC)', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = True)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsv3,\n",
    "                        version = 'v3', \n",
    "                        numgenes = 20, \n",
    "                        grouping = 'All Curated Clinical Genes (GenCC)', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = True)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsv2,\n",
    "                        version = 'v2', \n",
    "                        numgenes = 20, \n",
    "                        grouping = 'Reportable Secondary Findings (ACMG)', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = False)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsv3,\n",
    "                        version = 'v3', \n",
    "                        numgenes = 20, \n",
    "                        grouping = 'Reportable Secondary Findings (ACMG)', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = False)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsmavev2,\n",
    "                        version = 'v2',\n",
    "                        numgenes = 20, \n",
    "                        grouping = 'SGE', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = False)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsmavev3,\n",
    "                        version = 'v3',\n",
    "                        numgenes = 20, \n",
    "                        grouping = 'SGE', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = False)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsmavev2,\n",
    "                        version = 'v2',\n",
    "                        numgenes = 20, \n",
    "                        grouping = 'VAMPseq', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = False)\n",
    "\n",
    "makebargraphsoftopgenes(finaltable = finaltableforbargraphsmavev3,\n",
    "                        version = 'v3',\n",
    "                        numgenes = 20, \n",
    "                        grouping = 'VAMPseq', \n",
    "                        significancelist = significancelist,\n",
    "                        outputdir = 'output',\n",
    "                        outputdf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c574029",
   "metadata": {},
   "source": [
    "# Part 5: <a class=\"anchor\" id=\"part-5\"></a>Variant Reclassification Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a5bdb",
   "metadata": {},
   "source": [
    "# Step 31: <a class=\"anchor\" id=\"step-31\"></a>Import input data specifically for Variant Reclassification\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to prepare for intervar annotation\n",
    "def rename_columns(df):\n",
    "    new_column_names = {\n",
    "        \"Chromosome\": \"Chrom\",\n",
    "        \"Position\": \"Pos\"\n",
    "    }\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a925bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gnomad data for variant reclassification\n",
    "BRCA1_gnomadv2 = pd.read_csv('inputs/BRCA1_gnomadv2.csv')\n",
    "TP53_gnomadv2 = pd.read_csv('inputs/TP53_gnomadv2.csv')\n",
    "PTEN_gnomadv2 = pd.read_csv('inputs/PTEN_gnomadv2.csv')\n",
    "\n",
    "\n",
    "BRCA1_gnomadv3 = pd.read_csv('inputs/BRCA1_gnomadv3.csv')\n",
    "PTEN_gnomadv3 = pd.read_csv('inputs/PTEN_gnomadv3.csv')\n",
    "TP53_gnomadv3 = pd.read_csv('inputs/TP53_gnomadv3.csv')\n",
    "\n",
    "#import fayer data \n",
    "BRCA1_fayer = pd.read_excel('inputs/FayerBRCA1Variants.xlsx')\n",
    "TP53_fayer = pd.read_excel('inputs/FayerTP53Variants.xlsx')\n",
    "PTEN_fayer = pd.read_excel('inputs/FayerPTENVariants.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb3c64",
   "metadata": {},
   "source": [
    "# Step 32: <a class=\"anchor\" id=\"step-32\"></a>Filling in Evidence Codes based on 2015 Criteria\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to call the api for intervar and query each variant\n",
    "#will receive back all the evidence codes based on the 2015 Richards et al rules\n",
    "#these evidence codes still need to be further filtered to match the gene specific requirements of each VCEP recommendation\n",
    "\n",
    "\n",
    "def fetch_and_append_data(df, version):\n",
    "    api_columns = [\n",
    "        \"Intervar\", \"Build\", \"Chromosome\", \"Position\", \"Ref_allele\", \"Alt_allele\",\n",
    "        \"Gene\", \"PVS1\", \"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PM1\", \"PM2\", \"PM3\", \"PM4\",\n",
    "        \"PM5\", \"PM6\", \"PP1\", \"PP2\", \"PP3\", \"PP4\", \"PP5\", \"BA1\", \"BP1\", \"BP2\",\n",
    "        \"BP3\", \"BP4\", \"BP5\", \"BP6\", \"BP7\", \"BS1\", \"BS2\", \"BS3\", \"BS4\"\n",
    "    ]\n",
    "    \n",
    "    for index, row in df.iloc[1:].iterrows():  # Start from the second row\n",
    "        chromosome = row[\"Chrom\"]\n",
    "        position = row[\"Pos\"]\n",
    "        reference = row[\"Reference\"]\n",
    "        alternate = row[\"Alternate\"]\n",
    "        \n",
    "        url=''\n",
    "        if version == \"v2\":\n",
    "            url = f\"http://wintervar.wglab.org/api_new.php?queryType=position&chr={chromosome}&pos={position}&ref={reference}&alt={alternate}&build=hg19\"\n",
    "        elif version == 'v3':\n",
    "            url = f\"http://wintervar.wglab.org/api_new.php?queryType=position&chr={chromosome}&pos={position}&ref={reference}&alt={alternate}&build=hg38\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            print(url)\n",
    "            response.raise_for_status()  # Raise an error for non-2xx responses\n",
    "            data_json = response.json()\n",
    "            new_values = [data_json.get(column, None) for column in api_columns]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            #print(f\"Error processing row {index}: {e}\")\n",
    "            new_values = [None] * len(api_columns)\n",
    "        \n",
    "        df.loc[index, api_columns] = new_values\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            print(f\"Processed {index} rows out of {len(df) - 1}\")  # Subtract 1 for skipping the header row\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c878d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add evidence codes for each row for v2 gnomad data\n",
    "# BRCA1_gnomadv2_withevidencecodes_pre_functionaldata = fetch_and_append_data(BRCA1_gnomadv2, 'v2')\n",
    "# PTEN_gnomadv2_withevidencecodes_pre_functionaldata = fetch_and_append_data(PTEN_gnomadv2, 'v2')\n",
    "# TP53_gnomadv2_withevidencecodes_pre_functionaldata = fetch_and_append_data(TP53_gnomadv2, 'v2')\n",
    "    \n",
    "# #add evidence codes for each row for v3 gnomad data\n",
    "# BRCA1_gnomadv3_withevidencecodes_pre_functionaldata = fetch_and_append_data(BRCA1_gnomadv3, 'v3')\n",
    "# PTEN_gnomadv3_withevidencecodes_pre_functionaldata = fetch_and_append_data(PTEN_gnomadv3, 'v3')\n",
    "# TP53_gnomadv3_withevidencecodes_pre_functionaldata = fetch_and_append_data(TP53_gnomadv3, 'v3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #download after api calls\n",
    "# BRCA1_gnomadv2_withevidencecodes_pre_functionaldata.to_excel('output/BRCA1_gnomadv2_withevidencecodes_pre_functionaldata.xlsx') \n",
    "# PTEN_gnomadv2_withevidencecodes_pre_functionaldata.to_excel('output/PTEN_gnomadv2_withevidencecodes_pre_functionaldata.xlsx') \n",
    "# TP53_gnomadv2_withevidencecodes_pre_functionaldata.to_excel('output/TP53_gnomadv2_withevidencecodes_pre_functionaldata.xlsx') \n",
    "\n",
    "# BRCA1_gnomadv3_withevidencecodes_pre_functionaldata.to_excel('output/BRCA1_gnomadv3_withevidencecodes_pre_functionaldata.xlsx') \n",
    "# PTEN_gnomadv3_withevidencecodes_pre_functionaldata.to_excel('output/PTEN_gnomadv3_withevidencecodes_pre_functionaldata.xlsx') \n",
    "# TP53_gnomadv3_withevidencecodes_pre_functionaldata.to_excel('output/TP53_gnomadv3_withevidencecodes_pre_functionaldata.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your  files are saved\n",
    "directory = \"inputs\"\n",
    "\n",
    "# Read each Excel file into a separate DataFrame\n",
    "BRCA1_gnomadv2_withevidencecodes_pre_functionaldata = pd.read_excel(os.path.join(directory, 'BRCA1_gnomadv2_withevidencecodes_pre_functionaldata.xlsx'))\n",
    "PTEN_gnomadv2_withevidencecodes_pre_functionaldata = pd.read_excel(os.path.join(directory, 'PTEN_gnomadv2_withevidencecodes_pre_functionaldata.xlsx'))\n",
    "TP53_gnomadv2_withevidencecodes_pre_functionaldata = pd.read_excel(os.path.join(directory, 'TP53_gnomadv2_withevidencecodes_pre_functionaldata.xlsx'))\n",
    "\n",
    "BRCA1_gnomadv3_withevidencecodes_pre_functionaldata = pd.read_excel(os.path.join(directory, 'BRCA1_gnomadv3_withevidencecodes_pre_functionaldata.xlsx'))\n",
    "PTEN_gnomadv3_withevidencecodes_pre_functionaldata = pd.read_excel(os.path.join(directory, 'PTEN_gnomadv3_withevidencecodes_pre_functionaldata.xlsx'))\n",
    "TP53_gnomadv3_withevidencecodes_pre_functionaldata = pd.read_excel(os.path.join(directory, 'TP53_gnomadv3_withevidencecodes_pre_functionaldata.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d5dc8",
   "metadata": {},
   "source": [
    "# Step 33: <a class=\"anchor\" id=\"step-33\"></a>Import MAVE data\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in assay data\n",
    "BRCA1assaydata = pd.read_excel('inputs/assaydata/BRCA1_SGE_Findlay2018.xlsx')\n",
    "TP53assaydata = pd.read_excel('inputs/assaydata/TP53_SupTableS4_FayerAJHG.xlsx')\n",
    "MSH2assaydata = pd.read_excel('inputs/assaydata/MSH2.xlsx')\n",
    "PTENAbundanceassaydata = pd.read_csv('inputs/assaydata/PTEN_Abundance.txt',sep='\\t')\n",
    "PTENActivityassaydata = pd.read_excel('inputs/assaydata/PTEN_Activity.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1bd82d",
   "metadata": {},
   "source": [
    "# Step 34: <a class=\"anchor\" id=\"step-34\"></a> General functions for variant reclassification\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liftover_dataframe(df, chrom_col, pos_col):\n",
    "    converter = get_lifter('hg19', 'hg38')\n",
    "    \n",
    "    def liftover_single_row(row):\n",
    "        chrom = row[chrom_col]\n",
    "        pos = row[pos_col]\n",
    "        lifted_coords = converter.convert_coordinate(chrom, pos)\n",
    "        if lifted_coords:\n",
    "            new_chrom, new_pos, strand = lifted_coords[0]\n",
    "            return pd.Series({'hg38_pos': new_pos})\n",
    "        else:\n",
    "            return pd.Series({'hg38_pos': None})\n",
    "    \n",
    "    new_df = df.apply(liftover_single_row, axis=1)\n",
    "    \n",
    "    columns_to_convert = ['hg38_pos']\n",
    "    new_df = new_df[columns_to_convert].astype(int)\n",
    "    return pd.concat([df, new_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c052019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRCA1_gnomadv2_newsignificance = accumulate_evidence(newcodesforbrca1(add_evidence_codes_column(concatenate_headers(BRCA1_gnomadv2_updatedfunctionalevidence))))\n",
    "# BRCA1_gnomadv2_newsignificance['NewClinicalSignificance'] = process_dictionary_column_brca1(BRCA1_gnomadv2_newsignificance['EvidenceDictionary'])\n",
    "\n",
    "def concatenate_columns_as_string(dataframe, chrom_col, pos_col, ref_col, alt_col):\n",
    "    concatenated_column = (\n",
    "        dataframe[chrom_col].astype(str) + '_' +\n",
    "        dataframe[pos_col].astype(str) + '_' +\n",
    "        dataframe[ref_col].astype(str) + '_' +\n",
    "        dataframe[alt_col].astype(str))\n",
    "    return concatenated_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_headers(df):\n",
    "    specific_headers = [\n",
    "        \"PVS1\", \"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PM1\", \"PM2\", \"PM3\", \"PM4\",\n",
    "        \"PM5\", \"PM6\", \"PP1\", \"PP2\", \"PP3\", \"PP4\", \"PP5\", \"BA1\", \"BP1\",\n",
    "        \"BP2\", \"BP3\", \"BP4\", \"BP5\", \"BP6\", \"BP7\", \"BS1\", \"BS2\", \"BS3\", \"BS4\"]\n",
    "    \n",
    "    #df['Concatenated Headers'] = df.apply(lambda row: '|'.join(header for header in specific_headers if row[header] == 1), axis=1)\n",
    "    df['Concatenated Headers'] = df.apply(lambda row: '|'.join(header for header in specific_headers if header in row and row[header] == 1), axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_headers(headers):\n",
    "    headers = headers.split('|')\n",
    "    #''.join(filter(str.isalpha, headers))  # Remove numbers\n",
    "    return headers  # Split by pipe delimiter into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_evidence_codes_column(inputdf):\n",
    "    inputdf['PreGeneSpecificEvidenceCodes'] = inputdf['Concatenated Headers'].apply(process_headers)\n",
    "    inputdf['IntermediateGeneSpecificEvidenceCodes'] = inputdf['Concatenated Headers'].apply(process_headers)\n",
    "    inputdf['PostGeneSpecificEvidenceCodes'] = inputdf['Concatenated Headers'].apply(process_headers)\n",
    "    return inputdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e340af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(column_with_lists):\n",
    "    modified_lists = []\n",
    "    \n",
    "    for original_list in column_with_lists:\n",
    "        modified_list = [ ''.join(filter(str.isalpha, item)) for item in original_list ]\n",
    "        modified_lists.append(modified_list)\n",
    "    \n",
    "    return modified_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b44981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values_to_column(column_with_lists):\n",
    "    counts_list = []\n",
    "\n",
    "    for original_list in column_with_lists:\n",
    "        value_counts = Counter(original_list)\n",
    "        counts_list.append(dict(value_counts))\n",
    "\n",
    "    return counts_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34467e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_evidence(df, colname='IntermediateGeneSpecificEvidenceCodes'):    \n",
    "    df['EvidenceAccumulated'] = remove_numbers(df[colname])\n",
    "    df['EvidenceDictionary'] = count_unique_values_to_column(df['EvidenceAccumulated'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e060137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def determine_clinical_significance(df):\n",
    "#     def get_clinical_significance(headers):\n",
    "#         count = dict()\n",
    "        \n",
    "#         for header in headers:\n",
    "#             count[header] = count.get(header, 0) + 1\n",
    "        \n",
    "#         df['NumberOfPathEvidence'] = df['Concatenated Headers'].apply(lambda headers: get_clinical_significance(process_headers(headers)))\n",
    "#         df['NumberOfBenignEvidence'] = df['Concatenated Headers'].apply(lambda headers: get_clinical_significance(process_headers(headers)))\n",
    "\n",
    "\n",
    "granthamdict = {('A', 'A'): '0', ('A', 'C'): '195', ('A', 'D'): '126', ('A', 'E'): '107', ('A', 'F'): '113', ('A', 'G'): '60', ('A', 'H'): '86', ('A', 'I'): '94', ('A', 'K'): '106', ('A', 'L'): '96', ('A', 'M'): '84', ('A', 'N'): '111', ('A', 'P'): '27', ('A', 'Q'): '91', ('A', 'R'): '112', ('A', 'S'): '99', ('A', 'T'): '58', ('A', 'V'): '64', ('A', 'W'): '148', ('A', 'Y'): '112', ('C', 'A'): '195', ('C', 'C'): '0', ('C', 'D'): '154', ('C', 'E'): '170', ('C', 'F'): '205', ('C', 'G'): '159', ('C', 'H'): '174', ('C', 'I'): '198', ('C', 'K'): '202', ('C', 'L'): '198', ('C', 'M'): '196', ('C', 'N'): '139', ('C', 'P'): '169', ('C', 'Q'): '154', ('C', 'R'): '180', ('C', 'S'): '112', ('C', 'T'): '149', ('C', 'V'): '192', ('C', 'W'): '215', ('C', 'Y'): '194', ('D', 'A'): '126', ('D', 'C'): '154', ('D', 'D'): '0', ('D', 'E'): '45', ('D', 'F'): '177', ('D', 'G'): '94', ('D', 'H'): '81', ('D', 'I'): '168', ('D', 'K'): '101', ('D', 'L'): '172', ('D', 'M'): '160', ('D', 'N'): '23', ('D', 'P'): '108', ('D', 'Q'): '61', ('D', 'R'): '96', ('D', 'S'): '65', ('D', 'T'): '85', ('D', 'V'): '152', ('D', 'W'): '181', ('D', 'Y'): '160', ('E', 'A'): '107', ('E', 'C'): '170', ('E', 'D'): '45', ('E', 'E'): '0', ('E', 'F'): '140', ('E', 'G'): '98', ('E', 'H'): '40', ('E', 'I'): '134', ('E', 'K'): '56', ('E', 'L'): '138', ('E', 'M'): '126', ('E', 'N'): '42', ('E', 'P'): '93', ('E', 'Q'): '29', ('E', 'R'): '54', ('E', 'S'): '80', ('E', 'T'): '65', ('E', 'V'): '121', ('E', 'W'): '152', ('E', 'Y'): '122', ('F', 'A'): '113', ('F', 'C'): '205', ('F', 'D'): '177', ('F', 'E'): '140', ('F', 'F'): '0', ('F', 'G'): '153', ('F', 'H'): '100', ('F', 'I'): '21', ('F', 'K'): '102', ('F', 'L'): '22', ('F', 'M'): '28', ('F', 'N'): '158', ('F', 'P'): '114', ('F', 'Q'): '116', ('F', 'R'): '97', ('F', 'S'): '155', ('F', 'T'): '103', ('F', 'V'): '50', ('F', 'W'): '40', ('F', 'Y'): '22', ('G', 'A'): '60', ('G', 'C'): '159', ('G', 'D'): '94', ('G', 'E'): '98', ('G', 'F'): '153', ('G', 'G'): '0', ('G', 'H'): '98', ('G', 'I'): '135', ('G', 'K'): '127', ('G', 'L'): '138', ('G', 'M'): '127', ('G', 'N'): '80', ('G', 'P'): '42', ('G', 'Q'): '87', ('G', 'R'): '125', ('G', 'S'): '56', ('G', 'T'): '59', ('G', 'V'): '109', ('G', 'W'): '184', ('G', 'Y'): '147', ('H', 'A'): '86', ('H', 'C'): '174', ('H', 'D'): '81', ('H', 'E'): '40', ('H', 'F'): '100', ('H', 'G'): '98', ('H', 'H'): '0', ('H', 'I'): '94', ('H', 'K'): '32', ('H', 'L'): '99', ('H', 'M'): '87', ('H', 'N'): '68', ('H', 'P'): '77', ('H', 'Q'): '24', ('H', 'R'): '29', ('H', 'S'): '89', ('H', 'T'): '47', ('H', 'V'): '84', ('H', 'W'): '115', ('H', 'Y'): '83', ('I', 'A'): '94', ('I', 'C'): '198', ('I', 'D'): '168', ('I', 'E'): '134', ('I', 'F'): '21', ('I', 'G'): '135', ('I', 'H'): '94', ('I', 'I'): '0', ('I', 'K'): '102', ('I', 'L'): '5', ('I', 'M'): '10', ('I', 'N'): '149', ('I', 'P'): '95', ('I', 'Q'): '109', ('I', 'R'): '97', ('I', 'S'): '142', ('I', 'T'): '89', ('I', 'V'): '29', ('I', 'W'): '61', ('I', 'Y'): '33', ('K', 'A'): '106', ('K', 'C'): '202', ('K', 'D'): '101', ('K', 'E'): '56', ('K', 'F'): '102', ('K', 'G'): '127', ('K', 'H'): '32', ('K', 'I'): '102', ('K', 'K'): '0', ('K', 'L'): '107', ('K', 'M'): '95', ('K', 'N'): '94', ('K', 'P'): '103', ('K', 'Q'): '53', ('K', 'R'): '26', ('K', 'S'): '121', ('K', 'T'): '78', ('K', 'V'): '97', ('K', 'W'): '110', ('K', 'Y'): '85', ('L', 'A'): '96', ('L', 'C'): '198', ('L', 'D'): '172', ('L', 'E'): '138', ('L', 'F'): '22', ('L', 'G'): '138', ('L', 'H'): '99', ('L', 'I'): '5', ('L', 'K'): '107', ('L', 'L'): '0', ('L', 'M'): '15', ('L', 'N'): '153', ('L', 'P'): '98', ('L', 'Q'): '113', ('L', 'R'): '102', ('L', 'S'): '145', ('L', 'T'): '92', ('L', 'V'): '32', ('L', 'W'): '61', ('L', 'Y'): '36', ('M', 'A'): '84', ('M', 'C'): '196', ('M', 'D'): '160', ('M', 'E'): '126', ('M', 'F'): '28', ('M', 'G'): '127', ('M', 'H'): '87', ('M', 'I'): '10', ('M', 'K'): '95', ('M', 'L'): '15', ('M', 'M'): '0', ('M', 'N'): '142', ('M', 'P'): '87', ('M', 'Q'): '101', ('M', 'R'): '91', ('M', 'S'): '135', ('M', 'T'): '81', ('M', 'V'): '21', ('M', 'W'): '67', ('M', 'Y'): '36', ('N', 'A'): '111', ('N', 'C'): '139', ('N', 'D'): '23', ('N', 'E'): '42', ('N', 'F'): '158', ('N', 'G'): '80', ('N', 'H'): '68', ('N', 'I'): '149', ('N', 'K'): '94', ('N', 'L'): '153', ('N', 'M'): '142', ('N', 'N'): '0', ('N', 'P'): '91', ('N', 'Q'): '46', ('N', 'R'): '86', ('N', 'S'): '46', ('N', 'T'): '65', ('N', 'V'): '133', ('N', 'W'): '174', ('N', 'Y'): '143', ('P', 'A'): '27', ('P', 'C'): '169', ('P', 'D'): '108', ('P', 'E'): '93', ('P', 'F'): '114', ('P', 'G'): '42', ('P', 'H'): '77', ('P', 'I'): '95', ('P', 'K'): '103', ('P', 'L'): '98', ('P', 'M'): '87', ('P', 'N'): '91', ('P', 'P'): '0', ('P', 'Q'): '76', ('P', 'R'): '103', ('P', 'S'): '74', ('P', 'T'): '38', ('P', 'V'): '68', ('P', 'W'): '147', ('P', 'Y'): '110', ('Q', 'A'): '91', ('Q', 'C'): '154', ('Q', 'D'): '61', ('Q', 'E'): '29', ('Q', 'F'): '116', ('Q', 'G'): '87', ('Q', 'H'): '24', ('Q', 'I'): '109', ('Q', 'K'): '53', ('Q', 'L'): '113', ('Q', 'M'): '101', ('Q', 'N'): '46', ('Q', 'P'): '76', ('Q', 'Q'): '0', ('Q', 'R'): '43', ('Q', 'S'): '68', ('Q', 'T'): '42', ('Q', 'V'): '96', ('Q', 'W'): '130', ('Q', 'Y'): '99', ('R', 'A'): '112', ('R', 'C'): '180', ('R', 'D'): '96', ('R', 'E'): '54', ('R', 'F'): '97', ('R', 'G'): '125', ('R', 'H'): '29', ('R', 'I'): '97', ('R', 'K'): '26', ('R', 'L'): '102', ('R', 'M'): '91', ('R', 'N'): '86', ('R', 'P'): '103', ('R', 'Q'): '43', ('R', 'R'): '0', ('R', 'S'): '110', ('R', 'T'): '71', ('R', 'V'): '96', ('R', 'W'): '101', ('R', 'Y'): '77', ('S', 'A'): '99', ('S', 'C'): '112', ('S', 'D'): '65', ('S', 'E'): '80', ('S', 'F'): '155', ('S', 'G'): '56', ('S', 'H'): '89', ('S', 'I'): '142', ('S', 'K'): '121', ('S', 'L'): '145', ('S', 'M'): '135', ('S', 'N'): '46', ('S', 'P'): '74', ('S', 'Q'): '68', ('S', 'R'): '110', ('S', 'S'): '0', ('S', 'T'): '58', ('S', 'V'): '124', ('S', 'W'): '177', ('S', 'Y'): '144', ('T', 'A'): '58', ('T', 'C'): '149', ('T', 'D'): '85', ('T', 'E'): '65', ('T', 'F'): '103', ('T', 'G'): '59', ('T', 'H'): '47', ('T', 'I'): '89', ('T', 'K'): '78', ('T', 'L'): '92', ('T', 'M'): '81', ('T', 'N'): '65', ('T', 'P'): '38', ('T', 'Q'): '42', ('T', 'R'): '71', ('T', 'S'): '58', ('T', 'T'): '0', ('T', 'V'): '69', ('T', 'W'): '128', ('T', 'Y'): '92', ('V', 'A'): '64', ('V', 'C'): '192', ('V', 'D'): '152', ('V', 'E'): '121', ('V', 'F'): '50', ('V', 'G'): '109', ('V', 'H'): '84', ('V', 'I'): '29', ('V', 'K'): '97', ('V', 'L'): '32', ('V', 'M'): '21', ('V', 'N'): '133', ('V', 'P'): '68', ('V', 'Q'): '96', ('V', 'R'): '96', ('V', 'S'): '124', ('V', 'T'): '69', ('V', 'V'): '0', ('V', 'W'): '88', ('V', 'Y'): '55', ('W', 'A'): '148', ('W', 'C'): '215', ('W', 'D'): '181', ('W', 'E'): '152', ('W', 'F'): '40', ('W', 'G'): '184', ('W', 'H'): '115', ('W', 'I'): '61', ('W', 'K'): '110', ('W', 'L'): '61', ('W', 'M'): '67', ('W', 'N'): '174', ('W', 'P'): '147', ('W', 'Q'): '130', ('W', 'R'): '101', ('W', 'S'): '177', ('W', 'T'): '128', ('W', 'V'): '88', ('W', 'W'): '0', ('W', 'Y'): '37', ('Y', 'A'): '112', ('Y', 'C'): '194', ('Y', 'D'): '160', ('Y', 'E'): '122', ('Y', 'F'): '22', ('Y', 'G'): '147', ('Y', 'H'): '83', ('Y', 'I'): '33', ('Y', 'K'): '85', ('Y', 'L'): '36', ('Y', 'M'): '36', ('Y', 'N'): '143', ('Y', 'P'): '110', ('Y', 'Q'): '99', ('Y', 'R'): '77', ('Y', 'S'): '144', ('Y', 'T'): '92', ('Y', 'V'): '55', ('Y', 'W'): '37', ('Y', 'Y'): '0'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_out(inputdf, code):\n",
    "    for idx, row in inputdf.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if code in int_evidence_codes:\n",
    "            int_evidence_codes.remove(code)\n",
    "        if code in post_evidence_codes:\n",
    "            post_evidence_codes.remove(code)\n",
    "        \n",
    "        inputdf.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        inputdf.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "\n",
    "    return inputdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_matching_values(df1, df2, df3, column_df1, column_df2, column_df3):\n",
    "#     # Create an empty list to store the results\n",
    "#     allele_found = []\n",
    "\n",
    "#     # Iterate through the rows of df1\n",
    "#     for index, row in df1.iterrows():\n",
    "#         value_to_find = row[column_df1]\n",
    "\n",
    "#         # Check if the value is in either df2 or df3\n",
    "#         if value_to_find in df2[column_df2].values or value_to_find in df3[column_df3].values:\n",
    "#             allele_found.append(True)\n",
    "#         else:\n",
    "#             allele_found.append(False)\n",
    "\n",
    "#     # Add the results as a new column to df1\n",
    "#     df1['AlleleFoundIngnomad'] = allele_found\n",
    "\n",
    "#     return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_values(df1, df2, df3, column_df1, column_df2, column_df3):\n",
    "    # Create an empty list to store the results\n",
    "    allele_found = []\n",
    "    allele_frequencies = []  # To store Allele Frequency values\n",
    "\n",
    "    # Iterate through the rows of df1\n",
    "    for index, row in df1.iterrows():\n",
    "        value_to_find = row[column_df1]\n",
    "\n",
    "        # Check if the value is in df2\n",
    "        if value_to_find in df2[column_df2].values:\n",
    "            allele_found.append(True)\n",
    "            allele_frequencies.append(df2[df2[column_df2] == value_to_find]['Allele Frequency'].values[0])\n",
    "\n",
    "        # Check if the value is in df3\n",
    "        elif value_to_find in df3[column_df3].values:\n",
    "            allele_found.append(True)\n",
    "            allele_frequencies.append(df3[df3[column_df3] == value_to_find]['Allele Frequency'].values[0])\n",
    "\n",
    "        else:\n",
    "            allele_found.append(False)\n",
    "            allele_frequencies.append(None)  # If value not found, set Allele Frequency to None\n",
    "\n",
    "    # Add the results as new columns to df1\n",
    "    df1['AlleleFoundIngnomad'] = allele_found\n",
    "    df1['Allele Frequency'] = allele_frequencies\n",
    "\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e79e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aabeforeandafterwithblosumandgranthamdf(df, colname, suffix):\n",
    "    # Function to convert three-letter amino acid code to one-letter code\n",
    "    def three_to_one(three_letter_code):\n",
    "        aa_dict = {\n",
    "            'Ala': 'A', 'Arg': 'R', 'Asn': 'N', 'Asp': 'D',\n",
    "            'Cys': 'C', 'Glu': 'E', 'Gln': 'Q', 'Gly': 'G',\n",
    "            'His': 'H', 'Ile': 'I', 'Leu': 'L', 'Lys': 'K',\n",
    "            'Met': 'M', 'Phe': 'F', 'Pro': 'P', 'Ser': 'S',\n",
    "            'Thr': 'T', 'Trp': 'W', 'Tyr': 'Y', 'Val': 'V',\n",
    "            'Sub': '='\n",
    "        }\n",
    "        return aa_dict.get(three_letter_code, None)  # Return 'X' for unknown codes\n",
    "    \n",
    "    df['copy'] = df[colname].str.replace('=', 'Sub')\n",
    "    \n",
    "    print(df[colname])\n",
    "    # Extract values into a new DataFrame\n",
    "    extracted_df = df[colname].str.extract(r'p\\.([A-Z][a-z]{2})(\\d+)([A-Z][a-z]{2})')\n",
    "    \n",
    "    print(extracted_df)\n",
    "    \n",
    "    # Convert three-letter codes to one-letter codes\n",
    "    extracted_df[0] = extracted_df[0].apply(three_to_one)\n",
    "    extracted_df[2] = extracted_df[2].apply(three_to_one)\n",
    "    \n",
    "    # Assign columns to original DataFrame\n",
    "    df['Before_AA'+suffix] = extracted_df[0]\n",
    "    df['AA_Pos'+suffix] = extracted_df[1]\n",
    "    df['After_AA'+suffix] = extracted_df[2]\n",
    "    \n",
    "    df = df.drop('copy', axis=1)\n",
    "    \n",
    "    # Loop through the DataFrame to check and replace values as needed\n",
    "    for index, row in df.iterrows():\n",
    "        if row['After_AA'+suffix] == '=':\n",
    "            df.at[index, 'After_AA'+suffix] = row['Before_AA'+suffix]\n",
    "            print(row['AA_Pos'+suffix])\n",
    "            print(row['Before_AA'+suffix])\n",
    "    \n",
    "    # Get BLOSUM62 matrix score for amino acid substitutions\n",
    "    matrix = bl.BLOSUM(62)\n",
    "    \n",
    "    def get_blosum_score(row):\n",
    "        return matrix.get(row[\"Before_AA\"+suffix], {}).get(row[\"After_AA\"+suffix], None)\n",
    "\n",
    "    def get_grantham_score(row):\n",
    "        if (row[\"Before_AA\"+suffix] != None) and (row[\"After_AA\"+suffix] != None):\n",
    "            return granthamdict[(row[\"Before_AA\"+suffix], row[\"After_AA\"+suffix])]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    df['BLOSUM62_Score'+suffix] = df.apply(get_blosum_score, axis=1)\n",
    "    df['Grantham_Score'+suffix] = df.apply(get_grantham_score, axis =1)\n",
    "    \n",
    "    df.loc[df['AA_Pos'+suffix].isna(), 'BLOSUM62_Score'+suffix] = pd.NA\n",
    "    df.loc[df['AA_Pos'+suffix].isna(), 'Grantham_Score'+suffix] = pd.NA\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aabeforeandafterwithblosumandgranthamforbrca1fayerdf(df, colname, suffix):\n",
    "    # Function to convert three-letter amino acid code to one-letter code\n",
    "    def three_to_one(three_letter_code):\n",
    "        aa_dict = {\n",
    "            'Ala': 'A', 'Arg': 'R', 'Asn': 'N', 'Asp': 'D',\n",
    "            'Cys': 'C', 'Glu': 'E', 'Gln': 'Q', 'Gly': 'G',\n",
    "            'His': 'H', 'Ile': 'I', 'Leu': 'L', 'Lys': 'K',\n",
    "            'Met': 'M', 'Phe': 'F', 'Pro': 'P', 'Ser': 'S',\n",
    "            'Thr': 'T', 'Trp': 'W', 'Tyr': 'Y', 'Val': 'V'\n",
    "        }\n",
    "        return aa_dict.get(three_letter_code, None)  # Return 'X' for unknown codes\n",
    "\n",
    "    print(df[colname])\n",
    "    # Extract values into a new DataFrame\n",
    "    #extracted_df = df[colname].str.extract(r'p\\.([A-Z][a-z]{2})(\\d+)([A-Z][a-z]{2})')\n",
    "    extracted_df = df[colname].str.extract(r'p\\.([A-Z])(\\d+)([A-Z])')\n",
    "\n",
    "    print(extracted_df)\n",
    "    \n",
    "    # Convert three-letter codes to one-letter codes\n",
    "    #extracted_df[0] = extracted_df[0].apply(three_to_one)\n",
    "    #extracted_df[2] = extracted_df[2].apply(three_to_one)\n",
    "    \n",
    "    # Assign columns to original DataFrame\n",
    "    df['Before_AA'+suffix] = extracted_df[0]\n",
    "    df['AA_Pos'+suffix] = extracted_df[1]\n",
    "    df['After_AA'+suffix] = extracted_df[2]\n",
    "    \n",
    "    # Get BLOSUM62 matrix score for amino acid substitutions\n",
    "    matrix = bl.BLOSUM(62)\n",
    "    \n",
    "    def get_blosum_score(row):\n",
    "        return matrix.get(row[\"Before_AA\"+suffix], {}).get(row[\"After_AA\"+suffix], None)\n",
    "\n",
    "    def get_grantham_score(row):\n",
    "        if (row[\"Before_AA\"+suffix] != None) and (row[\"After_AA\"+suffix] != None) and (math.isnan(row[\"Before_AA\"+suffix]) != True) and (math.isnan(row[\"After_AA\"+suffix]) != True):\n",
    "            return granthamdict[(row[\"Before_AA\"+suffix], row[\"After_AA\"+suffix])]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    df[\"Before_AA\" + suffix] = pd.to_numeric(df[\"Before_AA\" + suffix], errors=\"coerce\")\n",
    "    df[\"AA_Pos\" + suffix] = pd.to_numeric(df[\"AA_Pos\" + suffix], errors=\"coerce\")\n",
    "    df[\"After_AA\" + suffix] = pd.to_numeric(df[\"After_AA\" + suffix], errors=\"coerce\")\n",
    "\n",
    "    \n",
    "    df['BLOSUM62_Score'+suffix] = df.apply(get_blosum_score, axis=1)\n",
    "    df['Grantham_Score'+suffix] = df.apply(get_grantham_score, axis =1)\n",
    "    \n",
    "    df.loc[df['AA_Pos'+suffix].isna(), 'BLOSUM62_Score'+suffix] = pd.NA\n",
    "    df.loc[df['AA_Pos'+suffix].isna(), 'Grantham_Score'+suffix] = pd.NA\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68645c",
   "metadata": {},
   "source": [
    "# Part 6: <a class=\"anchor\" id=\"part-6\"></a>ClinGen VCEP Gene-specific Criteria Specifications for VUS Reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46864ed6",
   "metadata": {},
   "source": [
    "# Step 35: <a class=\"anchor\" id=\"step-35\"></a> Updating Final Decision Trees of Evidence Codes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3820a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dictionary_column(input_column):\n",
    "    output_column = []\n",
    "    for dictionary in input_column:\n",
    "        count = dictionary\n",
    "        if count.get('PS', 0) >= 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 2 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PP', 0) == 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) == 2 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PS', 0) == 2:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 3 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) == 2 and count.get('PM', 0) == 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) == 4 and count.get('PM', 0) == 1 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PP', 0) == 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 3:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PP', 0) == 2 and count.get('PM', 0) == 2:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PP', 0) == 4 and count.get('PM', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('BP', 0) == 1 and count.get('BS', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BP', 0) == 2:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BA', 0) >= 1:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) >= 2:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('PP', 0) > 0 and count.get('PM', 0) > 0:\n",
    "            output_column.append('Variant of Uncertain Significance')\n",
    "        else:\n",
    "            output_column.append('Variant of Uncertain Significance')\n",
    "    return output_column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f269353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dictionary_column_brca1(input_column):\n",
    "    output_column = []\n",
    "    for dictionary in input_column:\n",
    "        count = dictionary\n",
    "        if count.get('PS', 0) >= 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PS', 0) >= 3:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 1 and count.get('PS', 0) == 2:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PS', 0) == 2:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 3 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PM', 0) == 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PS', 0) == 1 and count.get('PP', 0) >= 4:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PS', 0) == 2:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PP', 0) == 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) >= 3:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) >= 2 and count.get('PP', 0) >= 2:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PP', 0) >= 4:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('BA', 0) == 1:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) >= 2:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BM', 0) >= 2:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BM', 0) == 1 and count.get('BP', 0) >= 1:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BP', 0) >= 3:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BP', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BP', 0) >= 2:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BM', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BM', 0) == 1 and count.get('BP', 0) >= 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BS', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        else:\n",
    "            output_column.append('Variant of Uncertain Significance')\n",
    "    \n",
    "    return output_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dictionary_column_pten(input_column):\n",
    "    output_column = []\n",
    "    \n",
    "    for dictionary in input_column:\n",
    "        count = dictionary\n",
    "        if count.get('PS', 0) >= 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 2 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PP', 0) == 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PVS', 0) == 1 and count.get('PP', 0) >= 2:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PS', 0) >= 2:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 3 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PM', 0) >= 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 4 and count.get('PM', 0) == 1 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PVS', 0) == 1 and count.get('PM', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PVS', 0) == 1 and count.get('PP', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('BA', 0) == 1:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) >= 2:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BP', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BP', 0) >= 2:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BS', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        else:\n",
    "            output_column.append('Variant of Uncertain Significance')\n",
    "    \n",
    "    return output_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dictionary_column_tp53(input_column):\n",
    "    output_column = []\n",
    "    \n",
    "    for dictionary in input_column:\n",
    "        count = dictionary\n",
    "        if count.get('PS', 0) >= 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 2 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) == 1 and count.get('PM', 0) == 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PS', 0) >= 2:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 3 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) >= 3 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PM', 0) == 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PP', 0) >= 4 and count.get('PM', 0) == 1 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PP', 0) == 1 and count.get('PVS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) >= 1 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PP', 0) >= 2 and count.get('PS', 0) == 1:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) >= 3:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 2 and count.get('PP', 0) >= 2:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('PM', 0) == 1 and count.get('PP', 0) >= 4:\n",
    "            output_column.append('Likely Pathogenic')\n",
    "        elif count.get('BA', 0) == 1:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) >= 2:\n",
    "            output_column.append('Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BM', 0) >= 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BS', 0) == 1 and count.get('BP', 0) >= 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BP', 0) >= 2:\n",
    "            output_column.append('Likely Benign')\n",
    "        elif count.get('BM', 0) == 1 and count.get('BP', 0) == 1:\n",
    "            output_column.append('Likely Benign')\n",
    "        else:\n",
    "            output_column.append('Variant of Uncertain Significance')\n",
    "    \n",
    "    return output_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480d7b1",
   "metadata": {},
   "source": [
    "# Step 36: <a class=\"anchor\" id=\"step-36\"></a> Updating BRCA1 evidence codes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea573c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_BRCA1_BA_BScodes(df):\n",
    "    df[\"AlleleFreq\"] = pd.to_numeric(df[\"AlleleFreq\"], errors=\"coerce\")\n",
    "    df['AlleleFreq'] = df['AlleleFreq'].fillna(0).astype(float)\n",
    "    print(df)\n",
    "    for idx, row in df.iterrows():\n",
    "        allele_freq = row['AlleleFreq']\n",
    "    \n",
    "    #just need the gnomAD v2.1 (non-cancer, exome only subset)\n",
    "#         try:\n",
    "#             if row['AlleleFreq'] < row['Allele Frequency']:\n",
    "#                 allele_freq = row['Allele Frequency']\n",
    "#         except:\n",
    "#             allele_freq = row['AlleleFreq']\n",
    "                \n",
    "        print(allele_freq)\n",
    "        \n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "\n",
    "        if allele_freq > 0.001 and 'BA1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BA1')\n",
    "            post_evidence_codes.append('BA1')\n",
    "        elif 0.0001 <= allele_freq <= 0.001 and 'BS1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BS1')\n",
    "            post_evidence_codes.append('BS1')\n",
    "        elif 0.00002 <= allele_freq < 0.0001:\n",
    "            int_evidence_codes.append('BP')\n",
    "            post_evidence_codes.append('BS1_Supporting')\n",
    "        elif allele_freq == 0 and 'PM2_Supporting' not in post_evidence_codes:\n",
    "            int_evidence_codes.append('PP')\n",
    "            post_evidence_codes.append('PM2_Supporting')\n",
    "        \n",
    "        print(int_evidence_codes)\n",
    "        print(post_evidence_codes)\n",
    "        \n",
    "        df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98903ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_functionalevidencecolumnsforbrca1(df):\n",
    "    for index, row in df.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "\n",
    "        if row[\"func.class\"] == \"FUNC\" and 'BS3' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BS3')\n",
    "            post_evidence_codes.append('BS3')\n",
    "            \n",
    "            if 'PS3' in int_evidence_codes:\n",
    "                print(int_evidence_codes)\n",
    "                int_evidence_codes.remove('PS3')\n",
    "                print(post_evidence_codes)\n",
    "                post_evidence_codes.remove('PS3')\n",
    "\n",
    "        elif row[\"func.class\"] == \"LOF\" and 'PS3' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('PS3')\n",
    "            post_evidence_codes.append('PS3')\n",
    "            \n",
    "            if 'BS3' in int_evidence_codes:\n",
    "                int_evidence_codes.remove('BS3')\n",
    "                post_evidence_codes.remove('BS3')\n",
    "        \n",
    "        df.at[index, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        df.at[index, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1pvs1splice = pd.read_csv('inputs/BRCA1_PVS1Splice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f115851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted impact on splicing function\n",
    "def updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore):\n",
    "    if spliceaiscore >= 0.2:\n",
    "        return 'Impact'\n",
    "    elif 0.2 < spliceaiscore < 0.1:\n",
    "        return 'Not informative'\n",
    "    elif spliceaiscore <= 0.1:\n",
    "        return 'No impact'\n",
    "    \n",
    "#inide functional domain function\n",
    "def updateevidencecode_BRCA1_insidefunctionaldomain(aapos):\n",
    "    aapos = int(aapos)\n",
    "    return (2 <= aapos <= 101) or (1391 <= aapos <= 1424) or (1650 <= aapos <= 1857)\n",
    "\n",
    "#predicted impact on protein\n",
    "def updateevidencecode_BRCA1_predictedimpactonprotein(bayesdelscore):\n",
    "    if bayesdelscore >= 0.28:\n",
    "        return 'Impact'\n",
    "    elif 0.15 < bayesdelscore < 0.28:\n",
    "        return 'Not informative'\n",
    "    elif bayesdelscore <= 0.15:\n",
    "        return 'No impact'\n",
    "\n",
    "\n",
    "\n",
    "def updateevidencecodes_BRCA1_varianttypeandlocation(df):\n",
    "    \n",
    "    df['InsideFunctionalDomain'] = ''\n",
    "    df['SpliceAIscore'] = ''\n",
    "    df['BayesDelNoAFScore'] = ''\n",
    "    df['AlleleFreq'] = ''\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        conseq = row['VEP Annotation']\n",
    "        \n",
    "        aa_pos = row['AA_Pos_gnomad']\n",
    "        \n",
    "        variant = row['Transcript Consequence']\n",
    "        genetranscript= row['Transcript']\n",
    "        data = fetch_variant_dbnsfp_data(genetranscript, variant, species=\"homo_sapiens\", dbnsfp_fields=\"ALL\")    \n",
    "        bayesdelscore, spliceaiscore, gnomad_exomes_non_cancer_af = extract_values(data[0])\n",
    "        df.at[idx, 'AlleleFreq'] = gnomad_exomes_non_cancer_af\n",
    "        \n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        df.at[idx, 'SpliceAIscore'] = spliceaiscore\n",
    "        \n",
    "        \n",
    "        \n",
    "        if any(value in conseq for value in ['splice']):\n",
    "            matching_row = brca1pvs1splice[brca1pvs1splice['Transcript'] == row['transcript_variant']]\n",
    "            print(conseq)\n",
    "            print(row['transcript_variant'])\n",
    "            if not matching_row.empty:\n",
    "                post_code = matching_row['PostCode'].iloc[0]\n",
    "                intermediate_code = matching_row['IntermediateCode'].iloc[0]\n",
    "                print(post_code)\n",
    "                print(intermediate_code)\n",
    "                if 'PVS1' in int_evidence_codes:\n",
    "                    post_evidence_codes.remove('PVS1')\n",
    "                    int_evidence_codes.remove('PVS1')\n",
    "                \n",
    "                if pd.isna(intermediate_code):\n",
    "                    intermediate_code = \"\"\n",
    "                                \n",
    "                int_evidence_codes.append(intermediate_code)\n",
    "                post_evidence_codes.append(post_code)\n",
    "        \n",
    "                \n",
    "        if any(value in conseq for value in ['intron','splice_region']):\n",
    "            print(conseq)\n",
    "            print(row['transcript_variant'])\n",
    "            print(updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore))\n",
    "            if updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'Impact':\n",
    "                if 'PP3' not in int_evidence_codes:\n",
    "                    int_evidence_codes.append('PP3')\n",
    "                    post_evidence_codes.append('PP3')\n",
    "                elif updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'Not informative':\n",
    "                    pass\n",
    "\n",
    "                elif updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'No impact':\n",
    "                    if 'BP4' not in int_evidence_codes:\n",
    "                        int_evidence_codes.append('BP4')\n",
    "                        post_evidence_codes.append('BP4')\n",
    "        \n",
    "        \n",
    "        \n",
    "        if any(value in conseq for value in ['synonymous_variant', 'missense_variant', 'inframe_deletion', 'inframe_insertion']):\n",
    "        #if conseq in ['synonymous_variant', 'missense_variant', 'inframe_deletion', 'inframe_insertion']:\n",
    "            df.at[idx, 'InsideFunctionalDomain'] = updateevidencecode_BRCA1_insidefunctionaldomain(aa_pos)\n",
    "            \n",
    "            \n",
    "            #aa_pos = row['AA_Pos_gnomad']\n",
    "            #variant = row['Transcript Consequence']\n",
    "            #genetranscript= row['Transcript']\n",
    "            #data = fetch_variant_dbnsfp_data(genetranscript, variant, species=\"homo_sapiens\", dbnsfp_fields=\"ALL\")\n",
    "            #bayesdelscore, spliceaiscore, gnomad_exomes_non_cancer_af = extract_values(data[0])\n",
    "            \n",
    "            df.at[idx, 'BayesDelNoAFScore'] = bayesdelscore\n",
    "            #df.at[idx, 'AlleleFreq'] = gnomad_exomes_non_cancer_af\n",
    "\n",
    "            if any(value in conseq for value in ['synonymous_variant']):\n",
    "            #if conseq == 'synonymous_variant':\n",
    "                if updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'Impact':\n",
    "                    if 'PP3' not in int_evidence_codes:\n",
    "                        int_evidence_codes.append('PP3')\n",
    "                        post_evidence_codes.append('PP3')\n",
    "\n",
    "\n",
    "                elif updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'Not informative':\n",
    "                    pass\n",
    "\n",
    "                elif updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'No impact':\n",
    "                    if updateevidencecode_BRCA1_insidefunctionaldomain(aa_pos):\n",
    "                        if 'BP4' not in int_evidence_codes:\n",
    "                            int_evidence_codes.append('BP4')\n",
    "                            post_evidence_codes.append('BP4')\n",
    "\n",
    "                        if 'BP7' not in int_evidence_codes:\n",
    "                            post_evidence_codes.append('BP7')\n",
    "                            int_evidence_codes.append('BP7')\n",
    "                    else:\n",
    "                        if 'BP1' in int_evidence_codes:\n",
    "                            post_evidence_codes.remove('BP1')\n",
    "                            int_evidence_codes.remove('BP1')\n",
    "                            post_evidence_codes.append('BP1_Strong')\n",
    "                            int_evidence_codes.append('BS')\n",
    "                        else:\n",
    "                            post_evidence_codes.append('BP1_Strong')\n",
    "                            int_evidence_codes.append('BS')\n",
    "\n",
    "            if any(value in conseq for value in ['missense_variant', 'inframe_deletion', 'inframe_insertion']):\n",
    "            #if conseq in ['missense_variant', 'inframe_deletion', 'inframe_insertion']:\n",
    "                if updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'Impact':\n",
    "                    if 'PP3' not in int_evidence_codes:\n",
    "                        int_evidence_codes.append('PP3')\n",
    "                        post_evidence_codes.append('PP3')\n",
    "\n",
    "\n",
    "                elif updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'Not informative':\n",
    "                    if updateevidencecode_BRCA1_insidefunctionaldomain(aa_pos):\n",
    "                        if updateevidencecode_BRCA1_predictedimpactonprotein(bayesdelscore) == 'Impact':\n",
    "                            if 'PP3' not in int_evidence_codes:\n",
    "                                int_evidence_codes.append('PP3')\n",
    "                                post_evidence_codes.append('PP3')\n",
    "\n",
    "\n",
    "                elif updateevidencecode_BRCA1_predictedimpactonsplicing(spliceaiscore) == 'No impact':\n",
    "                    if updateevidencecode_BRCA1_insidefunctionaldomain(aa_pos):\n",
    "                        if updateevidencecode_BRCA1_predictedimpactonprotein(bayesdelscore) == 'Impact':\n",
    "                            if 'PP3' not in int_evidence_codes:\n",
    "                                int_evidence_codes.append('PP3')\n",
    "                                post_evidence_codes.append('PP3')\n",
    "\n",
    "\n",
    "                        elif updateevidencecode_BRCA1_predictedimpactonprotein(bayesdelscore) == 'No impact':\n",
    "                            if 'BP4' not in int_evidence_codes:\n",
    "                                int_evidence_codes.append('BP4')\n",
    "                                post_evidence_codes.append('BP4')\n",
    "\n",
    "                    else:\n",
    "                        if 'BP1' in int_evidence_codes:\n",
    "                            post_evidence_codes.remove('BP1')\n",
    "                            int_evidence_codes.remove('BP1')\n",
    "                            post_evidence_codes.append('BP1_Strong')\n",
    "                            int_evidence_codes.append('BS')\n",
    "                        else:\n",
    "                            post_evidence_codes.append('BP1_Strong')\n",
    "                            int_evidence_codes.append('BS')\n",
    "\n",
    "        df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27791b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_variant_dbnsfp_data(genetranscript, variant, species, dbnsfp_fields):\n",
    "    server = \"https://rest.ensembl.org\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Construct the endpoint URL\n",
    "    endpoint = f\"/vep/{species}/hgvs/{genetranscript}:{variant}?dbNSFP={dbnsfp_fields},transcript_match=1&SpliceAI=1\"\n",
    "\n",
    "    r = requests.get(server + endpoint, headers=headers)\n",
    "    \n",
    "    if not r.ok:\n",
    "        r.raise_for_status()\n",
    "        sys.exit()\n",
    "    \n",
    "    print(endpoint)\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "\n",
    "def extract_values(api_output):\n",
    "    # Ensure the input is a dictionary\n",
    "    if not isinstance(api_output, dict):\n",
    "        raise ValueError(\"Input should be a dictionary containing API output.\")\n",
    "\n",
    "    # Extract bayesdel_noaf_score\n",
    "    bayesdel_noaf_score = None\n",
    "    for transcript in api_output.get(\"transcript_consequences\", []):\n",
    "        if \"bayesdel_noaf_score\" in transcript:\n",
    "            bayesdel_noaf_score = float(transcript[\"bayesdel_noaf_score\"])\n",
    "            break  # exit the loop once the value is found\n",
    "            \n",
    "    # Extract gnomad af\n",
    "    gnomad_exomes_non_cancer_af = None\n",
    "    for transcript in api_output.get(\"transcript_consequences\", []):\n",
    "        if \"gnomad_exomes_non_cancer_af\" in transcript:\n",
    "            gnomad_exomes_non_cancer_af = float(transcript[\"gnomad_exomes_non_cancer_af\"])\n",
    "            break  # exit the loop once the value is found\n",
    "            \n",
    "    if gnomad_exomes_non_cancer_af == None:\n",
    "        gnomad_exomes_non_cancer_af = 0\n",
    "    print(gnomad_exomes_non_cancer_af)    \n",
    "        \n",
    "\n",
    "            \n",
    "    # Extract and find the largest SpliceAI score\n",
    "    spliceai_scores = []\n",
    "    for transcript in api_output.get(\"transcript_consequences\", []):\n",
    "        if \"spliceai\" in transcript:\n",
    "            spliceai_data = transcript[\"spliceai\"]\n",
    "            for key in [\"DS_DL\", \"DS_DG\", \"DS_AL\", \"DS_AG\"]:\n",
    "                if key in spliceai_data:\n",
    "                    spliceai_scores.append(spliceai_data[key])\n",
    "    max_spliceai_score = max(spliceai_scores, default=None)\n",
    "\n",
    "    return bayesdel_noaf_score, max_spliceai_score, gnomad_exomes_non_cancer_af\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06681695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateBRCA1_PS1code(gnomaddf):\n",
    "    # Import the CSV\n",
    "    df = pd.read_csv('inputs/BRCA1_PS1.csv')\n",
    "    \n",
    "    df['AAPos'] = df['AAPos'].fillna(-1).astype(int)\n",
    "    df['Classification'] = df['Classification'].astype(str)\n",
    "    gnomaddf['AA_Pos_gnomad'] = gnomaddf['AA_Pos_gnomad'].fillna(0).astype(int)\n",
    "    \n",
    "    #df = process_aabeforeandafterwithblosumandgranthamdf(df, colname='Preferred Variant Title', suffix='_VCEP')\n",
    "    # Filter rows based on the Classification column\n",
    "    #df = df[df['Classification'].isin(['Pathogenic', 'Likely Pathogenic'])]\n",
    "    \n",
    "    # Mark rows where AA_Pos matches with the other dataframe\n",
    "    for idx, row in gnomaddf.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        conseq = row['VEP Annotation']\n",
    "        \n",
    "        if any(value in conseq for value in ['missense_variant']):\n",
    "            filtered_df = df[(df['AAPos'] == row['AA_Pos_gnomad']) & (df['Before1AA'] == row['Before_AA_gnomad']) & (df['After1AA'] == row['After_AA_gnomad'])]           \n",
    "            filtered_df = filtered_df.reset_index()\n",
    "            if not filtered_df.empty:\n",
    "                classcheck = filtered_df.at[0, 'Classification']\n",
    "                if classcheck == 'Pathogenic':\n",
    "                    int_evidence_codes.append('PS1')\n",
    "                    post_evidence_codes.append('PS1')\n",
    "                elif classcheck == 'Likely Pathogenic':\n",
    "                    int_evidence_codes.append('PM')\n",
    "                    post_evidence_codes.append('PS1_Moderate')\n",
    "\n",
    "        gnomaddf.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        gnomaddf.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    \n",
    "    return gnomaddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write BRCA1 gene specific rules\n",
    "#redo new significance for BRCA1\n",
    "\n",
    "def newcodesforbrca1(df):\n",
    "    \n",
    "    #PVS1: current set of BRCA1 alleles that lead to Termination are predicted to undergo NMD in an exon in the biologically relevant transcript\n",
    "    #Using Intervar call should recapitulate this pretty well and will complete a manual re-check\n",
    "    #also using PVS1 for splcing using the recommended table by the VCEP\n",
    "    \n",
    "    #PS1: Can only compare to variants asserted as pathogenic by the ClinGen BRCA1 VCEP\n",
    "    df = zero_out(df, 'PS1')\n",
    "    df = updateBRCA1_PS1code(df)\n",
    "    \n",
    "    #PS2: De novo status; given these alleles originate from gnomAD we do not know the true de novo status. Thus PS2=0 for everything.\n",
    "    #brca1 vcep does not use this code anyway\n",
    "    df = zero_out(df, 'PS2')\n",
    "\n",
    "    #PS3: functional data; MAVE data incorporated\n",
    "\n",
    "    #PS4: Prevalence in cases vs. controls; given these alleles originate from gnomAD we do not know the true prevalence. Thus PS4=0 for everything.\n",
    "    #Using Intervar call and manual re-check\n",
    "    df = zero_out(df, 'PS4')\n",
    "\n",
    "    #PM1: location in mutational hotspot or well established functional domain; BRCA1 vcep N/A code\n",
    "    df = zero_out(df, 'PM1')\n",
    "\n",
    "    #PM2: allele needs to be absent from gnomAD but these are all gnomAD variants\n",
    "    df = zero_out(df, 'PM2')\n",
    "\n",
    "    #PM3: detected in trans with confirmed phasing; unknown for gnomAD variants thus zero out PM3\n",
    "    df = zero_out(df, 'PM3')\n",
    "\n",
    "    #PM4: protein length changes; Not applicable in BRCA1 criteria\n",
    "    df = zero_out(df, 'PM4')\n",
    "\n",
    "    #PM5 genomic ptc changes; not really applicable for this analysis\n",
    "    df = zero_out(df, 'PM5')\n",
    "\n",
    "    #PM6: de novo; unknown for public variant data; n/a for brca1\n",
    "    df = zero_out(df, 'PM6')\n",
    "\n",
    "    #PP1: Co-segregation; unknown for gnomAD data\n",
    "    df = zero_out(df, 'PP1')\n",
    "\n",
    "    #PP2: Missense in a gene with low rate of benign missense; Not applicable in BRCA1 criteria\n",
    "    df = zero_out(df, 'PP2')\n",
    "\n",
    "    #PP3: in silico; using vcep criteria below\n",
    "    df = zero_out(df, 'PP3')\n",
    "\n",
    "    #PP4: Family history; unknown for gnomAD alleles\n",
    "    df = zero_out(df, 'PP4')\n",
    "\n",
    "    #PP5: Reputable source; Not applicable \n",
    "    df = zero_out(df, 'PP5')\n",
    "\n",
    "    #BA1 and #BS1\n",
    "    df = zero_out(df, 'BA1')\n",
    "    df = zero_out(df, 'BS1')\n",
    "\n",
    "    #BS2; evidence code not applicable\n",
    "    df = zero_out(df, 'BS2')\n",
    "\n",
    "    #BS3 update functional code with PS3; MAVE data incorporated\n",
    "\n",
    "    #BS4: Segregation data unknown for gnomAD alleles\n",
    "    df = zero_out(df, 'BS4')\n",
    "\n",
    "    #BP1: missene variant in gene with known truncations; Not applicable\n",
    "    df = zero_out(df, 'BP1')\n",
    "    \n",
    "    #BP2: Observed in trans or cis with known path variant; unknown in gnomAD data; Not applicable in BRCA1 criteria\n",
    "    df = zero_out(df, 'BP2')\n",
    "\n",
    "    #BP3: In-frame deletions; Not applicable in BRCA1 criteria\n",
    "    df = zero_out(df, 'BP3')\n",
    "\n",
    "    #BP4 handled with other insilico codes\n",
    "    df = zero_out(df, 'BP4')\n",
    "\n",
    "    #BP5: Alternate molecular basis of disease; Not applicable \n",
    "    df = zero_out(df, 'BP5')\n",
    "\n",
    "    #BP6: Reputable sources report as benign; Not applicable \n",
    "    df = zero_out(df, 'BP6')\n",
    "\n",
    "    #BP7: Synonymous variant; done with other insilico above\n",
    "    df = zero_out(df, 'BP7')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PVS1, PP3, BP4, BP7, BP1\n",
    "    df = updateevidencecodes_BRCA1_varianttypeandlocation(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    #BA1, BS1, PM2\n",
    "    df = update_BRCA1_BA_BScodes(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51a3b5",
   "metadata": {},
   "source": [
    "# Step 37: <a class=\"anchor\" id=\"step-37\"></a> BRCA1 Reclassifications\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRCA1assaydata_hg38 = liftover_dataframe(BRCA1assaydata, 'chromosome', 'position (hg19)')\n",
    "\n",
    "\n",
    "BRCA1assaydata['concatenatedpositionassay'] = concatenate_columns_as_string(BRCA1assaydata, 'chromosome', 'position (hg19)', 'reference', 'alt')\n",
    "BRCA1_gnomadv2_withevidencecodes_pre_functionaldata['concatenatedpositiongnomad'] = concatenate_columns_as_string(BRCA1_gnomadv2_withevidencecodes_pre_functionaldata, 'Chrom', 'Pos', 'Reference', 'Alternate')\n",
    "BRCA1_gnomadv2_withevidencecodes_withfunctionaldata = BRCA1_gnomadv2_withevidencecodes_pre_functionaldata.merge(BRCA1assaydata, how=\"left\", left_on=\"concatenatedpositiongnomad\", right_on=\"concatenatedpositionassay\").dropna(subset=['concatenatedpositionassay'])\n",
    "BRCA1_gnomadv2_withfunctionaldataandcorrectaa = process_aabeforeandafterwithblosumandgranthamdf(update_functionalevidencecolumnsforbrca1(add_evidence_codes_column(concatenate_headers(BRCA1_gnomadv2_withevidencecodes_withfunctionaldata))), colname='Protein Consequence', suffix='_gnomad')\n",
    "BRCA1_gnomadv2_newsignificance = accumulate_evidence(newcodesforbrca1(BRCA1_gnomadv2_withfunctionaldataandcorrectaa))\n",
    "BRCA1_gnomadv2_newsignificance['NewClinicalSignificance'] = process_dictionary_column_brca1(BRCA1_gnomadv2_newsignificance['EvidenceDictionary'])\n",
    "\n",
    "\n",
    "\n",
    "BRCA1assaydata_hg38['concatenatedpositionassay'] = concatenate_columns_as_string(BRCA1assaydata_hg38, 'chromosome', 'hg38_pos', 'reference', 'alt')\n",
    "BRCA1_gnomadv3_withevidencecodes_pre_functionaldata['concatenatedpositiongnomad'] = concatenate_columns_as_string(BRCA1_gnomadv3_withevidencecodes_pre_functionaldata, 'Chrom', 'Pos', 'Reference', 'Alternate')\n",
    "BRCA1_gnomadv3_withevidencecodes_withfunctionaldata = BRCA1_gnomadv3_withevidencecodes_pre_functionaldata.merge(BRCA1assaydata_hg38, how=\"left\", left_on=\"concatenatedpositiongnomad\", right_on=\"concatenatedpositionassay\").dropna(subset=['concatenatedpositionassay'])\n",
    "BRCA1_gnomadv3_withfunctionaldataandcorrectaa = process_aabeforeandafterwithblosumandgranthamdf(update_functionalevidencecolumnsforbrca1(add_evidence_codes_column(concatenate_headers(BRCA1_gnomadv3_withevidencecodes_withfunctionaldata))), colname='Protein Consequence', suffix='_gnomad')\n",
    "BRCA1_gnomadv3_newsignificance = accumulate_evidence(newcodesforbrca1(BRCA1_gnomadv3_withfunctionaldataandcorrectaa))\n",
    "BRCA1_gnomadv3_newsignificance['NewClinicalSignificance'] = process_dictionary_column_brca1(BRCA1_gnomadv3_newsignificance['EvidenceDictionary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e10cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRCA1_gnomadv2_newsignificance.to_csv('output/reclassified/BRCA1_gnomadv2_newsignificance.csv')\n",
    "BRCA1_gnomadv3_newsignificance.to_csv('output/reclassified/BRCA1_gnomadv3_newsignificance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936dc13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "BRCA1assaydata['concatenatedpositionassay'] = concatenate_columns_as_string(BRCA1assaydata, 'chromosome', 'position (hg19)', 'reference', 'alt')\n",
    "BRCA1_fayer['concatenatedpositiongnomad'] = concatenate_columns_as_string(BRCA1_fayer, 'Chrom', 'Pos', 'Reference', 'Alternate')\n",
    "BRCA1_fayer_withfunctionaldata = BRCA1_fayer.merge(BRCA1assaydata, how=\"left\", left_on=\"concatenatedpositiongnomad\", right_on=\"concatenatedpositionassay\").dropna(subset=['concatenatedpositionassay'])\n",
    "BRCA1_fayer_withfunctionaldataandcorrectaa = process_aabeforeandafterwithblosumandgranthamforbrca1fayerdf(update_functionalevidencecolumnsforbrca1(add_evidence_codes_column(concatenate_headers(BRCA1_fayer_withfunctionaldata))), colname='Protein Consequence', suffix='_gnomad')\n",
    "BRCA1_fayer_newsignificance = accumulate_evidence(newcodesforbrca1(BRCA1_fayer_withfunctionaldataandcorrectaa))\n",
    "BRCA1_fayer_newsignificance['NewClinicalSignificance'] = process_dictionary_column_brca1(BRCA1_fayer_newsignificance['EvidenceDictionary'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49384081",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRCA1_fayer_newsignificance.to_csv('output/reclassified/BRCA1_Fayer_newsignificance.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fec93",
   "metadata": {},
   "source": [
    "# Step 38: <a class=\"anchor\" id=\"step-38\"></a> Updating TP53 evidence codes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_PS1(df1, df2):\n",
    "#     # Loop through rows of df1 where PS1 column has value 1\n",
    "#     for idx, row in df1[df1['PS1'] == 1].iterrows():\n",
    "#         #print(df2['AA_Pos'].values)\n",
    "#         # Check if AApos value in current row is not in df2's AApos column\n",
    "#         if row['AA_Pos'] not in df2['AA_Pos'].values:\n",
    "#             #print('inside')\n",
    "#             # If PS1 is in PreGeneSpecificEvidenceCodes list, remove it\n",
    "#             if 'PS1' in row['PostGeneSpecificEvidenceCodes']:\n",
    "#                 updated_list = [code for code in row['PostGeneSpecificEvidenceCodes'] if code != 'PS1']\n",
    "#                 df1.at[idx, 'PostGeneSpecificEvidenceCodes'] = updated_list\n",
    "#     return df1\n",
    "\n",
    "p53missense = pd.read_csv(\"inputs/p53missensetable.csv\")\n",
    "\n",
    "def process_PM1(df1, cancerhotspots_snv):\n",
    "    for idx, row in df1.iterrows():\n",
    "        aa_pos = int(row['AA_Pos'])\n",
    "                \n",
    "        int_gene_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_gene_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if 'PM1' in int_gene_codes:\n",
    "            post_gene_codes.remove('PM1')\n",
    "            int_gene_codes.remove('PM1')\n",
    "        \n",
    "        if aa_pos in [175, 245, 248, 249, 273, 282]:\n",
    "            if 'PM1' not in int_gene_codes:\n",
    "                post_gene_codes.append('PM1')\n",
    "                int_gene_codes.append('PM1')        \n",
    "               \n",
    "        cancerhotspots_snv['Amino_Acid_Position'] = pd.Series(dtype='int')\n",
    "        \n",
    "        tp53_row = cancerhotspots_snv[(cancerhotspots_snv['Hugo_Symbol'] == 'TP53') & (cancerhotspots_snv['Amino_Acid_Position'] == aa_pos)]\n",
    "        if not tp53_row.empty and tp53_row.iloc[0]['Mutation_Count'] >= 10 and 'PM1' in int_gene_codes == 0:\n",
    "            if 'PM1' not in post_gene_codes:\n",
    "                post_gene_codes.append('PM1')\n",
    "                int_gene_codes.append('PM1')\n",
    "        \n",
    "        df1.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_gene_codes\n",
    "        df1.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_gene_codes\n",
    "        \n",
    "    return df1\n",
    "\n",
    "def updateTP53_PS1code(gnomaddf):\n",
    "    # Import the CSV\n",
    "    df = pd.read_csv('inputs/TP53_PLP.csv')\n",
    "    \n",
    "    df['AAPos'] = df['AAPos'].fillna(-1).astype(int)   \n",
    "    gnomaddf['AA_Pos_gnomad'] = gnomaddf['AA_Pos_gnomad'].fillna(0).astype(int)\n",
    "    \n",
    "    #df = process_aabeforeandafterwithblosumandgranthamdf(df, colname='Preferred Variant Title', suffix='_VCEP')\n",
    "    # Filter rows based on the Classification column\n",
    "    #df = df[df['Classification'].isin(['Pathogenic', 'Likely Pathogenic'])]\n",
    "    \n",
    "    # Mark rows where AA_Pos matches with the other dataframe\n",
    "    for idx, row in gnomaddf.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        filtered_df = df[(df['AAPos'] == row['AA_Pos_gnomad']) & (df['Before1AA'] == row['Before_AA_gnomad']) & (df['After1AA'] == row['After_AA_gnomad'])]           \n",
    "        if not filtered_df.empty:\n",
    "            if 'PS1' not in post_evidence_codes:\n",
    "                int_evidence_codes.append('PS1')\n",
    "                post_evidence_codes.append('PS1')\n",
    "\n",
    "        gnomaddf.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        gnomaddf.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    \n",
    "    return gnomaddf\n",
    "\n",
    "\n",
    "\n",
    "# def process_PM4(df1, df2):\n",
    "#     # Loop through rows of df1 where PS1 column has value 1\n",
    "#     for idx, row in df1[df1['PM4'] == 1].iterrows():\n",
    "#         #print(df2['AA_Pos'].values)\n",
    "#         # Check if AApos value in current row is not in df2's AApos column\n",
    "#         if row['AA_Pos'] not in df2['AA_Pos'].values:\n",
    "#             #print('inside')\n",
    "#             # If PS1 is in PreGeneSpecificEvidenceCodes list, remove it\n",
    "#             if 'PS1' in row['PostGeneSpecificEvidenceCodes']:\n",
    "#                 updated_list = [code for code in row['PostGeneSpecificEvidenceCodes'] if code != 'PS1']\n",
    "#                 df1.at[idx, 'PostGeneSpecificEvidenceCodes'] = updated_list\n",
    "#     return df1\n",
    "\n",
    "\n",
    "\n",
    "#prepare TP53 assay data for merge with gnomad data annotated with intervar evidence codes\n",
    "#two functions to switch from single letter amino acid codes to three letter codes in the assay data\n",
    "\n",
    "\n",
    "def updatedPM2forFayer(inputdf):\n",
    "    df = find_matching_values(df1 = inputdf, df2 = TP53_gnomadv2, df3 = TP53_gnomadv3, column_df1 = 'Protein Consequence', column_df2 = 'Protein Consequence', column_df3 = 'Protein Consequence')\n",
    "    for index, row in df.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if row[\"AlleleFoundIngnomad\"] == False:\n",
    "            int_evidence_codes.append('PP')\n",
    "            post_evidence_codes.append('PM2_Supporting')\n",
    "                \n",
    "        df.at[index, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        df.at[index, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "    return df\n",
    "\n",
    "\n",
    "def aa_mutation_singletothreeletter(mutation):\n",
    "    one_letter_to_three_letter = {\n",
    "        'A': 'Ala', 'R': 'Arg', 'N': 'Asn', 'D': 'Asp', 'C': 'Cys', \n",
    "        'Q': 'Gln', 'E': 'Glu', 'G': 'Gly', 'H': 'His', 'I': 'Ile', \n",
    "        'L': 'Leu', 'K': 'Lys', 'M': 'Met', 'F': 'Phe', 'P': 'Pro', \n",
    "        'S': 'Ser', 'T': 'Thr', 'W': 'Trp', 'Y': 'Tyr', 'V': 'Val', 'Z': 'Ter'\n",
    "    }\n",
    "    #print(mutation)\n",
    "    parts = mutation.split('.')\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(\"Invalid mutation format\")\n",
    "\n",
    "    first_part = parts[0]\n",
    "    second_part = parts[1]\n",
    "    \n",
    "    aa_from = second_part[0]\n",
    "    aa_to = second_part[-1]\n",
    "    \n",
    "    if aa_to=='B':\n",
    "        aa_to=aa_from\n",
    "        \n",
    "    if aa_from not in one_letter_to_three_letter or aa_to not in one_letter_to_three_letter:\n",
    "        return mutation\n",
    "    \n",
    "    aa_from_three = one_letter_to_three_letter[aa_from]\n",
    "    aa_to_three = one_letter_to_three_letter[aa_to]\n",
    "    \n",
    "    return f'p.{aa_from_three}{second_part[1:-1]}{aa_to_three}'\n",
    "\n",
    "\n",
    "def convert_aamutations(df, column_name):\n",
    "    df['HGVSp'] = df[column_name].apply(aa_mutation_singletothreeletter)\n",
    "    return df\n",
    "\n",
    "# Convert mutations and create new 'HGVSp' column\n",
    "TP53assaydata_mutationsconverted = convert_aamutations(TP53assaydata, 'Variant')\n",
    "#PTENassaydata_mutationsconverted = convert_aamutations(PTENassaydata, 'Variant')\n",
    "\n",
    "def add_classifier_predictions(df1, df2):\n",
    "    merged_df = df1.merge(df2, how=\"left\", left_on=\"Protein Consequence\", right_on=\"HGVSp\").dropna(subset=[\"HGVSp\"])\n",
    "    \n",
    "#     if 'Intervar' in merged_df.columns:\n",
    "#         merged_df.dropna(subset=['Intervar'])\n",
    "        \n",
    "    return merged_df\n",
    "\n",
    "#Strong path moderate benign \n",
    "def update_functionalevidencecolumnsforp53(df):\n",
    "    for index, row in df.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if row[\"Classifier_prediction\"] == \"Functionally abnormal\" and 'PS3' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('PS3')\n",
    "            post_evidence_codes.append('PS3')\n",
    "            \n",
    "            if 'BS3' in int_evidence_codes:\n",
    "                int_evidence_codes.remove('BS3')\n",
    "                post_evidence_codes.remove('BS3')\n",
    "                \n",
    "        elif row[\"Classifier_prediction\"] == \"Functionally normal\":\n",
    "            if 'BS3' in int_evidence_codes:\n",
    "                int_evidence_codes.remove('BS3')\n",
    "                post_evidence_codes.remove('BS3')\n",
    "                \n",
    "            int_evidence_codes.append('BM')\n",
    "            post_evidence_codes.append('BS3_Moderate')\n",
    "            \n",
    "            if 'PS3' in int_evidence_codes:\n",
    "                int_evidence_codes.remove('PS3')\n",
    "                post_evidence_codes.remove('PS3')\n",
    "    \n",
    "        df.at[index, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        df.at[index, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "    return df\n",
    "\n",
    "        \n",
    "\n",
    "def update_PP3andBP4forp53(df):\n",
    "    # Merge the dataframes based on the specified columns\n",
    "    merged_df = df.merge(p53missense, \n",
    "                         left_on=['Reference', 'Alternate', 'Protein Consequence'],\n",
    "                         right_on=['Reference', 'Alternate', 'ProteinChange(P04637)'],\n",
    "                         how='left')\n",
    "\n",
    "    \n",
    "    updateddf = merged_df.copy()\n",
    "    \n",
    "    for idx, row in merged_df.iterrows():\n",
    "        conseq = row['VEP Annotation']\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if 'PP3' in int_evidence_codes:\n",
    "            int_evidence_codes.remove('PP3')\n",
    "            post_evidence_codes.remove('PP3')\n",
    "            \n",
    "        if 'BP4' in int_evidence_codes:\n",
    "            int_evidence_codes.remove('BP4')\n",
    "            post_evidence_codes.remove('BP4')\n",
    "\n",
    "        align_gvgd = row['AlignGVGD']\n",
    "        bayesdel_score = row['BayesDel']\n",
    "\n",
    "        if conseq == 'missense_variant':\n",
    "            if align_gvgd == 'Class C65' and bayesdel_score >= 0.16:\n",
    "                int_evidence_codes.append('PM')\n",
    "                post_evidence_codes.append('PP3_Moderate')\n",
    "                \n",
    "            elif align_gvgd in ['Class C25', 'Class C35', 'Class C45', 'Class C55', 'Class C65'] and bayesdel_score >= 0.16:\n",
    "                int_evidence_codes.append('PP3')\n",
    "                post_evidence_codes.append('PP3')\n",
    "                \n",
    "                \n",
    "            elif align_gvgd in ['Class C0', 'Class C15'] and bayesdel_score < 0.16:\n",
    "                int_evidence_codes.append('BP4')\n",
    "                post_evidence_codes.append('BP4')\n",
    "        \n",
    "        merged_df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        merged_df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#write TP53 gene specific rules\n",
    "#redo new significance for TP53\n",
    "\n",
    "cancerhotspots_snv = pd.read_excel('inputs/cancerhotspotsSNVs.xlsx')\n",
    "TP53_VCEP = pd.read_csv('inputs/TP53_VCEP.csv')\n",
    "\n",
    "\n",
    "def updateP53_PM5code(gnomaddf):\n",
    "        # Import the CSV\n",
    "        df = pd.read_csv('inputs/TP53_VCEP.csv')\n",
    "        df = process_aabeforeandafterwithblosumandgranthamdf(df, colname='Preferred Variant Title', suffix='_VCEP')\n",
    "        gnomaddf['Grantham_Score_VCEP'] = 0\n",
    "\n",
    "        # Filter rows based on the Classification column\n",
    "        df = df[df['Classification'].isin(['Pathogenic', 'Likely Pathogenic'])]\n",
    "\n",
    "        # Mark rows where AA_Pos matches with the other dataframe\n",
    "        print(gnomaddf.columns)\n",
    "        \n",
    "        gnomaddf['AA_Pos_gnomad'] = gnomaddf['AA_Pos_gnomad'].fillna(0).astype(int)\n",
    "        df['AA_Pos_VCEP'] = df['AA_Pos_VCEP'].fillna(-1).astype(int)\n",
    "        \n",
    "        for idx, row in gnomaddf.iterrows():\n",
    "            if row['VEP Annotation'] == 'missense_variant':\n",
    "                if row['AA_Pos_gnomad'] in df['AA_Pos_VCEP'].values:\n",
    "                    print('a')\n",
    "                    print(row['VEP Annotation'])\n",
    "                    row['AA_Pos_KnownPorLP'] = (df['AA_Pos_VCEP'] == row['AA_Pos_gnomad']).sum()\n",
    "                    print(row['AA_Pos_KnownPorLP'])\n",
    "                    matching_row = df[df['AA_Pos_VCEP'] == row['AA_Pos_gnomad']]\n",
    "                    print(matching_row)\n",
    "                    if not matching_row.empty:\n",
    "                        if matching_row['Grantham_Score_VCEP'].iloc[0] != None:\n",
    "                            print(row['Grantham_Score_VCEP'])\n",
    "                            row['Grantham_Score_VCEP'] = matching_row['Grantham_Score_VCEP'].iloc[0]\n",
    "                            print(row['Grantham_Score_VCEP'])\n",
    "                        else:\n",
    "                            row['Grantham_Score_VCEP'] = 0\n",
    "                else:\n",
    "                    row['AA_Pos_KnownPorLP'] = 0\n",
    "\n",
    "\n",
    "                int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "                post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "\n",
    "                if 'PM5' in int_evidence_codes:\n",
    "                    int_evidence_codes.remove('PM5')\n",
    "                    post_evidence_codes.remove('PM5')\n",
    "\n",
    "\n",
    "                if row['AA_Pos_KnownPorLP'] >= 1 and int(row['Grantham_Score_VCEP']) >= int(row['Grantham_Score_gnomad']):\n",
    "                    if row['AA_Pos_KnownPorLP'] >= 2 and int(row['Grantham_Score_VCEP']) >= int(row['Grantham_Score_gnomad']):\n",
    "                        if 'PM1' not in int_evidence_codes:\n",
    "                            post_evidence_codes.append('PM5')\n",
    "                            int_evidence_codes.append('PM5')\n",
    "                    else:\n",
    "                        if 'PM1' not in int_evidence_codes:\n",
    "                            post_evidence_codes.append('PM5_Supporting')\n",
    "                            int_evidence_codes.append('PP')\n",
    "\n",
    "                gnomaddf.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "                gnomaddf.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    \n",
    "        return gnomaddf\n",
    "\n",
    "\n",
    "def update_TP53_BA_BScodes(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        allele_freq = row['Allele Frequency']\n",
    "\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "       \n",
    "        if allele_freq > 0.001 and 'BA1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BA1')\n",
    "            post_evidence_codes.append('BA1')\n",
    "            \n",
    "        elif 0.0003 <= allele_freq <= 0.001 and 'BS1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BS1')\n",
    "            post_evidence_codes.append('BS1')            \n",
    "\n",
    "        df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    return df\n",
    "\n",
    "\n",
    "def newcodesfortp53(df):\n",
    "    df = process_aabeforeandafterwithblosumandgranthamdf(df, colname='HGVSp', suffix='_gnomad')\n",
    "\n",
    "    TP53_VCEP['HGVSp'] = TP53_VCEP['Preferred Variant Title'].str.extract(r'\\((p\\..+?)\\)$')\n",
    "    TP53_VCEP['AA_Pos'] = TP53_VCEP['HGVSp'].str.extract(r'p\\.\\D+(\\d+)\\D')\n",
    "    df['AA_Pos'] = df['HGVSp'].str.extract(r'p\\.\\D+(\\d+)\\D')\n",
    "    \n",
    "    #PVS1: current set of P53 alleles that lead to Termination are predicted to undergo NMD in an exon in the biologically relevant transcript\n",
    "    #Using Intervar call and manual re-check\n",
    "\n",
    "    #PS1: Can only compare to variants asserted as pathogenic by the ClinGen TP53 VCEP\n",
    "    df = zero_out(df, 'PS1')\n",
    "    df = updateTP53_PS1code(df)\n",
    "   \n",
    "    #PS2: De novo status; given these alleles originate from gnomAD we do not know the true de novo status. Thus PS2=0 for everything.\n",
    "    df = zero_out(df, 'PS2')\n",
    "\n",
    "    #PS3: functional data; MAVE data incorporated\n",
    "\n",
    "    #PS4: Prevalence in cases vs. controls; given these alleles originate from gnomAD we do not know the true prevalence. Thus PS4=0 for everything.\n",
    "    df = zero_out(df, 'PS4')\n",
    "\n",
    "    #PM1: 'This rule can be applied to variants in hot spots (codons 175, 245, 248, 249, 273, 282) but not to variants within functional domains. Use transcript NM_000546.4. \n",
    "    #Also use rule for variants with >=10 somatic observations in cancerhotspots.org (v2).'    \n",
    "    df = zero_out(df, 'PM1')\n",
    "    df = process_PM1(df, cancerhotspots_snv)\n",
    "    \n",
    "    #PM2: allele needs to be absent from gnomAD but these are all gnomAD variants\n",
    "    df = zero_out(df, 'PM2')\n",
    "\n",
    "    #PM3: detected in trans with confirmed phasing; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'PM3')\n",
    "\n",
    "    #PM4: protein length changes; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'PM4')\n",
    "\n",
    "    #PM5\n",
    "    df = zero_out(df, 'PM5')\n",
    "    df = updateP53_PM5code(df)\n",
    "    \n",
    "    #PM6: de novo; unknown for gnomAD data\n",
    "    df = zero_out(df, 'PM6')\n",
    "\n",
    "    #PP1: Co-segregation; unknown for gnomAD data\n",
    "    df = zero_out(df, 'PP1')\n",
    "\n",
    "    #PP2: Missense in a gene with low rate of benign missense; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'PP2')\n",
    "\n",
    "    #PP3\n",
    "    df = zero_out(df, 'PP3')\n",
    "    df = zero_out(df, 'BP4')\n",
    "    df = update_PP3andBP4forp53(df)\n",
    "\n",
    "    #PP4: Family history; unknown for gnomAD alleles\n",
    "    df = zero_out(df, 'PP4')\n",
    "\n",
    "    #PP5: Reputable source; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'PP5')\n",
    "\n",
    "    #BA1 and #BS1\n",
    "    df = zero_out(df, 'BA1')\n",
    "    df = zero_out(df, 'BS1')\n",
    "\n",
    "    #BS2 Healthy adult data unknown\n",
    "    df = zero_out(df, 'BS2')\n",
    "\n",
    "    #BS3\n",
    "\n",
    "    #BS4: Segregation data unknown for gnomAD alleles\n",
    "    df = zero_out(df, 'BS4')\n",
    "\n",
    "    #BP1: missene variant in gene with known truncations; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'BP1')\n",
    "\n",
    "    #BP2: Observed in trans or cis with known path variant; unknown in gnomAD data; Not applicable\n",
    "    df = zero_out(df, 'BP2')\n",
    "\n",
    "    #BP3: In-frame deletions; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'BP3')\n",
    "\n",
    "    #BP4\n",
    "    #updated above with PP3\n",
    "\n",
    "    #BP5: Alternate molecular basis of disease; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'BP5')\n",
    "\n",
    "    #BP6: Reputable sources report as benign; Not applicable in TP53 criteria\n",
    "    df = zero_out(df, 'BP6')\n",
    "\n",
    "    #BP7: Synonymous variant; Not applicable NEED TO COME BACK HERE\n",
    "    #Using Intervar call and manual re-check\n",
    "    df = zero_out(df, 'BP7')\n",
    "    \n",
    "    #BA1 and BS1\n",
    "    df = update_TP53_BA_BScodes(df)\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d5d08",
   "metadata": {},
   "source": [
    "# Step 39: <a class=\"anchor\" id=\"step-39\"></a> TP53 Reclassifications\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077090d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP53_gnomadv2_withevidencecodes_withfunctionaldata = add_classifier_predictions(TP53_gnomadv2_withevidencecodes_pre_functionaldata, TP53assaydata_mutationsconverted)\n",
    "TP53_gnomadv2_newsignificance = accumulate_evidence(newcodesfortp53(update_functionalevidencecolumnsforp53(add_evidence_codes_column(concatenate_headers(TP53_gnomadv2_withevidencecodes_withfunctionaldata)))))\n",
    "TP53_gnomadv2_newsignificance['NewClinicalSignificance'] = process_dictionary_column_tp53(TP53_gnomadv2_newsignificance['EvidenceDictionary'])\n",
    "\n",
    "\n",
    "\n",
    "TP53_gnomadv3_withevidencecodes_withfunctionaldata = add_classifier_predictions(TP53_gnomadv3_withevidencecodes_pre_functionaldata, TP53assaydata_mutationsconverted)\n",
    "TP53_gnomadv3_newsignificance = accumulate_evidence(newcodesfortp53(update_functionalevidencecolumnsforp53(add_evidence_codes_column(concatenate_headers(TP53_gnomadv3_withevidencecodes_withfunctionaldata)))))\n",
    "TP53_gnomadv3_newsignificance['NewClinicalSignificance'] = process_dictionary_column_tp53(TP53_gnomadv3_newsignificance['EvidenceDictionary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP53_gnomadv2_newsignificance.to_csv('output/reclassified/TP53_gnomadv2_newsignificance.csv')\n",
    "TP53_gnomadv3_newsignificance.to_csv('output/reclassified/TP53_gnomadv3_newsignificance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TP53_fayer_mutationsconverted = convert_aamutations(TP53_fayer, 'Protein Consequence')\n",
    "#TP53_fayer_functionalevidence = TP53_fayer_mutationsconverted.merge(TP53_fayer, how=\"left\", left_on=\"HGVSp\", right_on=\"HGVSp\").dropna(subset=[\"HGVSp\"])\n",
    "\n",
    "TP53_fayer['Allele Frequency'] = np.nan\n",
    "TP53_fayer_withevidencecodes_withfunctionaldata = add_classifier_predictions(TP53_fayer, TP53assaydata_mutationsconverted)\n",
    "TP53_fayer_newsignificance = accumulate_evidence(update_TP53_BA_BScodes(updatedPM2forFayer(newcodesfortp53(update_functionalevidencecolumnsforp53(add_evidence_codes_column(concatenate_headers(TP53_fayer_withevidencecodes_withfunctionaldata)))))))\n",
    "TP53_fayer_newsignificance['NewClinicalSignificance'] = process_dictionary_column_tp53(TP53_fayer_newsignificance['EvidenceDictionary'])\n",
    "\n",
    "#TP53_fayer_newsignificance['Match'] = compare_columns_and_assign_match(TP53_fayer_newsignificance, 'ACMG_Rules_Reinterpretation', 'NewClinicalSignificance')\n",
    "#TP53_fayer_newsignificance.to_excel('reclassified/TP53_fayer_newsignificance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP53_fayer_newsignificance.to_csv('output/reclassified/TP53_Fayer_newsignificance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_columns_and_assign_match(df, col1, col2):\n",
    "    print(len(df[col1]))\n",
    "    print(len(df[col2]))\n",
    "    # Ensure column names are valid\n",
    "    if col1 not in df.columns or col2 not in df.columns:\n",
    "        raise ValueError(f\"One of the specified columns does not exist in the DataFrame: {col1} or {col2}\")\n",
    "        \n",
    "    # Initialize the 'Match' column with default values (False)\n",
    "    df['Match'] = False\n",
    "    \n",
    "    # Iterate over the DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        # Compare the two column values and assign the result to the 'Match' column\n",
    "        df.at[index, 'Match'] = row[col1] == row[col2]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 2, 4]})\n",
    "# result_df = compare_columns_and_assign_match(df, 'A', 'B')\n",
    "# print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6baae",
   "metadata": {},
   "source": [
    "# Step 40: <a class=\"anchor\" id=\"step-40\"></a> Updating PTEN evidence codes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98009d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PTEN_PM1(df1):\n",
    "    for idx, row in df1.iterrows():\n",
    "        aa_pos = row['AA_Pos_gnomad']\n",
    "        \n",
    "        if pd.notnull(aa_pos) and str(aa_pos).isdigit():\n",
    "            aa_pos = int(aa_pos)\n",
    "        \n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "                    \n",
    "        if aa_pos in [90, 91, 92, 93, 94, 123, 124, 125, 126, 127, 128, 129, 130, 166, 167, 168]:\n",
    "            if 'PM1' not in post_evidence_codes:\n",
    "                post_evidence_codes.append('PM1')\n",
    "                int_evidence_codes.append('PM1')\n",
    "        \n",
    "        df1.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        df1.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def updatePTEN_PS1code(gnomaddf):\n",
    "    # Import the CSV\n",
    "    df = pd.read_csv('inputs/PTEN_PS1_Variants.csv')\n",
    "    \n",
    "    df['AAPos'] = df['AAPos'].fillna(-1).astype(int)   \n",
    "    gnomaddf['AA_Pos_gnomad'] = gnomaddf['AA_Pos_gnomad'].fillna(0).astype(int)\n",
    "    \n",
    "    \n",
    "    #df = process_aabeforeandafterwithblosumandgranthamdf(df, colname='Preferred Variant Title', suffix='_VCEP')\n",
    "    \n",
    "    # Filter rows based on the Classification column\n",
    "    #df = df[df['Classification'].isin(['Pathogenic', 'Likely Pathogenic'])]\n",
    "    \n",
    "    # Mark rows where AA_Pos matches with the other dataframe\n",
    "    for idx, row in gnomaddf.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        if row['VEP Annotation'] == 'missense_variant':\n",
    "            filtered_df = df[(df['AAPos'] == row['AA_Pos_gnomad']) & (df['Before1AA'] == row['Before_AA_gnomad']) & (df['After1AA'] == row['After_AA_gnomad'])]\n",
    "            \n",
    "            if not filtered_df.empty:\n",
    "                if 'PS1' in int_evidence_codes:\n",
    "                    int_evidence_codes.remove('PS1')\n",
    "                    post_evidence_codes.remove('PS1')\n",
    "                    \n",
    "                if 'PS1' not in post_evidence_codes:\n",
    "                    int_evidence_codes.append('PS1')\n",
    "                    post_evidence_codes.append('PS1')\n",
    "\n",
    "        gnomaddf.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        gnomaddf.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    \n",
    "    return gnomaddf\n",
    "\n",
    "\n",
    "\n",
    "def updatePTEN_PM5code(gnomaddf):\n",
    "    # Import the CSV\n",
    "    df = pd.read_csv('inputs/ptenknownvcepvariants.csv')\n",
    "    df = process_aabeforeandafterwithblosumandgranthamdf(df, colname='Preferred Variant Title', suffix='_VCEP')\n",
    "    \n",
    "    # Filter rows based on the Classification column\n",
    "    df = df[df['Classification'].isin(['Pathogenic', 'Likely Pathogenic'])]\n",
    "    \n",
    "    df['AA_Pos_VCEP'] = df['AA_Pos_VCEP'].fillna(-1).astype(int)\n",
    "    \n",
    "    gnomaddf['AA_Pos_gnomad'] = gnomaddf['AA_Pos_gnomad'].fillna(0).astype(int)\n",
    "    \n",
    "    \n",
    "    # Mark rows where AA_Pos matches with the other dataframe\n",
    "    for idx, row in gnomaddf.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        if row['VEP Annotation'] == 'missense_variant':\n",
    "            if row['AA_Pos_gnomad'] in df['AA_Pos_VCEP'].values:\n",
    "                row['AA_Pos_KnownPorLP'] = 1\n",
    "                matching_row = df[df['AA_Pos_VCEP'] == row['AA_Pos_gnomad']]\n",
    "                if not matching_row.empty:\n",
    "                    if matching_row['BLOSUM62_Score_VCEP'].iloc[0] != None:\n",
    "                        row['BLOSUM62_Score_VCEP'] = matching_row['BLOSUM62_Score_VCEP'].iloc[0]\n",
    "                    else:\n",
    "                        row['BLOSUM62_Score_VCEP'] = 0\n",
    "            else:\n",
    "                row['AA_Pos_KnownPorLP'] = 0\n",
    "\n",
    "                \n",
    "            \n",
    "\n",
    "            if 'PM5' in int_evidence_codes:\n",
    "                int_evidence_codes.remove('PM5')\n",
    "                post_evidence_codes.remove('PM5')\n",
    "\n",
    "            if row['AA_Pos_KnownPorLP'] == 1 and row['BLOSUM62_Score_VCEP'] >= row['BLOSUM62_Score_gnomad']:\n",
    "                if 'PM5' not in post_evidence_codes:\n",
    "                    int_evidence_codes.append('PM5')\n",
    "                    post_evidence_codes.append('PM5')\n",
    "\n",
    "        gnomaddf.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        gnomaddf.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    \n",
    "    return gnomaddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_PTEN_BA_BS_PM2codes(df):\n",
    "    df['Allele Frequency'] = df['Allele Frequency'].fillna(-1).astype(float)\n",
    "    df['AlleleFreq'] = df['AlleleFreq'].fillna(-1).astype(float)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "                \n",
    "        try:\n",
    "            if row['Allele Frequency'] >= row['AlleleFreq']:\n",
    "                allele_freq = row['Allele Frequency']\n",
    "            else:\n",
    "                allele_freq = row['AlleleFreq']\n",
    "        except:\n",
    "            allele_freq = row['AlleleFreq']\n",
    "            \n",
    "        print(allele_freq)\n",
    "        \n",
    "        if pd.isna(allele_freq):\n",
    "            allele_freq = 0\n",
    "            \n",
    "        print(allele_freq)\n",
    "        \n",
    "        if allele_freq > 0.00056 and 'BA1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BA1')\n",
    "            post_evidence_codes.append('BA1')\n",
    "            \n",
    "        elif 0.000043 < allele_freq <= 0.00056 and 'BS1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BS1')\n",
    "            post_evidence_codes.append('BS1')\n",
    "            \n",
    "        elif 0.0000043 <= allele_freq <= 0.000043 and 'BS1' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('BP')\n",
    "            post_evidence_codes.append('BS1_Supporting')\n",
    "            \n",
    "        if allele_freq < 0.00001 and 'PM2' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('PP')\n",
    "            post_evidence_codes.append('PM2_Supporting')\n",
    "            \n",
    "        df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e51748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PTEN API for Revel scores\n",
    "\n",
    "\n",
    "def extract_values_pten(api_output):\n",
    "    # Ensure the input is a dictionary\n",
    "    if not isinstance(api_output, dict):\n",
    "        raise ValueError(\"Input should be a dictionary containing API output.\")\n",
    "\n",
    "    # Extract revel\n",
    "    revel_score = None\n",
    "    for transcript in api_output.get(\"transcript_consequences\", []):\n",
    "        if \"revel_score\" in transcript:\n",
    "            revel_score = float(transcript[\"revel_score\"])\n",
    "            break  # exit the loop once the value is found\n",
    "    \n",
    "    # Extract gnomad_exomes_popmax_af \n",
    "    gnomad_exomes_popmax_af = None\n",
    "    for transcript in api_output.get(\"transcript_consequences\", []):\n",
    "        if \"gnomad_exomes_popmax_af\" in transcript:\n",
    "            gnomad_exomes_popmax_af = float(transcript[\"gnomad_exomes_popmax_af\"])\n",
    "            break  # exit the loop once the value is found\n",
    "            \n",
    "    if gnomad_exomes_popmax_af == None:\n",
    "        gnomad_exomes_popmax_af = 0\n",
    "        \n",
    "    print(gnomad_exomes_popmax_af)\n",
    "    \n",
    "    # Extract gnomad_genomes_popmax_af \n",
    "    gnomad_genomes_popmax_af = None\n",
    "    for transcript in api_output.get(\"transcript_consequences\", []):\n",
    "        if \"gnomad_genomes_popmax_af\" in transcript:\n",
    "            gnomad_genomes_popmax_af = float(transcript[\"gnomad_genomes_popmax_af\"])\n",
    "            break  # exit the loop once the value is found\n",
    "            \n",
    "    if gnomad_genomes_popmax_af == None:\n",
    "        gnomad_genomes_popmax_af = 0\n",
    "\n",
    "    print(gnomad_genomes_popmax_af)\n",
    "        \n",
    "    gnomadaf = 0\n",
    "    if gnomad_exomes_popmax_af > gnomad_genomes_popmax_af:\n",
    "        gnomadaf = gnomad_exomes_popmax_af\n",
    "    else:\n",
    "        gnomadaf = gnomad_genomes_popmax_af\n",
    "        \n",
    "    print(gnomadaf)    \n",
    "\n",
    "    return revel_score, gnomadaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross 2 PTEN assays against 2 PTEN gnomad \n",
    "#get to new significance\n",
    "\n",
    "\n",
    "def add_classifier_predictionsPTEN(ptengnomaddf, abundancedf, activitydf):\n",
    "    merged_df = ptengnomaddf.merge(abundancedf, how=\"left\", left_on=\"Protein Consequence\", right_on=\"hgvs\")\n",
    "    print(len(merged_df))\n",
    "    activitydf['HGVSp'] = 'p.' + activitydf['Variant (three letter)']\n",
    "    filtered_activitydf = activitydf[activitydf['High_conf'] == 1]\n",
    "    bothmergeddf = merged_df.merge(filtered_activitydf, how=\"left\", left_on=\"Protein Consequence\", right_on=\"HGVSp\")#.dropna(subset=['Intervar'])\n",
    "    print(len(bothmergeddf))\n",
    "    finaldf = bothmergeddf.dropna(subset=['hgvs', 'HGVSp'], how='all') \n",
    "    print(len(finaldf))\n",
    "    \n",
    "    return finaldf\n",
    "\n",
    "\n",
    "# def update_functionalevidencecolumnsforPTEN(df):\n",
    "#     df[\"BSM3\"] = 0  # Initialize BSM3 column with zeros\n",
    "#     df[\"BSS3\"] = 0  # Initialize BSM3 column with zeros\n",
    "#     df['Cum_score'] = df['Cum_score'].astype(float)\n",
    "#     df.loc[df['abundance_class'].isin(['wt-like', 'possibly_wt-like']), 'BSS3'] = 1\n",
    "#     df.loc[df['Cum_score'] > -5, 'BSM3'] = 1\n",
    "#     # Filter out rows where both 'BSS3' and 'BSM3' are still zero\n",
    "#     df_filtered = df[(df['BSS3'] != 0) | (df['BSM3'] != 0)]\n",
    "    \n",
    "#     return df_filtered\n",
    "\n",
    "\n",
    "def update_functionalevidencecolumnsforPTEN(df):\n",
    "    df['Cum_score'] = df['Cum_score'].astype(float)\n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():       \n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if 'BS3' in post_evidence_codes:\n",
    "            int_evidence_codes.remove('BS3')\n",
    "            post_evidence_codes.remove('BS3')\n",
    "        \n",
    "        if 'PS3' in post_evidence_codes:\n",
    "            int_evidence_codes.remove('PS3')\n",
    "            post_evidence_codes.remove('PS3')\n",
    "        \n",
    "        if row['abundance_class'] in ['wt-like', 'possibly_wt-like']:    \n",
    "            int_evidence_codes.append('BP')\n",
    "            post_evidence_codes.append('BS3_Supporting')\n",
    "            \n",
    "        if row['Cum_score'] > 0:\n",
    "            if 'BS3_Supporting' not in post_evidence_codes:\n",
    "                int_evidence_codes.append('BP')\n",
    "                post_evidence_codes.append('BS3_Supporting')\n",
    "                \n",
    "        elif row['Cum_score'] <= -1.11:\n",
    "            if 'BS3_Supporting' in post_evidence_codes:\n",
    "                int_evidence_codes.remove('BP')\n",
    "                post_evidence_codes.remove('BS3_Supporting')\n",
    "                \n",
    "            if 'BS3_Moderate' in post_evidence_codes:\n",
    "                int_evidence_codes.remove('BM')\n",
    "                post_evidence_codes.remove('BS3_Moderate')\n",
    "                \n",
    "            int_evidence_codes.append('PM')\n",
    "            post_evidence_codes.append('PS3_Moderate')\n",
    "        \n",
    "    df.at[index, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    df.at[index, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_PTEN_PP2code(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        conseq = row['VEP Annotation']\n",
    "\n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "        \n",
    "        if conseq == 'missense_variant' and 'PP2' not in int_evidence_codes:\n",
    "            int_evidence_codes.append('PP2')\n",
    "            post_evidence_codes.append('PP2')\n",
    "            \n",
    "        df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_PTEN_BP4andPP3codes(df):\n",
    "    df['revelscore'] = ''\n",
    "    df['AlleleFreq'] = ''\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        conseq = row['VEP Annotation']\n",
    "        \n",
    "        int_evidence_codes = row['IntermediateGeneSpecificEvidenceCodes']\n",
    "        post_evidence_codes = row['PostGeneSpecificEvidenceCodes']\n",
    "\n",
    "        aa_pos = row['AA_Pos_gnomad']\n",
    "        variant = row['Transcript Consequence']\n",
    "        genetranscript= row['Transcript']\n",
    "        data = fetch_variant_dbnsfp_data(genetranscript, variant, species=\"homo_sapiens\", dbnsfp_fields=\"ALL\")\n",
    "        revelscore, gnomadaf = extract_values_pten(data[0])\n",
    "        \n",
    "        revelscoredf = pd.read_excel('inputs/PTENRevelScores.xlsx')\n",
    "        \n",
    "        if not isinstance(revelscore, float):\n",
    "            # Find the row in PTENRevelScores where the Transcript Consequence matches\n",
    "            matching_row = revelscoredf[revelscoredf['Transcript Consequence'] == row['Transcript Consequence']]\n",
    "        \n",
    "            # If a matching row is found, take the revelscore from that row\n",
    "            if not matching_row.empty:\n",
    "                revelscore = matching_row.iloc[0]['revelscore']\n",
    "                print(f\"Revelscore assigned from PTENRevelScores: {revelscore}\")\n",
    "            else:\n",
    "                print(\"No matching Transcript Consequence found in PTENRevelScores.\")\n",
    "        else:\n",
    "            print(\"Revelscore is already a float.\")\n",
    "        \n",
    "        \n",
    "        print('gnomad is '+str(gnomadaf))\n",
    "        print('REVEL is '+str(revelscore))\n",
    "        \n",
    "        df.at[idx, 'revelscore'] = revelscore\n",
    "        df.at[idx, 'AlleleFreq'] = gnomadaf\n",
    "        \n",
    "        if row['VEP Annotation'] == 'missense_variant':\n",
    "            if revelscore > 0.7 and 'PP3' not in int_evidence_codes:\n",
    "                int_evidence_codes.append('PP3')\n",
    "                post_evidence_codes.append('PP3')\n",
    "\n",
    "            elif revelscore < 0.5 and 'BP4' not in int_evidence_codes:\n",
    "                post_evidence_codes.append('BP4')\n",
    "                int_evidence_codes.append('BP4')\n",
    "            \n",
    "        df.at[idx, 'PostGeneSpecificEvidenceCodes'] = post_evidence_codes\n",
    "        df.at[idx, 'IntermediateGeneSpecificEvidenceCodes'] = int_evidence_codes\n",
    "    return df\n",
    "\n",
    "\n",
    "# Use the function:\n",
    "#revelv2 = 'opencravaoutputptenv2tovcfforensembl.vcf.variant.csv'\n",
    "#revelv3 = 'opencravatoutputptenv3tovcfforensembl.vcf.variant.csv'\n",
    "\n",
    "\n",
    "\n",
    "def process_and_merge_forptenrevel(reveldf, gnomadfunctionaldataframe):\n",
    "    # Read the CSVs\n",
    "    df1 = pd.read_csv(reveldf)\n",
    "    \n",
    "    # Helper function to replace 'chr' with ''\n",
    "    def newchrom(df):\n",
    "        df['new_chrom'] = df['chrom'].str.replace('chr', '')\n",
    "        return df\n",
    "\n",
    "    # Apply the newchrom function\n",
    "    df1 = newchrom(df1)\n",
    "\n",
    "    # Apply the concatenate function\n",
    "    df1['concatenatedpositiongnomad'] = concatenate_columns_as_string(df1, 'new_chrom', 'extra_vcf_info.pos', 'extra_vcf_info.ref', 'extra_vcf_info.alt')\n",
    "    gnomadfunctionaldataframe['concatenatedpositiongnomad'] = concatenate_columns_as_string(gnomadfunctionaldataframe, 'Chrom', 'Pos', 'Reference', 'Alternate')\n",
    "\n",
    "    # Merge with the other dataframe\n",
    "    merged_df1 = pd.merge(gnomadfunctionaldataframe, df1[['concatenatedpositiongnomad', 'revel.transcript', 'revel.score', 'revel.rankscore', 'revel.all']], on='concatenatedpositiongnomad', how='inner')\n",
    "    \n",
    "    return merged_df1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#write PTEN gene specific rules\n",
    "#redo new significance for PTEN\n",
    "\n",
    "def newcodesforpten(ptendf):\n",
    "    ptendf = process_aabeforeandafterwithblosumandgranthamdf(ptendf, colname='HGVSp', suffix='_gnomad')\n",
    "    #ptendf = process_and_merge_forptenrevel(reveldf, ptendf)\n",
    "\n",
    "    #PVS1: current set of PTEN alleles that lead to Termination are predicted to undergo NMD in an exon in the biologically relevant transcript\n",
    "    #set limits at the end\n",
    "    \n",
    "    #PS1: Can only compare to variants asserted as pathogenic by the ClinGen TP53 VCEP\n",
    "    ptendf = zero_out(ptendf, 'PS1')\n",
    "    ptendf = updatePTEN_PS1code(ptendf)\n",
    "    \n",
    "    #PS2: De novo status; given these alleles originate from gnomAD we do not know the true de novo status. Thus PS2=0 for everything.\n",
    "    ptendf = zero_out(ptendf, 'PS2')\n",
    "\n",
    "    #PS3: functional data; MAVE data incorporated\n",
    "\n",
    "    #PS4: Prevalence in cases vs. controls; given these alleles originate from gnomAD we do not know the true prevalence. Thus PS4=0 for everything.\n",
    "    ptendf = zero_out(ptendf, 'PS4')\n",
    "\n",
    "    #PM1: \n",
    "    #Using Intervar call and manual re-check\n",
    "    #Write function for this\n",
    "    ptendf = zero_out(ptendf, 'PM1')\n",
    "    ptendf = process_PTEN_PM1(ptendf)\n",
    "\n",
    "    #PM2: allele needs to be absent from gnomAD but these are all gnomAD variants\n",
    "    #with BA and BS code update\n",
    "    ptendf = zero_out(ptendf, 'PM2')\n",
    "    \n",
    "    #PM3: detected in trans with confirmed phasing; Not applicable in TP53 criteria\n",
    "    ptendf = zero_out(ptendf, 'PM3')\n",
    "\n",
    "    #PM4: protein length changes; Not applicable for gnomAD SNVs\n",
    "    ptendf = zero_out(ptendf, 'PM4')\n",
    "\n",
    "    #PM5\n",
    "    ptendf = zero_out(ptendf, 'PM5')\n",
    "    ptendf = updatePTEN_PM5code(ptendf)\n",
    "\n",
    "    #PM6: de novo; unknown for gnomAD data\n",
    "    ptendf = zero_out(ptendf, 'PM6')\n",
    "\n",
    "    #PP1: Co-segregation; unknown for gnomAD data\n",
    "    ptendf = zero_out(ptendf, 'PP1')\n",
    "\n",
    "    #PP2: Missense in a gene with low rate of benign missense; Not applicable in TP53 criteria\n",
    "    ptendf = zero_out(ptendf, 'PP2')\n",
    "    ptendf = update_PTEN_PP2code(ptendf)\n",
    "\n",
    "    #PP3\n",
    "    ptendf = zero_out(ptendf, 'PP3')\n",
    "    ptendf = zero_out(ptendf, 'BP4')\n",
    "    ptendf = update_PTEN_BP4andPP3codes(ptendf)\n",
    "\n",
    "    #PP4: Family history; unknown for gnomAD alleles; Not applicable in PTEN criteria\n",
    "    ptendf = zero_out(ptendf, 'PP4')\n",
    "\n",
    "    #PP5: Reputable source; Not applicable in PTEN criteria\n",
    "    ptendf = zero_out(ptendf, 'PP5')\n",
    "\n",
    "\n",
    "\n",
    "    #BA1 and BS1\n",
    "    ptendf = zero_out(ptendf, 'BA1')\n",
    "    ptendf = zero_out(ptendf, 'BS1')\n",
    "    \n",
    "    ptendf = update_PTEN_BA_BS_PM2codes(ptendf)\n",
    "\n",
    "    #BS2: healthy population; unknown data for gnomAD alleles\n",
    "    ptendf = zero_out(ptendf, 'BS2')\n",
    "\n",
    "    #BS3\n",
    "\n",
    "    #BS4: Segregation data unknown for gnomAD alleles\n",
    "    ptendf = zero_out(ptendf, 'BS4')\n",
    "\n",
    "    #BP1: missene variant in gene with known truncations\n",
    "    #Using Intervar call and manual re-check\n",
    "    ptendf = zero_out(ptendf, 'BP1')\n",
    "\n",
    "\n",
    "    #BP2: Observed in trans or cis with known path variant; unknown \n",
    "    ptendf = zero_out(ptendf, 'BP2')\n",
    "\n",
    "    #BP3: In-frame deletions; Not applicable in PTEN criteria\n",
    "    ptendf = zero_out(ptendf, 'BP3')\n",
    "\n",
    "    #BP4: in silico evidence\n",
    "\n",
    "    #BP5: Alternate molecular basis of disease; Unknown\n",
    "    ptendf = zero_out(ptendf, 'BP5')\n",
    "\n",
    "    #BP6: Reputable sources report as benign; Not applicable in PTEN criteria\n",
    "    ptendf = zero_out(ptendf, 'BP6')\n",
    "\n",
    "    #BP7: intronic variant; Unknown \n",
    "    ptendf = zero_out(ptendf, 'BP7')\n",
    "    \n",
    "    return ptendf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5223694",
   "metadata": {},
   "source": [
    "# Step 41: <a class=\"anchor\" id=\"step-41\"></a> PTEN Reclassifications\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTEN_gnomadv3_withevidencecodes_withfunctionaldata = add_classifier_predictionsPTEN(PTEN_gnomadv3_withevidencecodes_pre_functionaldata, PTENAbundanceassaydata, PTENActivityassaydata)\n",
    "PTEN_gnomadv3_newsignificance = accumulate_evidence(newcodesforpten(update_functionalevidencecolumnsforPTEN(add_evidence_codes_column(concatenate_headers(PTEN_gnomadv3_withevidencecodes_withfunctionaldata)))))\n",
    "PTEN_gnomadv3_newsignificance['NewClinicalSignificance'] = process_dictionary_column_pten(PTEN_gnomadv3_newsignificance['EvidenceDictionary'])\n",
    "\n",
    "PTEN_gnomadv2_withevidencecodes_withfunctionaldata = add_classifier_predictionsPTEN(PTEN_gnomadv2_withevidencecodes_pre_functionaldata, PTENAbundanceassaydata, PTENActivityassaydata)\n",
    "PTEN_gnomadv2_newsignificance = accumulate_evidence(newcodesforpten(update_functionalevidencecolumnsforPTEN(add_evidence_codes_column(concatenate_headers(PTEN_gnomadv2_withevidencecodes_withfunctionaldata)))))\n",
    "PTEN_gnomadv2_newsignificance['NewClinicalSignificance'] = process_dictionary_column_pten(PTEN_gnomadv2_newsignificance['EvidenceDictionary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca16621",
   "metadata": {},
   "outputs": [],
   "source": [
    "PTEN_gnomadv3_newsignificance.to_csv('output/reclassified/PTEN_gnomadv3_newsignificance.csv')\n",
    "PTEN_gnomadv2_newsignificance.to_csv('output/reclassified/PTEN_gnomadv2_newsignificance.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e35e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PTEN_Fayer_withevidencecodes_withfunctionaldata = add_classifier_predictionsPTEN(PTEN_fayer, PTENAbundanceassaydata, PTENActivityassaydata)\n",
    "PTEN_Fayer_newsignificance = accumulate_evidence(newcodesforpten(update_functionalevidencecolumnsforPTEN(add_evidence_codes_column(concatenate_headers(PTEN_Fayer_withevidencecodes_withfunctionaldata)))))\n",
    "PTEN_Fayer_newsignificance['NewClinicalSignificance'] = process_dictionary_column_pten(PTEN_Fayer_newsignificance['EvidenceDictionary'])\n",
    "\n",
    "#PTEN_Fayer_newsignificance.to_excel('reclassified/PTEN_Fayer_newsignificance.xlsx')\n",
    "PTEN_Fayer_newsignificance.to_csv('output/reclassified/PTEN_Fayer_newsignificance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc229981",
   "metadata": {},
   "source": [
    "# Part 7: <a class=\"anchor\" id=\"part-7\"></a>Sankey Flow Diagrams of VUS Reclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f68ce",
   "metadata": {},
   "source": [
    "# Step 42: <a class=\"anchor\" id=\"step-42\"></a> Functions for Sankey flow visuals of variant reclassification\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42154ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRCA1_gnomadv2_newsignificance = pd.read_csv('output/reclassified/BRCA1_gnomadv2_newsignificance.csv')\n",
    "BRCA1_gnomadv2_newsignificance['NameOfGene'] = 'BRCA1'\n",
    "BRCA1_gnomadv2_newsignificance['ReferenceGenome'] = 'GRCh37'\n",
    "\n",
    "BRCA1_gnomadv3_newsignificance = pd.read_csv('output/reclassified/BRCA1_gnomadv3_newsignificance.csv') \n",
    "BRCA1_gnomadv3_newsignificance['NameOfGene'] = 'BRCA1'\n",
    "BRCA1_gnomadv3_newsignificance['ReferenceGenome'] = 'GRCh38'\n",
    "\n",
    "BRCA1_AoU_newsignificance = pd.read_csv('inputs/BRCA1_AoU_newsignificance.csv')\n",
    "BRCA1_AoU_newsignificance['NameOfGene'] = 'BRCA1'\n",
    "BRCA1_AoU_newsignificance['ReferenceGenome'] = 'GRCh38'\n",
    "\n",
    "\n",
    "\n",
    "TP53_gnomadv2_newsignificance = pd.read_csv('output/reclassified/TP53_gnomadv2_newsignificance.csv')\n",
    "TP53_gnomadv2_newsignificance['NameOfGene'] = 'TP53'\n",
    "TP53_gnomadv2_newsignificance['ReferenceGenome'] = 'GRCh37'\n",
    "\n",
    "TP53_gnomadv3_newsignificance = pd.read_csv('output/reclassified/TP53_gnomadv3_newsignificance.csv')\n",
    "TP53_gnomadv3_newsignificance['NameOfGene'] = 'TP53'\n",
    "TP53_gnomadv3_newsignificance['ReferenceGenome'] = 'GRCh38'\n",
    "\n",
    "TP53_AoU_newsignificance = pd.read_csv('inputs/TP53_AoU_newsignificance.csv')\n",
    "TP53_AoU_newsignificance['NameOfGene'] = 'TP53'\n",
    "TP53_AoU_newsignificance['ReferenceGenome'] = 'GRCh38'\n",
    "\n",
    "\n",
    "\n",
    "PTEN_gnomadv2_newsignificance = pd.read_csv('output/reclassified/PTEN_gnomadv2_newsignificance.csv')\n",
    "PTEN_gnomadv2_newsignificance['NameOfGene'] = 'PTEN'\n",
    "PTEN_gnomadv2_newsignificance['ReferenceGenome'] = 'GRCh37'\n",
    "\n",
    "PTEN_gnomadv3_newsignificance = pd.read_csv('output/reclassified/PTEN_gnomadv3_newsignificance.csv')\n",
    "PTEN_gnomadv3_newsignificance['NameOfGene'] = 'PTEN'\n",
    "PTEN_gnomadv3_newsignificance['ReferenceGenome'] = 'GRCh38'\n",
    "\n",
    "PTEN_AoU_newsignificance = pd.read_csv('inputs/PTEN_AoU_newsignificance.csv')\n",
    "PTEN_AoU_newsignificance['NameOfGene'] = 'PTEN'\n",
    "PTEN_AoU_newsignificance['ReferenceGenome'] = 'GRCh38'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29668609",
   "metadata": {},
   "outputs": [],
   "source": [
    "popdescrips2 = ['African/African American', 'Latino/Admixed American', 'East Asian', 'South Asian', 'Other']\n",
    "popdescrips1 = ['European (non-Finnish)']\n",
    "\n",
    "#df['Variant in Euro'] = (df[[f'Allele Count {ancestry}' for ancestry in popdescrips1]].sum(axis=1) > 0).astype(int)\n",
    "#df['Variant in nonEuro'] = (df[[f'Allele Count {ancestry}' for ancestry in popdescrips2]].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "\n",
    "#Fayer datasets\n",
    "BRCA1_Fayer_newsignificance = pd.read_csv('output/reclassified/BRCA1_Fayer_newsignificance.csv')\n",
    "BRCA1_Fayer_newsignificance['NameOfGene'] = 'BRCA1'\n",
    "BRCA1_Fayer_newsignificance['ReferenceGenome'] = 'GRCh37'\n",
    "#BRCA1_Fayer_newsignificance['Variant in Euro'] = (BRCA1_Fayer_newsignificance[[f'Allele Count {ancestry}' for ancestry in popdescrips1]].sum(axis=1) > 0).astype(int)\n",
    "#BRCA1_Fayer_newsignificance['Variant in nonEuro'] = (BRCA1_Fayer_newsignificance[[f'Allele Count {ancestry}' for ancestry in popdescrips2]].sum(axis=1) > 0).astype(int)\n",
    "BRCA1_Fayer_newsignificance['Variant in Euro'] = sum([BRCA1_Fayer_newsignificance[f'Allele Count {ancestry}'] for ancestry in popdescrips1])\n",
    "BRCA1_Fayer_newsignificance['Variant in nonEuro'] = sum([BRCA1_Fayer_newsignificance[f'Allele Count {ancestry}'] for ancestry in popdescrips2])\n",
    "\n",
    "\n",
    "TP53_Fayer_newsignificance = pd.read_csv('output/reclassified/TP53_Fayer_newsignificance.csv')\n",
    "TP53_Fayer_newsignificance['NameOfGene'] = 'TP53'\n",
    "TP53_Fayer_newsignificance['ReferenceGenome'] = 'GRCh37'\n",
    "#TP53_Fayer_newsignificance['Variant in Euro'] = (TP53_Fayer_newsignificance[[f'Allele Count {ancestry}' for ancestry in popdescrips1]].sum(axis=1) > 0).astype(int)\n",
    "#TP53_Fayer_newsignificance['Variant in nonEuro'] = (TP53_Fayer_newsignificance[[f'Allele Count {ancestry}' for ancestry in popdescrips2]].sum(axis=1) > 0).astype(int)\n",
    "TP53_Fayer_newsignificance['Variant in Euro'] = sum([TP53_Fayer_newsignificance[f'Allele Count {ancestry}'] for ancestry in popdescrips1])\n",
    "TP53_Fayer_newsignificance['Variant in nonEuro'] = sum([TP53_Fayer_newsignificance[f'Allele Count {ancestry}'] for ancestry in popdescrips2])\n",
    "\n",
    "\n",
    "PTEN_Fayer_newsignificance = pd.read_csv('output/reclassified/PTEN_Fayer_newsignificance.csv')\n",
    "PTEN_Fayer_newsignificance['NameOfGene'] = 'PTEN'\n",
    "PTEN_Fayer_newsignificance['ReferenceGenome'] = 'GRCh37'\n",
    "#PTEN_Fayer_newsignificance['Variant in Euro'] = (PTEN_Fayer_newsignificance[[f'Allele Count {ancestry}' for ancestry in popdescrips1]].sum(axis=1) > 0).astype(int)\n",
    "#PTEN_Fayer_newsignificance['Variant in nonEuro'] = (PTEN_Fayer_newsignificance[[f'Allele Count {ancestry}' for ancestry in popdescrips2]].sum(axis=1) > 0).astype(int)\n",
    "PTEN_Fayer_newsignificance['Variant in Euro'] = sum([PTEN_Fayer_newsignificance[f'Allele Count {ancestry}'] for ancestry in popdescrips1])\n",
    "PTEN_Fayer_newsignificance['Variant in nonEuro'] = sum([PTEN_Fayer_newsignificance[f'Allele Count {ancestry}'] for ancestry in popdescrips2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sankeyoutputdir = './output/sankeyoutputs'\n",
    "\n",
    "\n",
    "def twolevelsankey(df, sourcecolumn, targetcolumn, sumcolumn, titletext, color, inputname, output_dir, show_node_labels=True):\n",
    "    # Data preparation and processing\n",
    "    df['Allele Count Non-European'] = sum([df[f'Allele Count {ancestry}'] for ancestry in ['African/African American', 'Latino/Admixed American', 'East Asian', 'Other', 'South Asian']])\n",
    "    \n",
    "    \n",
    "    df_demo = df[df[sumcolumn] != 0]\n",
    "    df_vus = df_demo[df_demo[sourcecolumn] == 'Uncertain significance']\n",
    "    df_groupedforsankey = df_vus.groupby([sourcecolumn, targetcolumn])[sumcolumn].sum().reset_index()\n",
    "        \n",
    "    df_groupedforsankey.columns = ['source', 'target', 'value']\n",
    "    \n",
    "    unique_source_target = list(pd.unique(df_groupedforsankey[['source', 'target']].values.ravel(\"K\")))   \n",
    "    \n",
    "    mapping_dict = {k: v for v, k in enumerate(unique_source_target)}\n",
    "    df_groupedforsankey['source'] = df_groupedforsankey['source'].map(mapping_dict)\n",
    "    df_groupedforsankey['target'] = df_groupedforsankey['target'].map(mapping_dict)\n",
    "    links_dict = df_groupedforsankey.to_dict(orient='list')\n",
    "    \n",
    "    total_source_values = sum(links_dict['value'])\n",
    "    total_target_values = sum(links_dict['value'])\n",
    "    \n",
    "    node_labels = []\n",
    "    for label in unique_source_target:\n",
    "        source_idx = mapping_dict[label]\n",
    "        target_idx = mapping_dict[label]\n",
    "        \n",
    "        source_total_value = sum(links_dict['value'][idx] for idx in range(len(links_dict['source'])) if links_dict['source'][idx] == source_idx)\n",
    "        target_total_value = sum(links_dict['value'][idx] for idx in range(len(links_dict['target'])) if links_dict['target'][idx] == target_idx)\n",
    "        \n",
    "        source_percentage = (source_total_value / total_source_values) * 100\n",
    "        target_percentage = (target_total_value / total_target_values) * 100\n",
    "        \n",
    "        label_text = label\n",
    "        \n",
    "        if source_total_value > 0:\n",
    "            label_text += f\" ({int(source_total_value)}; {source_percentage:.2f}%)\"\n",
    "        \n",
    "        if target_total_value > 0:\n",
    "            label_text += f\" ({int(target_total_value)}; {target_percentage:.2f}%)\"\n",
    "        \n",
    "        node_labels.append(label_text)\n",
    "    \n",
    "    # Define custom colors for nodes based on their labels\n",
    "    node_colors = []\n",
    "    for label in node_labels:\n",
    "        label_lower = label.lower().strip()  # Convert label to lowercase and strip whitespace\n",
    "        \n",
    "        if 'uncertain significance' in label_lower or 'variant of uncertain significance' in label_lower:\n",
    "            node_colors.append('#808080')\n",
    "        elif 'likely benign' in label_lower:\n",
    "            node_colors.append('#45b6fe')  # Lighter shade of blue\n",
    "        elif 'benign' in label_lower:\n",
    "            node_colors.append('#0000FF')\n",
    "        elif 'likely pathogenic' in label_lower:\n",
    "            node_colors.append('#ff0000')\n",
    "        elif 'pathogenic' in label_lower:\n",
    "            node_colors.append('#8b0000')\n",
    "   \n",
    "    \n",
    "    # Define custom colors for links based on their source and target nodes\n",
    "    link_colors = []\n",
    "    for source, target in zip(links_dict['source'], links_dict['target']):\n",
    "        source_label = unique_source_target[source]\n",
    "        target_label = unique_source_target[target]\n",
    "        \n",
    "        if source_label == 'Uncertain significance' and target_label == 'Variant of Uncertain Significance':\n",
    "            link_colors.append('#d3d3d3')\n",
    "        elif source_label == 'Uncertain significance' and target_label == 'Likely Benign':\n",
    "            link_colors.append('#a8ddff')  # Lighter shade of blue\n",
    "        elif source_label == 'Uncertain significance' and target_label == 'Likely Pathogenic':\n",
    "            link_colors.append('#f08080')\n",
    "        elif source_label == 'Uncertain significance' and target_label == 'Pathogenic':\n",
    "            link_colors.append('#d80000')\n",
    "        elif source_label == 'Uncertain significance' and target_label == 'Benign':\n",
    "            link_colors.append('#0080ff')  # Default color for other links\n",
    "            \n",
    "    node_labels = node_labels if show_node_labels else [''] * len(node_labels)\n",
    "\n",
    "    # Creating the Sankey diagram with enhanced styling\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=60,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            label=node_labels,\n",
    "            color=node_colors,\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=links_dict['source'],\n",
    "            target=links_dict['target'],\n",
    "            value=links_dict['value'],\n",
    "            color=link_colors  # Set custom link colors\n",
    "        ),\n",
    "        orientation='h'  # Set orientation to vertical\n",
    "    )])\n",
    "    \n",
    "\n",
    "    # Updating layout and title with enhanced styling\n",
    "    fig.update_layout(\n",
    "        title_text=titletext, \n",
    "        font_size=15,\n",
    "        font_family=\"Arial, sans-serif\",\n",
    "        title_font=dict(size=30, family=\"Arial, sans-serif\")\n",
    "    )\n",
    "    \n",
    "    # Show the Sankey diagram\n",
    "    fig.show()\n",
    "    \n",
    "    label_for_filename = \"nodelabelstrue\" if show_node_labels else \"nodelabelsfalse\"\n",
    "    \n",
    "    # Save the Sankey diagram as a PNG file\n",
    "    titletext_cleaned = titletext.replace(' ', '_').replace('/', '_')\n",
    "    filename = f\"{inputname}_{titletext_cleaned}_{label_for_filename}.pdf\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    fig.write_image(filepath, format='pdf')\n",
    "    print(node_labels)\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec266e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twolevelsankeyaou(df, sourcecolumn, targetcolumn, sumcolumn, titletext, color, inputname, output_dir, show_node_labels=True, needtocalculatenoneuroac=True):\n",
    "    # Data preparation and processing\n",
    "    replace_dict = {\"Variant of Uncertain Significance\": 'VUS'}\n",
    "    df[sourcecolumn] = df[sourcecolumn].replace(replace_dict)\n",
    "    \n",
    "    if needtocalculatenoneuroac==True:\n",
    "        df['Allele Count Non-European'] = sum([df[f'gvs_{ancestry}_ac'] for ancestry in ['afr','oth','amr','sas','eas','mid']])\n",
    "        \n",
    "    df_demo = df[df[sumcolumn] != 0]\n",
    "    df_vus = df_demo[df_demo[sourcecolumn] == 'VUS']\n",
    "    \n",
    "    global df_groupedforsankey\n",
    "    \n",
    "    df_groupedforsankey = df_vus.groupby([sourcecolumn, targetcolumn])[sumcolumn].sum().reset_index()\n",
    "    \n",
    "    df_groupedforsankey.columns = ['source', 'target', 'value']\n",
    "    \n",
    "    df_groupedforsankey['source'] = pd.Categorical(df_groupedforsankey['source'], ['Benign','Likely Benign', 'VUS', 'Likely Pathogenic', 'Pathogenic'])\n",
    "    df_groupedforsankey['target'] = pd.Categorical(df_groupedforsankey['target'], ['Benign','Likely Benign', 'Variant of Uncertain Significance', 'Likely Pathogenic', 'Pathogenic'])\n",
    "    df_groupedforsankey.sort_values(['source', 'target'], inplace = True)\n",
    "    df_groupedforsankey.reset_index(drop=True)\n",
    "    \n",
    "    global dfforbarchart\n",
    "    dfforbarchart = df_groupedforsankey.copy()\n",
    "    \n",
    "    global unique_source_target\n",
    "    unique_source_target = list(pd.unique(df_groupedforsankey[['source', 'target']].values.ravel(\"K\")))   \n",
    "\n",
    "    global mapping_dict\n",
    "    mapping_dict = {k: v for v, k in enumerate(unique_source_target)}\n",
    "    df_groupedforsankey['source'] = df_groupedforsankey['source'].map(mapping_dict)\n",
    "    df_groupedforsankey['target'] = df_groupedforsankey['target'].map(mapping_dict)\n",
    "    df_groupedforsankey['source'] = df_groupedforsankey['source'].astype(int)\n",
    "    df_groupedforsankey['target'] = df_groupedforsankey['target'].astype(int)\n",
    "\n",
    "    \n",
    "    global links_dict\n",
    "    links_dict = df_groupedforsankey.to_dict(orient='list')\n",
    "    \n",
    "    total_source_values = sum(links_dict['value'])\n",
    "    total_target_values = sum(links_dict['value'])\n",
    "    \n",
    "    node_labels = []\n",
    "    for label in unique_source_target:\n",
    "        source_idx = mapping_dict[label]\n",
    "        target_idx = mapping_dict[label]\n",
    "        \n",
    "        source_total_value = sum(links_dict['value'][idx] for idx in range(len(links_dict['source'])) if links_dict['source'][idx] == source_idx)\n",
    "        target_total_value = sum(links_dict['value'][idx] for idx in range(len(links_dict['target'])) if links_dict['target'][idx] == target_idx)\n",
    "        \n",
    "        source_percentage = (source_total_value / total_source_values) * 100\n",
    "        target_percentage = (target_total_value / total_target_values) * 100\n",
    "        \n",
    "        label_text = label\n",
    "        \n",
    "        if source_total_value > 0:\n",
    "            label_text += f\" ({int(source_total_value)}; {source_percentage:.2f}%)\"\n",
    "        \n",
    "        if target_total_value > 0:\n",
    "            label_text += f\" ({int(target_total_value)}; {target_percentage:.2f}%)\"\n",
    "        \n",
    "        node_labels.append(label_text)\n",
    "    \n",
    "    # Define custom colors for nodes based on their labels\n",
    "    node_colors = []\n",
    "    for label in node_labels:\n",
    "        label_lower = label.lower().strip()  # Convert label to lowercase and strip whitespace\n",
    "        \n",
    "        if 'VUS' in label_lower or 'vus' in label_lower or 'variant of uncertain significance' in label_lower:\n",
    "            node_colors.append('#808080')\n",
    "        elif 'likely benign' in label_lower:\n",
    "            node_colors.append('#45b6fe')  # Lighter shade of blue\n",
    "        elif 'likely pathogenic' in label_lower:\n",
    "            node_colors.append('#ff0000')\n",
    "        elif 'pathogenic' in label_lower:\n",
    "            node_colors.append('#8b0000')\n",
    "        elif 'benign' in label_lower:\n",
    "            node_colors.append('#0000FF')\n",
    "\n",
    "    \n",
    "    # Define custom colors for links based on their source and target nodes\n",
    "    link_colors = []\n",
    "    for source, target in zip(links_dict['source'], links_dict['target']):\n",
    "        source_label = unique_source_target[source]\n",
    "        target_label = unique_source_target[target]\n",
    "        \n",
    "        if source_label == 'VUS' and target_label == 'Variant of Uncertain Significance':\n",
    "            link_colors.append('#d3d3d3')\n",
    "        elif source_label == 'VUS' and target_label == 'Likely Benign':\n",
    "            link_colors.append('#a8ddff')  # Lighter shade of blue\n",
    "        elif source_label == 'VUS' and target_label == 'Likely Pathogenic':\n",
    "            link_colors.append('#f08080')\n",
    "        elif source_label == 'VUS' and target_label == 'Pathogenic':\n",
    "            link_colors.append('#d80000')\n",
    "        elif source_label == 'VUS' and target_label == 'Benign':\n",
    "            link_colors.append('#0080ff')  # Default color for other links\n",
    "\n",
    "            \n",
    "    node_labels = node_labels if show_node_labels else [''] * len(node_labels)\n",
    "\n",
    "    \n",
    "\n",
    "    # Creating the Sankey diagram with enhanced styling\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        arrangement = 'perpendicular',\n",
    "        node=dict(\n",
    "            pad=20,\n",
    "            thickness=60,\n",
    "            #line=dict(color='black', width=0.5),\n",
    "            label=node_labels,\n",
    "            color=node_colors,\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=links_dict['source'],\n",
    "            target=links_dict['target'],\n",
    "            value=links_dict['value'],\n",
    "            color=link_colors  # Set custom link colors\n",
    "        ),\n",
    "        orientation='h'  # Set orientation to vertical\n",
    "    )])\n",
    "    \n",
    "\n",
    "    # Updating layout and title with enhanced styling\n",
    "    fig.update_layout(\n",
    "        title_text=titletext, \n",
    "        font_size=15,\n",
    "        font_family=\"Arial, sans-serif\",\n",
    "        title_font=dict(size=30, family=\"Arial, sans-serif\")\n",
    "    )\n",
    "    \n",
    "    # Show the Sankey diagram\n",
    "    fig.show()\n",
    "    \n",
    "    label_for_filename = \"nodelabelstrue\" if show_node_labels else \"nodelabelsfalse\"\n",
    "    \n",
    "    # Save the Sankey diagram as a PNG file\n",
    "    titletext_cleaned = titletext.replace(' ', '_').replace('/', '_')\n",
    "    filename = f\"{inputname}_{titletext_cleaned}_{label_for_filename}.pdf\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    fig.write_image(filepath, format='pdf')\n",
    "    print(node_labels)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aec541",
   "metadata": {},
   "source": [
    "# Step 43: <a class=\"anchor\" id=\"step-43\"></a> Sankey Flow Visuals for BRCA1\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(BRCA1_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'BRCA1_gnomadv2', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(BRCA1_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'BRCA1_gnomadv2', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(BRCA1_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'BRCA1_gnomadv2', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(BRCA1_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'BRCA1_gnomadv2', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(BRCA1_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'BRCA1_gnomadv3', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(BRCA1_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'BRCA1_gnomadv3', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893871be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(BRCA1_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'BRCA1_gnomadv3', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(BRCA1_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'BRCA1_gnomadv3', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(BRCA1_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'BRCA1_AoU', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankeyaou(BRCA1_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'BRCA1_AoU', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435711ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(BRCA1_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'BRCA1_AoU', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankeyaou(BRCA1_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'BRCA1_AoU', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(BRCA1_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'BRCA1_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(BRCA1_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'BRCA1_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd67e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(BRCA1_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'BRCA1_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(BRCA1_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'BRCA1_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1e89f",
   "metadata": {},
   "source": [
    "# Step 44: <a class=\"anchor\" id=\"step-44\"></a> Sankey Flow Visuals for TP53\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6007d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(TP53_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'TP53_gnomadv2', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(TP53_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'TP53_gnomadv2', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd84bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(TP53_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'TP53_gnomadv2', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(TP53_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'TP53_gnomadv2', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(TP53_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'TP53_gnomadv3', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(TP53_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'TP53_gnomadv3', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(TP53_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'TP53_gnomadv3', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(TP53_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'TP53_gnomadv3', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(TP53_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'TP53_AoU', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankeyaou(TP53_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'TP53_AoU', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(TP53_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'TP53_AoU', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankeyaou(TP53_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'TP53_AoU', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(TP53_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'TP53_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(TP53_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'TP53_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(TP53_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'TP53_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(TP53_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'TP53_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4d776",
   "metadata": {},
   "source": [
    "# Step 45: <a class=\"anchor\" id=\"step-45\"></a> Sankey Flow Visuals for PTEN\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(PTEN_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'PTEN_gnomadv2', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(PTEN_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'PTEN_gnomadv2', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2363a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(PTEN_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'PTEN_gnomadv2', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(PTEN_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'PTEN_gnomadv2', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1016cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(PTEN_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'PTEN_gnomadv3', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(PTEN_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'PTEN_gnomadv3', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d91805",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(PTEN_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'PTEN_gnomadv3', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(PTEN_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'PTEN_gnomadv3', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12420920",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(PTEN_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'PTEN_AoU', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankeyaou(PTEN_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'PTEN_AoU', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163afa1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "twolevelsankeyaou(PTEN_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'PTEN_AoU', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankeyaou(PTEN_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'PTEN_AoU', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(PTEN_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'PTEN_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(PTEN_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'PTEN_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e681c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(PTEN_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'PTEN_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(PTEN_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'PTEN_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb37f51",
   "metadata": {},
   "source": [
    "# Part 8: <a class=\"anchor\" id=\"part-8\"></a>Overall VUS Reclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07ba19",
   "metadata": {},
   "source": [
    "# Step 46: <a class=\"anchor\" id=\"step-46\"></a> Functions for Combining Variant Dataframes for Sankey Flow Visuals\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sankey_dataframe(df, columns_to_keep, inputterm):\n",
    "    \"\"\"\n",
    "    Filter a DataFrame to retain specified columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame to filter.\n",
    "        columns_to_keep (list): A list of column names to retain.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with only the specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    if inputterm == 'gnomad':\n",
    "        allelecountcols = [col for col in df.columns if col.startswith('Allele Count')]\n",
    "    elif inputterm == 'AoU':\n",
    "        allelecountcols = [col for col in df.columns if col.startswith('gvs')]\n",
    "    \n",
    "    finalcols = columns_to_keep + allelecountcols\n",
    "    filtered_df = df[finalcols]\n",
    "    return filtered_df\n",
    "\n",
    "# Example usage:\n",
    "# Define the list of columns to keep\n",
    "columns_to_keep = [\n",
    "    'Original ClinVar Clinical Significance',\n",
    "    'ClinVar Clinical Significance',\n",
    "    'NewClinicalSignificance',\n",
    "    'IntermediateGeneSpecificEvidenceCodes',\n",
    "    'PostGeneSpecificEvidenceCodes',\n",
    "    'NameOfGene',\n",
    "    'ReferenceGenome',\n",
    "    'Chrom',\n",
    "    'Pos',\n",
    "    'Reference',\n",
    "    'Alternate',\n",
    "    'Transcript',\n",
    "    'Transcript Consequence',\n",
    "    'VEP Annotation',\n",
    "    'Intervar'\n",
    "]\n",
    "\n",
    "\n",
    "columns_to_keep_aou = [\n",
    "    'Original ClinVar Clinical Significance',\n",
    "    'ClinVar Clinical Significance',\n",
    "    'NewClinicalSignificance',\n",
    "    'IntermediateGeneSpecificEvidenceCodes',\n",
    "    'PostGeneSpecificEvidenceCodes',\n",
    "    'NameOfGene',\n",
    "    'ReferenceGenome',\n",
    "    'vid',\n",
    "    'contig',\n",
    "    'Pos',\n",
    "    'ref_allele',\n",
    "    'alt_allele',\n",
    "    'transcript',\n",
    "    'gene_symbol',\n",
    "    'aa_change',\n",
    "    'dna_change_in_transcript',\n",
    "    'genomic_location',\n",
    "    'consequence',\n",
    "    'Intervar'\n",
    "]\n",
    "\n",
    "\n",
    "columns_to_keep_fayer = [\n",
    "    'AmbryOriginalclassification',\n",
    "    'FayerIntermediateEvidenceCodes',\n",
    "    'FayerCombinedPostEvidenceCodes',\n",
    "    'FayerReclassification',\n",
    "    'NewClinicalSignificance',\n",
    "    'IntermediateGeneSpecificEvidenceCodes',\n",
    "    'PostGeneSpecificEvidenceCodes',\n",
    "    'NameOfGene',\n",
    "    'ReferenceGenome',\n",
    "    'Transcript Consequence',\n",
    "    'VEP Annotation',\n",
    "    'Variant in nonEuro',\n",
    "    'Variant in Euro'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_rename_columns = {\n",
    "    'consequence_x':'consequence'\n",
    "}\n",
    "\n",
    "BRCA1_AoU_newsignificance = BRCA1_AoU_newsignificance.rename(columns=mini_rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_rename_columns = {\n",
    "    'ref_allele': 'Reference',\n",
    "    'alt_allele': 'Alternate',\n",
    "    'transcript': 'Transcript',\n",
    "    'dna_change_in_transcript': 'Transcript Consequence',\n",
    "    'consequence': 'VEP Annotation',\n",
    "    'gvs_afr_ac':'Allele Count African/African American',\n",
    "    'gvs_amr_ac':'Allele Count Latino/Admixed American',\n",
    "    'gvs_eas_ac':'Allele Count East Asian',\n",
    "    'gvs_eur_ac':'Allele Count European (non-Finnish)',\n",
    "    'gvs_oth_ac':'Allele Count Other',\n",
    "    'gvs_sas_ac':'Allele Count South Asian',\n",
    "    'gvs_mid_ac':'Allele Count Middle Eastern',\n",
    "    'gvs_all_ac':'Allele Count'\n",
    "}\n",
    "\n",
    "BRCA1_AoU_newcols = BRCA1_AoU_newsignificance.rename(columns=aou_rename_columns)\n",
    "PTEN_AoU_newcols = PTEN_AoU_newsignificance.rename(columns=aou_rename_columns)\n",
    "TP53_AoU_newcols = TP53_AoU_newsignificance.rename(columns=aou_rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_string_in_column(df, column_name, search_string):\n",
    "    \"\"\"\n",
    "    Find occurrences of a search string in a specific column of a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The pandas DataFrame to search.\n",
    "        column_name (str): The name of the column to search.\n",
    "        search_string (str): The string to search for.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list containing strings where the search string is found.\n",
    "    \"\"\"\n",
    "    found_strings = []\n",
    "    for index, row in df.iterrows():\n",
    "        if search_string in str(row[column_name]):\n",
    "            found_strings.append(row[column_name])\n",
    "    return found_strings\n",
    "\n",
    "\n",
    "#find_string_in_column(combinedvusstartandreclass, 'PostGeneSpecificEvidenceCodes', 'BP1_Strong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRCA1_AoU_newcols['PostGeneSpecificEvidenceCodes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa728bd",
   "metadata": {},
   "source": [
    "# Step 47: <a class=\"anchor\" id=\"step-47\"></a> Combined Sankey Flow Visuals for Each Gene\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    BRCA1_gnomadv2_newsignificance,\n",
    "    BRCA1_gnomadv3_newsignificance,\n",
    "    BRCA1_AoU_newcols\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedBRCA1_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ebdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedBRCA1_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedBRCA1', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedBRCA1_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedBRCA1', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29603a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedBRCA1_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedBRCA1', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedBRCA1_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedBRCA1', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c203986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    PTEN_gnomadv2_newsignificance,\n",
    "    PTEN_gnomadv3_newsignificance,\n",
    "    PTEN_AoU_newcols\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedPTEN_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedPTEN_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedPTEN', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedPTEN_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedPTEN', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b295fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedPTEN_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedPTEN', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedPTEN_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedPTEN', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    TP53_gnomadv2_newsignificance,\n",
    "    TP53_gnomadv3_newsignificance,\n",
    "    TP53_AoU_newcols\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedTP53_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cef27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedTP53_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedTP53', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedTP53_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedTP53', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedTP53_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedTP53', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedTP53_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedTP53', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd9e9b",
   "metadata": {},
   "source": [
    "# Step 48: <a class=\"anchor\" id=\"step-48\"></a> Combined Sankey Flow Visuals for Each Database\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db402048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    BRCA1_Fayer_newsignificance,\n",
    "    PTEN_Fayer_newsignificance,\n",
    "    TP53_Fayer_newsignificance,\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep_fayer, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedgenedf_Fayer_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa78a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    BRCA1_AoU_newsignificance,\n",
    "    PTEN_AoU_newsignificance,\n",
    "    TP53_AoU_newsignificance,\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep_aou, 'AoU')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedgenedf_AoU_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedgenedf_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combined_AoU', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankeyaou(combinedgenedf_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'combined_AoU', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedgenedf_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combined_AoU', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankeyaou(combinedgenedf_AoU_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'gvs_eur_ac', 'European-like', 'orange', 'combined_AoU', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027987fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    BRCA1_gnomadv2_newsignificance,\n",
    "    PTEN_gnomadv2_newsignificance,\n",
    "    TP53_gnomadv2_newsignificance,\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedgenedf_gnomadv2_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34819c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(combinedgenedf_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combined_gnomadv2', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(combinedgenedf_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combined_gnomadv2', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(combinedgenedf_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combined_gnomadv2', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(combinedgenedf_gnomadv2_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combined_gnomadv2', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d792433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    BRCA1_gnomadv3_newsignificance,\n",
    "    PTEN_gnomadv3_newsignificance,\n",
    "    TP53_gnomadv3_newsignificance,\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedgenedf_gnomadv3_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(combinedgenedf_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combined_gnomadv3', sankeyoutputdir, show_node_labels=True)\n",
    "twolevelsankey(combinedgenedf_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combined_gnomadv3', sankeyoutputdir, show_node_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cba107",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankey(combinedgenedf_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combined_gnomadv3', sankeyoutputdir, show_node_labels=False)\n",
    "twolevelsankey(combinedgenedf_gnomadv3_newsignificance, 'Original ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combined_gnomadv3', sankeyoutputdir, show_node_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e66c18",
   "metadata": {},
   "source": [
    "# Step 49: <a class=\"anchor\" id=\"step-49\"></a> Combined Sankey Flow Visuals for Combined Genes and Databases\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ac8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedgenedf_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'combined_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedgenedf_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'combined_Fayer', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35571728",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedgenedf_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in nonEuro', 'non-European-like', 'blue', 'combined_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "combinedFayerNONEUROforbarchartdf = dfforbarchart.copy()\n",
    "\n",
    "twolevelsankeyaou(combinedgenedf_Fayer_newsignificance, 'AmbryOriginalclassification', 'NewClinicalSignificance', 'Variant in Euro', 'European-like', 'orange', 'combined_Fayer', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "combinedFayerEUROforbarchartdf = dfforbarchart.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes to filter\n",
    "dataframes_to_filter = [\n",
    "    combinedBRCA1_newsignificance,\n",
    "    combinedPTEN_newsignificance,\n",
    "    combinedTP53_newsignificance\n",
    "]\n",
    "\n",
    "# Create an empty list to store filtered dataframes\n",
    "filtered_dataframes = []\n",
    "\n",
    "# Filter each dataframe and store the filtered version in the list\n",
    "for df in dataframes_to_filter:\n",
    "    filtered_df = filter_sankey_dataframe(df, columns_to_keep, 'gnomad')\n",
    "    filtered_dataframes.append(filtered_df.reset_index(drop=True))\n",
    "\n",
    "# Concatenate the filtered dataframes into a single dataframe\n",
    "combinedall3db_newsignificance = pd.concat(filtered_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d57337",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedall3db_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedall3db', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)\n",
    "twolevelsankeyaou(combinedall3db_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedall3db', sankeyoutputdir, show_node_labels=True, needtocalculatenoneuroac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "twolevelsankeyaou(combinedall3db_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count Non-European', 'non-European-like', 'blue', 'combinedall3db', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "combinedall3dbNONEUROforbarchartdf = dfforbarchart.copy()\n",
    "\n",
    "twolevelsankeyaou(combinedall3db_newsignificance, 'ClinVar Clinical Significance', 'NewClinicalSignificance', 'Allele Count European (non-Finnish)', 'European-like', 'orange', 'combinedall3db', sankeyoutputdir, show_node_labels=False, needtocalculatenoneuroac=False)\n",
    "combinedall3dbEUROforbarchartdf = dfforbarchart.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646a749",
   "metadata": {},
   "source": [
    "# Step 50: <a class=\"anchor\" id=\"step-50\"></a> VUS Reclassifications Bar Graph\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedFayerNONEUROforbarchartdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedFayerEUROforbarchartdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3bc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedall3dbNONEUROforbarchartdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedall3dbEUROforbarchartdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_proportions(proportion1, n1, proportion2, n2, alpha=0.05):\n",
    "    # Calculate the pooled proportion\n",
    "    pooled_proportion = (proportion1 * n1 + proportion2 * n2) / (n1 + n2)\n",
    "\n",
    "    # Calculate the standard error\n",
    "    standard_error = np.sqrt(pooled_proportion * (1 - pooled_proportion) * (1 / n1 + 1 / n2))\n",
    "\n",
    "    # Calculate the test statistic (Z-score)\n",
    "    z = (proportion1 - proportion2) / standard_error\n",
    "\n",
    "    # Calculate the two-tailed p-value\n",
    "    p_value = 1 * (1 - stats.norm.cdf(abs(z)))\n",
    "\n",
    "    # Check if the p-value is less than alpha\n",
    "    if p_value < alpha:\n",
    "        result = \"There is a statistically significant difference between the two proportions.\"\n",
    "    else:\n",
    "        result = \"There is no statistically significant difference between the two proportions.\"\n",
    "\n",
    "    return p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5010932",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedFayerNONEUROonlyVUSafterforbarchartdf = combinedFayerNONEUROforbarchartdf[combinedFayerNONEUROforbarchartdf['target'] == 'Variant of Uncertain Significance']\n",
    "combinedFayerEUROonlyVUSafterforbarchartdf = combinedFayerEUROforbarchartdf[combinedFayerEUROforbarchartdf['target'] == 'Variant of Uncertain Significance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedall3dbNONEUROonlyVUSafterforbarchartdf = combinedall3dbNONEUROforbarchartdf[combinedall3dbNONEUROforbarchartdf['target'] == 'Variant of Uncertain Significance']\n",
    "combinedall3dbEUROonlyVUSafterforbarchartdf = combinedall3dbEUROforbarchartdf[combinedall3dbEUROforbarchartdf['target'] == 'Variant of Uncertain Significance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65681bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedall3dbNONEUROonlyVUSafterforbarchartdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = {'non-European-like': '#1f77b4', 'European-like': '#ff7f0e'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=fNyJUeiXX3c\n",
    "\n",
    "def barchartforvuscomparison(df1, df2, title):\n",
    "    alpha = 0.05\n",
    "    dictionaryforfinalbarcharts = {\n",
    "        'non-European-like': df1['value'].sum(),\n",
    "        'European-like': df2['value'].sum()}\n",
    "    \n",
    "    \n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    plt.figure(figsize = (2,4))\n",
    "\n",
    "    ax = sns.barplot(x=list(dictionaryforfinalbarcharts.keys()), \n",
    "                     y=list(dictionaryforfinalbarcharts.values()), \n",
    "                     edgecolor='0.2', \n",
    "                     lw=1, \n",
    "                     palette = color_palette)\n",
    "    \n",
    "    rawpvalue = compare_proportions(proportion1 = df1['value'].sum() / totalnoneurocount, \n",
    "                                    n1 = totalnoneurocount, \n",
    "                                    proportion2 = df2['value'].sum() / totaleurocount, \n",
    "                                    n2 = totaleurocount, \n",
    "                                    alpha = 0.05)\n",
    "    \n",
    "    pvalue = round_to_significant_figures(rawpvalue,3)\n",
    "    \n",
    "    \n",
    "    max_val = max(list(dictionaryforfinalbarcharts.values())) + 20\n",
    "    plt.plot([0,0,1,1], [max_val+30, max_val+60, max_val+60, max_val+30], lw=1, color='0.2')\n",
    "    \n",
    "    if pvalue > alpha:\n",
    "        pvalueforgraph = 'ns'\n",
    "    elif pvalue < alpha:\n",
    "        pvalueforgraph = f'p = {pvalue}'\n",
    "        plt.text(x=0.5, y=max_val+60, s='****', ha='center', size=10, weight='bold', color='0.2')\n",
    "    \n",
    "    \n",
    "    plt.text(x=0.5, y=max_val+90, s=pvalueforgraph, ha='center', size=10, weight='bold', color='0.2')\n",
    "\n",
    "\n",
    "    for axis in ['bottom', 'left']:\n",
    "        ax.spines[axis].set_linewidth(1)\n",
    "        ax.spines[axis].set_color('0.2')\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.xticks(size=12, rotation=45, rotation_mode='anchor', ha='right', color='0.2')\n",
    "    plt.yticks(size=12, color='0.2')\n",
    "    ax.set_ylim(0, 700)\n",
    "    ax.tick_params(width=1, color='0.2')\n",
    "\n",
    "    xlabels = ax.get_xticklabels()\n",
    "    \n",
    "    # Add annotations to the bars\n",
    "    for i, value in enumerate(list(dictionaryforfinalbarcharts.values())):\n",
    "        plt.annotate(str(value), xy=(i, value), ha='center', va='bottom')\n",
    "    \n",
    "    for i, label in enumerate(xlabels):\n",
    "        current_text = label.get_text()\n",
    "        \n",
    "        numindividuals = 0\n",
    "        if current_text == 'non-European-like':\n",
    "            numindividuals = totalnoneurocount\n",
    "        elif current_text == 'European-like':\n",
    "            numindividuals = totaleurocount\n",
    "            \n",
    "        new_text = f\"{current_text}\\n(n = {format(numindividuals, ',')})\"\n",
    "        label.set_text(new_text)\n",
    "\n",
    "    ax.set_xticklabels(xlabels)\n",
    "    ax.set_ylabel('VUS Allele Count', fontsize=12)\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', y=1.1)\n",
    "\n",
    "    outputdir = sankeyoutputdir\n",
    "    #filename = f\"{inputname}_{titletext_cleaned}_{label_for_filename}.pdf\"\n",
    "    filepath = os.path.join(outputdir, title+'mainbarchart.pdf')\n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinedFayerEUROforbarchartdf\n",
    "\n",
    "#barchartforvuscomparison(df1=combinedFayerNONEUROforbarchartdf, df2=combinedFayerEUROforbarchartdf, title='No MAVE')\n",
    "#barchartforvuscomparison(df1=combinedFayerNONEUROonlyVUSafterforbarchartdf, df2=combinedFayerEUROonlyVUSafterforbarchartdf, title='With MAVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161640ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "barchartforvuscomparison(df1=combinedall3dbNONEUROforbarchartdf, df2=combinedall3dbEUROforbarchartdf, title='No MAVE')\n",
    "barchartforvuscomparison(df1=combinedall3dbNONEUROonlyVUSafterforbarchartdf, df2=combinedall3dbEUROonlyVUSafterforbarchartdf, title='With MAVE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bd49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedall3db_newsignificance['NewClinicalSignificance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb826183",
   "metadata": {},
   "source": [
    "# Part 9: <a class=\"anchor\" id=\"part-9\"></a>Evidence Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f92f71",
   "metadata": {},
   "source": [
    "# Step 51: <a class=\"anchor\" id=\"step-51\"></a> Functions for Evidence Code Breakdown\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ae5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedBRCA1_newsignificance[combinedBRCA1_newsignificance['ClinVar Clinical Significance'] == 'VUS']['PostGeneSpecificEvidenceCodes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedPTEN_newsignificance[combinedPTEN_newsignificance['ClinVar Clinical Significance'] == 'VUS']['PostGeneSpecificEvidenceCodes'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18354041",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedTP53_newsignificance[combinedTP53_newsignificance['ClinVar Clinical Significance'] == 'VUS']['PostGeneSpecificEvidenceCodes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinedgenedf_Fayer_newsignificance\n",
    "\n",
    "combinedfayervusstartandreclass = combinedgenedf_Fayer_newsignificance[\n",
    "    (combinedgenedf_Fayer_newsignificance['AmbryOriginalclassification'] == 'VUS') &\n",
    "    (combinedgenedf_Fayer_newsignificance['NewClinicalSignificance'] != 'Variant of Uncertain Significance')\n",
    "]\n",
    "\n",
    "combinedfayervusstartandreclass = combinedfayervusstartandreclass.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedvusstartandreclass = combinedall3db_newsignificance[\n",
    "    (combinedall3db_newsignificance['ClinVar Clinical Significance'] == 'VUS') &\n",
    "    (combinedall3db_newsignificance['NewClinicalSignificance'] != 'Variant of Uncertain Significance')\n",
    "]\n",
    "\n",
    "combinedvusstartandreclass = combinedvusstartandreclass.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d03043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processforallcodes(df, fayer=False):\n",
    "    # Create a new column with default empty lists\n",
    "    df['CountingEvidenceCodes'] = None\n",
    "    postcolumnname = ''\n",
    "    \n",
    "    if fayer:\n",
    "        df['Variant in nonEuro'] = df['Variant in nonEuro'].fillna(0) \n",
    "        df['Variant in Euro'] = df['Variant in Euro'].fillna(0)\n",
    "        df['TotalAlleleCount'] = df['Variant in nonEuro'] + df['Variant in Euro']\n",
    "        df['Allele Count European (non-Finnish)'] = df['Variant in Euro'].astype(int)\n",
    "        df['Allele Count Non-European'] = df['Variant in nonEuro'].astype(int)\n",
    "        df['TotalAlleleCount'] = df['TotalAlleleCount'].astype(int)\n",
    "        \n",
    "        postcolumnname = 'FayerCombinedPostEvidenceCodes'\n",
    "        # Check if the first element is a string. This assumes all elements are similarly formatted.\n",
    "        if isinstance(df['FayerCombinedPostEvidenceCodes'].iloc[0], str):\n",
    "            # Convert each string to a list by splitting the string on commas.\n",
    "            df['FayerCombinedPostEvidenceCodes'] = df['FayerCombinedPostEvidenceCodes'].apply(lambda x: x.split(', '))\n",
    "\n",
    "    else:\n",
    "        df['TotalAlleleCount'] = df['Allele Count European (non-Finnish)'] + df['Allele Count Non-European']\n",
    "        df['Allele Count European (non-Finnish)'] = df['Allele Count European (non-Finnish)'].astype(int)\n",
    "        df['Allele Count Non-European'] = df['Allele Count Non-European'].astype(int)\n",
    "        df['TotalAlleleCount'] = df['TotalAlleleCount'].astype(int)\n",
    "        \n",
    "        postcolumnname = 'PostGeneSpecificEvidenceCodes'\n",
    "        # Apply ast.literal_eval if the column contains string representations of lists\n",
    "        if isinstance(df['PostGeneSpecificEvidenceCodes'].iloc[0], str):\n",
    "            df['PostGeneSpecificEvidenceCodes'] = df['PostGeneSpecificEvidenceCodes'].apply(ast.literal_eval)\n",
    "        \n",
    "    df['NewClinicalSignificance'] = df['NewClinicalSignificance'].astype(str)\n",
    "\n",
    "    df['EuroCountingEvidenceCodes'] = None\n",
    "    df['NonEuroCountingEvidenceCodes'] = None\n",
    "    df['TotalCountingEvidenceCodes'] = None\n",
    "        \n",
    "    for index, row in df.iterrows():\n",
    "        # Fetch the value in NewClinicalSignificance column\n",
    "        significance = row['NewClinicalSignificance']\n",
    "\n",
    "        # Apply the logic based on the value of NewClinicalSignificance\n",
    "        if significance in ['Benign', 'Likely Benign']:\n",
    "            # Filter out strings starting with 'P' from PostGeneSpecificEvidenceCodes\n",
    "            filtered_codes = [code for code in row[postcolumnname] if not code.startswith('P')]\n",
    "            df.at[index, 'CountingEvidenceCodes'] = filtered_codes\n",
    "        elif significance in ['Pathogenic', 'Likely Pathogenic']:\n",
    "            # Filter out strings starting with 'P' from PostGeneSpecificEvidenceCodes\n",
    "            filtered_codes = [code for code in row[postcolumnname] if not code.startswith('B')]\n",
    "            df.at[index, 'CountingEvidenceCodes'] = filtered_codes\n",
    "\n",
    "\n",
    "        # Assign the resulting lists to the respective columns\n",
    "        df.at[index, 'EuroCountingEvidenceCodes'] = df.at[index, 'CountingEvidenceCodes'] * int(row['Allele Count European (non-Finnish)'])\n",
    "        df.at[index, 'NonEuroCountingEvidenceCodes'] = df.at[index, 'CountingEvidenceCodes'] * int(row['Allele Count Non-European'])\n",
    "        df.at[index, 'TotalCountingEvidenceCodes'] = df.at[index, 'CountingEvidenceCodes'] * int(row['TotalAlleleCount'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedfayervusstartandreclasswithcodesprocessed = processforallcodes(combinedfayervusstartandreclass, fayer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bb4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedfayervusstartandreclasswithcodesprocessed['NewClinicalSignificance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a347cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedvusstartandreclasswithcodesprocessed = processforallcodes(combinedvusstartandreclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedvusstartandreclasswithcodesprocessed['NewClinicalSignificance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef9d85",
   "metadata": {},
   "source": [
    "# Step 52: <a class=\"anchor\" id=\"step-52\"></a> Plotting Evidence Code Frequencies\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = {'PP3': 'Computational Predictors',\n",
    "            'PP3_Moderate': 'Computational Predictors',\n",
    "            'BP4': 'Computational Predictors',\n",
    "            'BS3': 'MAVE',\n",
    "            'PS3': 'MAVE',\n",
    "            'BS3_Moderate': 'MAVE',\n",
    "            'BS3_Supporting': 'MAVE',\n",
    "            'PS3_Moderate': 'MAVE',\n",
    "            'BA1': 'Allele Frequency',\n",
    "            'BS1': 'Allele Frequency',\n",
    "            'BS1_Supporting': 'Allele Frequency',\n",
    "            'BS3_M': 'MAVE',\n",
    "            'PM2_P': 'Allele Frequency',\n",
    "            'BS3_P': 'MAVE',\n",
    "            'PP3_M': 'Computational Predictors',\n",
    "            'PM2_Supporting': 'Allele Frequency'}\n",
    "\n",
    "nullgrouping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_string_frequencies(df, column_name, grouping):\n",
    "    #font\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "    \n",
    "    # Apply ast.literal_eval if the column contains string representations of lists\n",
    "    if isinstance(df[column_name].iloc[0], str):\n",
    "        df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "\n",
    "    # Flatten the list of lists into a single list\n",
    "    all_strings = [item for sublist in df[column_name] for item in sublist if item]\n",
    "\n",
    "    # If a grouping dictionary is provided, apply it\n",
    "    if grouping:\n",
    "        all_strings = [grouping.get(item, item) for item in all_strings]\n",
    "\n",
    "    # Count the frequencies of each string\n",
    "    frequencies = Counter(all_strings)\n",
    "    sorted_frequencies = dict(sorted(frequencies.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Create a bar graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(sorted_frequencies.keys(), sorted_frequencies.values(), edgecolor='black')\n",
    "    plt.xlabel('Evidence Code Categories')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequency of Evidence Codes')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add numbers above each bar\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "    \n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Add black perimeter\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b77b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_string_frequencies(combinedvusstartandreclasswithcodesprocessed, 'TotalCountingEvidenceCodes', grouping=grouping)\n",
    "plot_string_frequencies(combinedvusstartandreclasswithcodesprocessed, 'TotalCountingEvidenceCodes', grouping=nullgrouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_string_frequencies(combinedvusstartandreclasswithcodesprocessed, 'EuroCountingEvidenceCodes', grouping=grouping)\n",
    "plot_string_frequencies(combinedvusstartandreclasswithcodesprocessed, 'EuroCountingEvidenceCodes', grouping=nullgrouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_string_frequencies(combinedvusstartandreclasswithcodesprocessed, 'NonEuroCountingEvidenceCodes', grouping=grouping)\n",
    "plot_string_frequencies(combinedvusstartandreclasswithcodesprocessed, 'NonEuroCountingEvidenceCodes', grouping=nullgrouping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c78d40",
   "metadata": {},
   "source": [
    "# Step 53: <a class=\"anchor\" id=\"step-53\"></a> Paired Frequencies of Evidence Codes Based on Superpopulation Grouping\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plotpairedfrequenciesofevidencecodes(df, column_name_non_euro, column_name_euro, grouping, show_top_three=True):\n",
    "    # Font\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    # Apply ast.literal_eval if the column contains string representations of lists\n",
    "    if isinstance(df[column_name_non_euro].iloc[0], str):\n",
    "        df[column_name_non_euro] = df[column_name_non_euro].apply(ast.literal_eval)\n",
    "    if isinstance(df[column_name_euro].iloc[0], str):\n",
    "        df[column_name_euro] = df[column_name_euro].apply(ast.literal_eval)\n",
    "\n",
    "    # Flatten the lists into a single list for non-European and European data\n",
    "    all_strings_non_euro = [item for sublist in df[column_name_non_euro] for item in sublist if item]\n",
    "    all_strings_euro = [item for sublist in df[column_name_euro] for item in sublist if item]\n",
    "\n",
    "    # If a grouping dictionary is provided, apply it\n",
    "    if grouping:\n",
    "        all_strings_non_euro = [grouping.get(item, item) for item in all_strings_non_euro]\n",
    "        all_strings_euro = [grouping.get(item, item) for item in all_strings_euro]\n",
    "\n",
    "    # Count the frequencies of each string for non-European and European data\n",
    "    frequencies_non_euro = Counter(all_strings_non_euro)\n",
    "    frequencies_euro = Counter(all_strings_euro)\n",
    "\n",
    "    # Sort the frequencies for both non-European and European data\n",
    "    sorted_frequencies_non_euro = dict(sorted(frequencies_non_euro.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_frequencies_euro = dict(sorted(frequencies_euro.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Decide whether to use all categories or only the top three\n",
    "    if show_top_three:\n",
    "        top_3_non_euro = dict(list(sorted_frequencies_non_euro.items())[:3])\n",
    "        top_3_euro = dict(list(sorted_frequencies_euro.items())[:3])\n",
    "    else:\n",
    "        top_3_non_euro = sorted_frequencies_non_euro\n",
    "        top_3_euro = sorted_frequencies_euro\n",
    "\n",
    "    # Create a bar graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot non-European bars\n",
    "    bars_non_euro = plt.bar([x - 0.22 for x in range(len(top_3_non_euro))], top_3_non_euro.values(), width=0.4, color=color_palette['non-European-like'], label='Non-European', edgecolor='black')\n",
    "\n",
    "    # Plot European bars next to non-European bars\n",
    "    bars_euro = plt.bar([x + 0.22 for x in range(len(top_3_euro))], top_3_euro.values(), width=0.4, color=color_palette['European-like'], label='European', edgecolor='black')\n",
    "\n",
    "    plt.xlabel('Evidence Code Categories')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequency of Evidence Codes')\n",
    "    plt.xticks([x for x in range(len(top_3_non_euro))], top_3_non_euro.keys(), rotation=0, ha=\"center\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add numbers above each bar for non-European data\n",
    "    for bar in bars_non_euro:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "    # Add numbers above each bar for European data\n",
    "    for bar in bars_euro:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Add black perimeter\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136362ce",
   "metadata": {},
   "source": [
    "# Step 54: <a class=\"anchor\" id=\"step-54\"></a> Functions for Quantifying Essential Evidence Codes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatingonedropoutdataframe(input_list, substitution_dict, gene_name, new_clinical_significance):\n",
    "    data = []\n",
    "    for item in input_list:\n",
    "        original_set = input_list.copy()\n",
    "        dropped = item\n",
    "        original_set.remove(dropped)\n",
    "        new_set = [substitution_dict.get(element, element) for element in original_set]\n",
    "        intermediate_new_set = new_set.copy()\n",
    "        for i, element in enumerate(intermediate_new_set):\n",
    "            if element in substitution_dict.keys():\n",
    "                intermediate_new_set[i] = [key for key, value in substitution_dict.items() if value == element][0]\n",
    "        data.append([input_list, dropped, original_set, intermediate_new_set, gene_name, new_clinical_significance])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['OriginalEvidenceCodeSet', 'EvidenceCodeDropped', 'NewEvidenceCodeSet', 'IntermediateNewEvidenceCodeSet', 'GeneName', 'NewClinicalSignificanceAfterOriginalReclassification'])\n",
    "    return df\n",
    "\n",
    "def get_essential_list(df):\n",
    "    essential_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Essential']:\n",
    "            essential_list.append(row['EvidenceCodeDropped'])\n",
    "    return essential_list\n",
    "\n",
    "substitution_dict = {'PP3_Moderate': 'PM',\n",
    "            'BS3_Moderate': 'BM',\n",
    "            'BS3_Supporting': 'BP',\n",
    "            'BS1_Supporting': 'BP',\n",
    "            'BP1_Strong': 'BS', \n",
    "            'PM2_Supporting': 'PP',\n",
    "            'PP3_M': 'PM', \n",
    "            'BS3_P': 'BP', \n",
    "            'PM2_P': 'PP', \n",
    "            'BS3_M': 'BM'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def onedropoutanalysis(dfinput):\n",
    "    dfinput['EssentialCodes'] = None\n",
    "    dfinput['EuroEssentialCodes'] = None\n",
    "    dfinput['NonEuroEssentialCodes'] = None\n",
    "    dfinput['TotalEssentialCodes'] = None\n",
    "\n",
    "    for index, row in dfinput.iterrows():\n",
    "        gene_name = row['NameOfGene']\n",
    "        new_clinical_significance = row['NewClinicalSignificance']\n",
    "        input_list = row['CountingEvidenceCodes']\n",
    "        \n",
    "        newdf = evaluatingonedropoutdataframe(input_list, substitution_dict, gene_name, new_clinical_significance)\n",
    "        newdf = accumulate_evidence(newdf, colname='IntermediateNewEvidenceCodeSet')\n",
    "        \n",
    "        if gene_name == 'BRCA1':\n",
    "            newdf['NewClinicalSignificanceAfterOneDropOut'] = process_dictionary_column_brca1(newdf['EvidenceDictionary'])\n",
    "        elif gene_name == 'PTEN':\n",
    "            newdf['NewClinicalSignificanceAfterOneDropOut'] = process_dictionary_column_pten(newdf['EvidenceDictionary'])           \n",
    "        elif gene_name == 'TP53':\n",
    "            newdf['NewClinicalSignificanceAfterOneDropOut'] = process_dictionary_column_tp53(newdf['EvidenceDictionary'])\n",
    "        \n",
    "        newdf['Essential'] = newdf['NewClinicalSignificanceAfterOneDropOut'].apply(lambda x: True if x == \"Variant of Uncertain Significance\" else False)\n",
    "        \n",
    "        dfinput.at[index, 'EssentialCodes'] = get_essential_list(newdf)\n",
    "        dfinput.at[index, 'EuroEssentialCodes'] = dfinput.at[index, 'EssentialCodes'] * int(row['Allele Count European (non-Finnish)'])\n",
    "        dfinput.at[index, 'NonEuroEssentialCodes'] = dfinput.at[index, 'EssentialCodes'] * int(row['Allele Count Non-European'])\n",
    "        dfinput.at[index, 'TotalEssentialCodes'] = dfinput.at[index, 'EssentialCodes'] * int(row['TotalAlleleCount'])\n",
    "\n",
    "    \n",
    "    return dfinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onedropfayer = onedropoutanalysis(combinedfayervusstartandreclasswithcodesprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "onedropbreakdown = onedropoutanalysis(combinedvusstartandreclasswithcodesprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39beca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpairedfrequenciesofevidencecodes(df = onedropbreakdown, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes', \n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c3f6f",
   "metadata": {},
   "source": [
    "# Step 55: <a class=\"anchor\" id=\"step-55\"></a> Plotting Evidence Code Breakdown with Essential Codes\n",
    "* [Back Up to Table of Contents](#step-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cc70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df, title, column_name_non_euro, column_name_euro, column_name_non_euro_essential, column_name_euro_essential, grouping, show_top_three=True, alpha=0.05):\n",
    "    # Font\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "    # Apply ast.literal_eval if the column contains string representations of lists\n",
    "    for column in [column_name_non_euro, column_name_euro, column_name_non_euro_essential, column_name_euro_essential]:\n",
    "        if isinstance(df[column].iloc[0], str):\n",
    "            df[column] = df[column].apply(ast.literal_eval)\n",
    "\n",
    "    # Flatten the lists into a single list for non-European and European data\n",
    "    all_strings_non_euro = [item for sublist in df[column_name_non_euro] for item in sublist if item]\n",
    "    all_strings_euro = [item for sublist in df[column_name_euro] for item in sublist if item]\n",
    "    all_strings_non_euro_essential = [item for sublist in df[column_name_non_euro_essential] for item in sublist if item]\n",
    "    all_strings_euro_essential = [item for sublist in df[column_name_euro_essential] for item in sublist if item]\n",
    "\n",
    "    # If a grouping dictionary is provided, apply it\n",
    "    if grouping:\n",
    "        all_strings_non_euro = [grouping.get(item, item) for item in all_strings_non_euro]\n",
    "        all_strings_euro = [grouping.get(item, item) for item in all_strings_euro]\n",
    "        all_strings_non_euro_essential = [grouping.get(item, item) for item in all_strings_non_euro_essential]\n",
    "        all_strings_euro_essential = [grouping.get(item, item) for item in all_strings_euro_essential]\n",
    "\n",
    "    # Count the frequencies of each string for non-European and European data\n",
    "    frequencies_non_euro = Counter(all_strings_non_euro)\n",
    "    frequencies_euro = Counter(all_strings_euro)\n",
    "    frequencies_non_euro_essential = Counter(all_strings_non_euro_essential)\n",
    "    frequencies_euro_essential = Counter(all_strings_euro_essential)\n",
    "\n",
    "    # Sort the frequencies for both non-European and European data\n",
    "    sorted_frequencies_non_euro = dict(sorted(frequencies_non_euro.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_frequencies_euro = dict(sorted(frequencies_euro.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_frequencies_non_euro_essential = dict(sorted(frequencies_non_euro_essential.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_frequencies_euro_essential = dict(sorted(frequencies_euro_essential.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Calculate percentage of essential codes over the overall codes for all categories\n",
    "    percentage_non_euro_essential = {key: (frequencies_non_euro_essential[key] / frequencies_non_euro.get(key, 1)) * 100 for key in sorted_frequencies_non_euro}\n",
    "    percentage_euro_essential = {key: (frequencies_euro_essential[key] / frequencies_euro.get(key, 1)) * 100 for key in sorted_frequencies_euro}\n",
    "\n",
    "    # Determine the common categories\n",
    "    common_categories = sorted(set(sorted_frequencies_non_euro.keys()) | set(sorted_frequencies_non_euro_essential.keys()), key=lambda x: sorted_frequencies_non_euro.get(x, 0), reverse=True)\n",
    "\n",
    "    # Optionally select the top three pairs of bars\n",
    "    if show_top_three:\n",
    "        common_categories = common_categories[:3]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "    else:\n",
    "        # Create a bar graph\n",
    "        plt.figure(figsize=(24, 6))\n",
    "\n",
    "    # Plot non-European bars\n",
    "    bars_non_euro = plt.bar([x - 0.22 for x in range(len(common_categories))], [sorted_frequencies_non_euro.get(cat, 0) for cat in common_categories], width=0.4, color=color_palette['non-European-like'], label='Non-European', edgecolor='black')\n",
    "\n",
    "    # Plot European bars next to non-European bars\n",
    "    bars_euro = plt.bar([x + 0.22 for x in range(len(common_categories))], [sorted_frequencies_euro.get(cat, 0) for cat in common_categories], width=0.4, color=color_palette['European-like'], label='European', edgecolor='black')\n",
    "\n",
    "    # Shade non-European essential bars on top of non-European bars\n",
    "    bars_non_euro_essential = plt.bar([x - 0.22 for x in range(len(common_categories))], [sorted_frequencies_non_euro_essential.get(cat, 0) for cat in common_categories], width=0.4, color='gray', alpha=0.5, label='Non-European Essential', edgecolor='black', hatch='////')\n",
    "\n",
    "    # Shade European essential bars on top of European bars\n",
    "    bars_euro_essential = plt.bar([x + 0.22 for x in range(len(common_categories))], [sorted_frequencies_euro_essential.get(cat, 0) for cat in common_categories], width=0.4, color='lightgray', alpha=0.5, label='European Essential', edgecolor='black', hatch='////')\n",
    "\n",
    "    plt.xlabel('Evidence Code Categories')\n",
    "    plt.ylabel('Total Allele Count')\n",
    "    plt.title(title)\n",
    "    plt.xticks(range(len(common_categories)), common_categories, rotation=0, ha=\"center\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate the maximum frequency\n",
    "    max_frequency = max(max(sorted_frequencies_non_euro.values()), max(sorted_frequencies_euro.values()))\n",
    "\n",
    "    # Increase the maximum frequency by 30%\n",
    "    adjusted_max_frequency = max_frequency * 1.3\n",
    "\n",
    "    # Set the y-axis limits\n",
    "    plt.ylim(0, adjusted_max_frequency)\n",
    "\n",
    "#     # Add percentage text above each bar for non-European data\n",
    "#     for i, bar in enumerate(bars_non_euro):\n",
    "#         yval = bar.get_height()\n",
    "#         category = common_categories[i]\n",
    "#         percentage = percentage_non_euro_essential.get(category, 0)\n",
    "#         plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval} ({percentage:.1f}%)', ha='center', va='bottom')\n",
    "        \n",
    "       \n",
    "        \n",
    "#     # Add percentage text above each bar for European data\n",
    "#     for i, bar in enumerate(bars_euro):\n",
    "#         yval = bar.get_height()\n",
    "#         category = common_categories[i]\n",
    "#         percentage = percentage_euro_essential.get(category, 0)\n",
    "#         plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval} ({percentage:.1f}%)', ha='center', va='bottom')\n",
    "\n",
    "    # Add percentage text above each bar for non-European and European data simultaneously\n",
    "    for i, (bar_non_euro, bar_euro) in enumerate(zip(bars_non_euro, bars_euro)):\n",
    "        yval_non_euro = bar_non_euro.get_height()\n",
    "        yval_euro = bar_euro.get_height()\n",
    "        category = common_categories[i]\n",
    "        percentage_non_euro = percentage_non_euro_essential.get(category, 0)\n",
    "        percentage_euro = percentage_euro_essential.get(category, 0)\n",
    "        plt.text(bar_non_euro.get_x() + bar_non_euro.get_width() / 2, yval_non_euro, f'{yval_non_euro} ({percentage_non_euro:.1f}%)', ha='center', va='bottom')\n",
    "        plt.text(bar_euro.get_x() + bar_euro.get_width() / 2, yval_euro, f'{yval_euro} ({percentage_euro:.1f}%)', ha='center', va='bottom')\n",
    "        \n",
    "        proportion1 = percentage_non_euro / 100\n",
    "        proportion2 = percentage_euro / 100\n",
    "        \n",
    "        if (proportion1 != 1 and proportion2 != 1) and (proportion1 != 0 and proportion2 != 0):\n",
    "            rawpvalue = compare_proportions(proportion1 = proportion1, \n",
    "                                            n1 = yval_non_euro, \n",
    "                                            proportion2 = proportion2, \n",
    "                                            n2 = yval_euro, \n",
    "                                            alpha = 0.05)\n",
    "            pvalue = round_to_significant_figures(rawpvalue,3)\n",
    "            \n",
    "        else:\n",
    "            pvalue = 1\n",
    "            \n",
    "        \n",
    "        #print(rawpvalue)\n",
    "        \n",
    "        xcoord=bar_non_euro.get_x() + (bar_non_euro.get_width()/2)\n",
    "        \n",
    "        \n",
    "        if yval_non_euro > 10 and yval_euro > 10:\n",
    "            \n",
    "            if pvalue > alpha:\n",
    "                pvalueforgraph = 'ns'\n",
    "            elif pvalue < alpha:\n",
    "                pvalueforgraph = f'p = {pvalue}'\n",
    "\n",
    "\n",
    "            if yval_non_euro > 70 or yval_euro > 70:\n",
    "                ybase = 40\n",
    "            else:\n",
    "                ybase = 0\n",
    "\n",
    "            if yval_non_euro >= yval_euro:\n",
    "                ybase += yval_non_euro\n",
    "            else:\n",
    "                ybase += yval_euro\n",
    "\n",
    "\n",
    "            plt.plot([xcoord + 0.11, xcoord + 0.11, xcoord + 0.33, xcoord + 0.33], [ybase+.06*ybase, ybase+.08*ybase, ybase+.08*ybase, ybase+.06*ybase], lw=1, color='0.2')\n",
    "\n",
    "            plt.text(x=xcoord + 0.22, y=ybase + .085*ybase, s=pvalueforgraph, ha='center', size=14, weight='bold', color='0.2')\n",
    "    \n",
    "\n",
    "        \n",
    "    # Remove grid lines\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Add black perimeter\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "\n",
    "    # Show the legend\n",
    "    #plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93659786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replicating Main Results for Essential Codes in Fayer et al Dataset\n",
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = onedropfayer, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True,\n",
    "                                     title = 'Essential Codes Across Top Three Evidence Code Categories For Alleles from Fayer et al')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5b563",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Replicating VUS to LP/P Results for Essential Codes in Fayer et al Dataset\n",
    "filteredpath = onedropfayer[onedropfayer['NewClinicalSignificance'].str.contains('Pathogenic', case=False, na=False)]\n",
    "\n",
    "\n",
    "\n",
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = filteredpath, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True,\n",
    "                                     title = 'Essential Codes Across Top Three Evidence Code Categories for Alleles Upgraded from VUS to LP or P in the Fayer et al Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replicating VUS to LB/B Results for Essential Codes in Fayer et al Dataset\n",
    "\n",
    "filteredbenign = onedropfayer[onedropfayer['NewClinicalSignificance'].str.contains('Benign', case=False, na=False)]\n",
    "\n",
    "\n",
    "\n",
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = filteredbenign, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True,\n",
    "                                     title = 'Essential Codes Across Top Three Evidence Code Categories for Alleles Downgraded from VUS to LB or B in the Fayer et al Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e06d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main VUS Reclassification Analysis for Essential Codes in gnomad v2 + gnomad v3 + All of Us v7\n",
    "\n",
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = onedropbreakdown, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True,\n",
    "                                     title = 'Essential Codes Across Top Three Evidence Code Categories for Reclassified VUS Alleles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19559827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main VUS Reclassification Analysis for Essential Codes in gnomad v2 + gnomad v3 + All of Us v7\n",
    "#Seeing which VUS were reclassified without needing MAVE data\n",
    "\n",
    "filterednomave = onedropbreakdown[~onedropbreakdown['PostGeneSpecificEvidenceCodes'].apply(lambda x: any(code in ['BS3', 'PS3', 'BS3_Supporting', 'BS3_Moderate'] for code in x))]\n",
    "filterednomave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main VUS Reclassification Analysis for Essential Codes in gnomad v2 + gnomad v3 + All of Us v7\n",
    "filteredpath = onedropbreakdown[onedropbreakdown['NewClinicalSignificance'].str.contains('Pathogenic', case=False, na=False)]\n",
    "\n",
    "\n",
    "\n",
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = filteredpath, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True,\n",
    "                                     title = 'Essential Codes Across Top Three Evidence Code Categories for Alleles Downgraded from VUS to LP or P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b67aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredbenign = onedropbreakdown[onedropbreakdown['NewClinicalSignificance'].str.contains('Benign', case=False, na=False)]\n",
    "\n",
    "\n",
    "\n",
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = filteredbenign, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=grouping,\n",
    "                                     show_top_three=True,\n",
    "                                     title = 'Essential Codes Across Top Three Evidence Code Categories for Alleles Downgraded from VUS to LB or B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = onedropbreakdown, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=nullgrouping,\n",
    "                                     show_top_three=False,\n",
    "                                     title = 'Essential Codes Across All Evidence Codes Used for Reclassified VUS Alleles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0113f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotevidencecodebreakdownwithessentialshadingeurovsnoneuro(df = onedropfayer, \n",
    "                                     column_name_non_euro = 'NonEuroCountingEvidenceCodes', \n",
    "                                     column_name_euro = 'EuroCountingEvidenceCodes',\n",
    "                                     column_name_non_euro_essential = 'NonEuroEssentialCodes',\n",
    "                                     column_name_euro_essential = 'EuroEssentialCodes',\n",
    "                                     grouping=nullgrouping,\n",
    "                                     show_top_three=False,\n",
    "                                     title = 'Essential Codes Across All Evidence Codes For Alleles from Fayer et al')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnstokeepforfinaldf = ['NameOfGene', 'ReferenceGenome','Chrom', 'Pos', 'Reference', 'Alternate', 'Transcript','Transcript Consequence', 'VEP Annotation', \n",
    "                           'TotalAlleleCount', 'Allele Count African/African American', 'Allele Count Latino/Admixed American',\n",
    "                           'Allele Count East Asian', 'Allele Count European (non-Finnish)', 'Allele Count Other', \n",
    "                           'Allele Count South Asian', 'Allele Count Non-European',\n",
    "                           'ClinVar Clinical Significance',   \n",
    "                           'PostGeneSpecificEvidenceCodes', 'NewClinicalSignificance',\n",
    "                           'CountingEvidenceCodes', 'EssentialCodes']\n",
    "\n",
    "dfofvariantreclassificationsandevidencecodeforoutput = onedropbreakdown[columnstokeepforfinaldf]\n",
    "save_dataframe_to_pdf(df = dfofvariantreclassificationsandevidencecodeforoutput, filename = 'output/reclassifications.pdf')\n",
    "dfofvariantreclassificationsandevidencecodeforoutput.to_csv('output/reclassifications.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_columns(df, col1, col2):\n",
    "    # Define the groups\n",
    "    group1 = {'Benign', 'Likely Benign'}\n",
    "    group2 = {'Pathogenic', 'Likely Pathogenic'}\n",
    "\n",
    "    def classify(row):\n",
    "        val1, val2 = row[col1], row[col2]\n",
    "\n",
    "        # Check if the values are the same\n",
    "        if val1 == val2:\n",
    "            return True\n",
    "\n",
    "        # Check if the values belong to the same grouping\n",
    "        if (val1 in group1 and val2 in group1) or (val1 in group2 and val2 in group2):\n",
    "            return True\n",
    "\n",
    "        # Otherwise, they are discordant classifications\n",
    "        return False\n",
    "\n",
    "    # Apply the classification function to each row\n",
    "    df['SameOverallClassification'] = df.apply(classify, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnstokeepfayerfinaldf = ['NameOfGene', 'ReferenceGenome','Transcript Consequence', 'VEP Annotation', \n",
    "                           'AmbryOriginalclassification', 'FayerIntermediateEvidenceCodes', 'FayerCombinedPostEvidenceCodes', 'FayerReclassification',\n",
    "                           'TotalAlleleCount', 'Allele Count African/African American', 'Allele Count Latino/Admixed American',\n",
    "                           'Allele Count East Asian', 'Allele Count European (non-Finnish)', 'Allele Count Other', \n",
    "                           'Allele Count South Asian', 'Allele Count Non-European',   \n",
    "                           'PostGeneSpecificEvidenceCodes', 'NewClinicalSignificance', 'SameOverallClassification',\n",
    "                           'CountingEvidenceCodes', 'EssentialCodes']\n",
    "\n",
    "\n",
    "fayerfayerdf = compare_columns(onedropfayer, 'FayerReclassification', 'NewClinicalSignificance')\n",
    "fayerdfofvariantreclassificationsandevidencecodeforoutput = fayerfayerdf[columnstokeepfayerfinaldf]\n",
    "save_dataframe_to_pdf(df = fayerdfofvariantreclassificationsandevidencecodeforoutput, filename = 'output/fayerreclassifications.pdf')\n",
    "fayerdfofvariantreclassificationsandevidencecodeforoutput.to_csv('output/fayerreclassifications.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2977ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to this Jupyter Notebook file\n",
    "notebook_file = 'ReducingVariantClassificationInequitiesCode_20240727.ipynb'\n",
    "\n",
    "# Read the notebook using the nbformat library\n",
    "with open(notebook_file, 'r', encoding='utf-8') as notebook_file:\n",
    "    notebook_content = read(notebook_file, as_version=4)\n",
    "\n",
    "# Count the cells\n",
    "total_cells = sum(1 for cell in notebook_content.cells)\n",
    "\n",
    "print(f'The notebook contains {total_cells} cells.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
